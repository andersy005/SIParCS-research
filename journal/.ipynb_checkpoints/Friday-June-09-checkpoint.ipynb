{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8:15 AM - 9:05 AM\n",
    "\n",
    "- On Thursday, June 08th, I retrieved data from [NCDC Version 4.0 Extended Reconstructed SST Analyses](https://rda.ucar.edu/datasets/ds277.0/). The objective behind this, is to use this data with PySpark to calculate El Nino index.\n",
    "\n",
    "- The original data format is netCDF classic and the total files I retrieved is ~1900. \n",
    "- This morning I had to manually convert data from 2016-2006 in netCDF4 format. I manually run ```nccopy -k 3 ersst.v4.200706.nc v4/ersst.v4.200706.nc``` command on 120 files this morning.\n",
    "\n",
    "*Lessons Learned:*\n",
    "- Knowing how to automate terminal tasks with Bash is time saving:\n",
    "\n",
    "*Objective:*\n",
    "- Learn more Bash Scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9:20 AM - 1:30 PM\n",
    "\n",
    "- I worked on El Nino Index use case.:\n",
    "    - Read data from netCDF4 files into RDD\n",
    "    - Explored the created RDDs with Spark's actions and Transformations\n",
    "    - Use NumPy to find indices of our region of interest\n",
    "    - Use Spark's map transformation to create a new RDD containing elements with values from our region of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 PM - 5 PM\n",
    "- Dr. Paul and I met and discussed about some of the issues about the dataset I was using.\n",
    "- He explained to me what the scale_factor, scale_offset meant and the fact that they are a result of [CF Conventions and Metadata](http://cfconventions.org/)\n",
    "- Convert all the data files in NCDC dataset from netCDF classic to netCDF-4 (Thanks Kevin!)\n",
    "\n",
    "\n",
    "**TO DO:**\n",
    "- Finish up the experimentation with the 132 months data that I have started:\n",
    "    - Mask the _FillinValues\n",
    "    - calcute some statistics of the masked arrays from step above\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
