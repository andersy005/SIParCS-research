{
    "docs": [
        {
            "location": "/", 
            "text": "PySpark for \"Big\" Atmospheric \n Oceanic Data Analysis\n\n\nBackground\n\n\n\n\nClimate and Weather directly impact all aspects of society. Understanding these processeses provide important information for policy - and decision - makers. To understand the average climate conditions and extreme weather events, Earth science and climate change researchers need data from climate models and observations. As such in the typical work flow of climate change and atmospheric research, much time is wasted waiting to reformat and regrid data to homogeneous formats. Additionally, because of the data volume, in order to compute metrics, perform analysis, and visualize data / generate plots, multi-stage processes with repeated I/O are used - the root cause of performance bottlenecks and the resulting user\u2019s frustrations related to time inefficiencies.\n\nNASA-SciSpark project\n\n\n\n\n1\n: Spark is a cluster computing paradigm based on the MapReduce paradigm that has garnered many scientific analysis workflows and are very well suited for Spark.  As a result of this lack of great deal of interest for its power and ease of use in analyzing \u201cbig data\u201d in the commercial and computer science sectors.  In much of the scientific sector, however --- and specifically in the atmospheric and oceanic sciences --- Spark has not captured the interest of scientists for analyzing their data, even though their datasets may be larger than many commercial datasets interest, there are very few platforms on which scientists can experiment with and learn about using Hadoop and/or Spark for their scientific research.  Additionally, there are very few resources to teach and educate scientists on how or why to use Hadoop or Spark for their analysis.\n\n\nGoal\n\n\nPySpark for Big Atmospheric \n Oceanic Data Analysis\n is a \nCISL/SIParCS research project\n that seeks to explore the realm of distributed parallel computing on NCAR's Yellowstone and Cheyenne supercomputers by taking advantage of: \n\n\n\n\n\n\nApache Spark's potential to offer speed-up and advancements of nearly 1000x in-memory\n\n\n\n\n\n\nThe increasing growing community around Spark\n\n\n\n\n\n\nSpark's notion of Resilient Distributed Datasets(RDDs). RDDs represent immutable dataset that can be: \n\n\n\n\nreused across multi-stage operations.\n\n\npartitioned across multiple machines.\n\n\nautomatically reconstructed if a partition is lost.\n\n\n\n\nto address the pain points that scientists and researchers endure during model evaluation processes. \n\n\nExamples of these pain points include:\n\n\n\n\nTemporal and Zonal averaging of data\n\n\nComputation of climatologies\n\n\nPre-processing of CMIP data such as:\n\n\nRegridding \n\n\nVariable clustering (min/max)\n\n\ncalendar harmonizing\n\n\n\n\nWhat's Apache Spark?\n\n\n\n\n\n\nApache Spark is an open source cluster computing framework\n\n\n\n\n\n\nIt was developed at UCB AMPLab, 2014 v1.0, 2016 v.2\n\n\n\n\nActively developed, 1086 contributors, 19, 788 commits (As of June 5, 2017)\n\n\n\n\n\n\n\n\nProductive programming interface\n\n\n\n\n6 vs 28 lines of code compare to hadoop mapreduce\n\n\n\n\n\n\n\n\nImplicit data parallelism\n\n\n\n\nFault-tolerance\n\n\n\n\n\n\n\n\nSpark for Data-intensive Computing\n\n\n\n\nStreaming processing\n\n\nSQL\n\n\nMachine Learning, MLlib\n\n\nGraph Processing\n\n\n\n\n\n\n\n\nPorting Apache Spark software stack on HPC\n\n\n\n\n\n\nAdvantages of Porting Spark onto HPC\n\n\n\n\nA more productive API for data-intensive computing\n\n\nRelieve the users from:\n\n\nconcurrency control\n\n\ncommunication\n\n\nmemory management with traditional MPI model\n\n\n\n\n\n\nEmbarrassingly parallel computing, \ndata.map(func)\n\n\nFault tolerance, \nrecompute()\n\n\n\n\n\n\n\n\nHPC applications often rely on hierarchical data formats to organize files and datasets. \nHowever, accessing data stored in HDF/netCDF files is not natively supported in Spark.\n \n\n\n\n\n\n\nReasons of lack of netCDF/HDF support in Spark include:\n\n\n\n\nLack of an API in Spark to directly load or sub-select HDF/netCDF datasets into its in-memory data structures.\n\n\nHDF/netCDF have a deep hierarchy, which cannot simply be treated as a sequence of bytes nor be evenly divided.\n\n\nGPFS file system is not well tuned for SPARK I/O and vice versa.\n\n\n\n\n\n\n\n\nData in Spark\n\n\nResilient Distributed Datasets(RDDs)\n are:\n\n\n\n\n\n\nThe primary abstraction in Spark\n\n\n\n\nImmutable(Read-Only) once constructed\n\n\nSpark tracks lineage information to efficiently recompute lost data\n\n\nEnable operations on collection of elements in parallel\n\n\n\n\n\n\n\n\nYou construct RDDs\n\n\n\n\nby parallelizing existing Python collections (lists)\n\n\nby transforming an existing RDDs\n\n\nfrom files in HDFS or any other storage system (glade in case of Yellowstone and Cheyenne)\n\n\n\n\n\n\n\n\nThe programmer needs to specify the number of partitions for an RDD or the default value is used if unspecified.\n\n\n\n\n\n\n\n\nImage Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark\n\n\nThere are two types of operations on RDDs: \nTransformations\n and \nActions\n.\n\n\n\n\nTransformations\n are lazy in a sense that they are not computed immediately\n\n\nTransformed RDD is executed when action runs on it.\n\n\nRDDs can be persisted(cached) in memory or disk.\n\n\n\n\nWorking with RDDs\n:\n\n\n\n\nCreate an RDD from a data source\n\n\nApply transformations to an RDD: \n.map\n(...)\n\n\nApply actions to an RDD: \n.collect\n()\n, \n.count\n()\n\n\n\n\n\n\n\n\nImage Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark\n\n\nData in HDF/netCDF\n\n\n\n\nHierarchical Data Format v5\n\n\n\n\n\nSupport HDF/netCDF in Spark 2.x Series\n\n\n\n\n\n\nWhat does Spark have in reading various data formats?\n\n\n\n\nTextfile: \nsc.textFile()\n or \nspark.read.load(\nfilename.txt\n, format=\ntxt\n)\n\n\nParquet: \nspark.read.load(\nfilename.parquet\n, format=\nparquet\n)\n\n\nJson: \nspark.read.load(\nfilename.json\n, format=\njson\n)\n\n\ncsv: \nspark.read.load(\nfilename.csv\n, format=\ncsv)\n\n\njdbc: \nspark.read.load(\nfilename.jdbc\n, format=\njdbc\n)\n\n\norc, libsvm follow the same pattern.\n\n\n\n\n\n\n\n\nHow about HDF5/netCDF4?:\n\n\n\n\nThere is no such thing as \nspark.read.load(\nfilename.hdf5\n, format=\nhdf5\n)\n or \nspark.read.load(\nfilename.nc\n, format=\nnc\n)\n\n\n\n\n\n\n\n\nChallenges: Functionality and Performance\n\n\n\n\nHow to transform an netCDF dataset into an RDD?\n\n\nHow to utilize the netCDF I/O libraries in Spark?\n\n\nHow to enable parallel I/O on HPC?\n\n\nWhat is the impact of a parallel filesytem striping like \nGPFS\n?\n\n\nWhat is the effect of Caching on I/O in Spark?\n\n\n\n\n\n\n\n\nAny Solution?\n\n\n\n\nPySpark4Climate Package\n is a high level library for parsing netCDF data with Apache Spark, for Spark SQL and DataFrames.", 
            "title": "Home"
        }, 
        {
            "location": "/#pyspark-for-big-atmospheric-oceanic-data-analysis", 
            "text": "", 
            "title": "PySpark for \"Big\" Atmospheric &amp; Oceanic Data Analysis"
        }, 
        {
            "location": "/#background", 
            "text": "Climate and Weather directly impact all aspects of society. Understanding these processeses provide important information for policy - and decision - makers. To understand the average climate conditions and extreme weather events, Earth science and climate change researchers need data from climate models and observations. As such in the typical work flow of climate change and atmospheric research, much time is wasted waiting to reformat and regrid data to homogeneous formats. Additionally, because of the data volume, in order to compute metrics, perform analysis, and visualize data / generate plots, multi-stage processes with repeated I/O are used - the root cause of performance bottlenecks and the resulting user\u2019s frustrations related to time inefficiencies. NASA-SciSpark project   1 : Spark is a cluster computing paradigm based on the MapReduce paradigm that has garnered many scientific analysis workflows and are very well suited for Spark.  As a result of this lack of great deal of interest for its power and ease of use in analyzing \u201cbig data\u201d in the commercial and computer science sectors.  In much of the scientific sector, however --- and specifically in the atmospheric and oceanic sciences --- Spark has not captured the interest of scientists for analyzing their data, even though their datasets may be larger than many commercial datasets interest, there are very few platforms on which scientists can experiment with and learn about using Hadoop and/or Spark for their scientific research.  Additionally, there are very few resources to teach and educate scientists on how or why to use Hadoop or Spark for their analysis.", 
            "title": "Background"
        }, 
        {
            "location": "/#goal", 
            "text": "PySpark for Big Atmospheric   Oceanic Data Analysis  is a  CISL/SIParCS research project  that seeks to explore the realm of distributed parallel computing on NCAR's Yellowstone and Cheyenne supercomputers by taking advantage of:     Apache Spark's potential to offer speed-up and advancements of nearly 1000x in-memory    The increasing growing community around Spark    Spark's notion of Resilient Distributed Datasets(RDDs). RDDs represent immutable dataset that can be:    reused across multi-stage operations.  partitioned across multiple machines.  automatically reconstructed if a partition is lost.   to address the pain points that scientists and researchers endure during model evaluation processes.   Examples of these pain points include:   Temporal and Zonal averaging of data  Computation of climatologies  Pre-processing of CMIP data such as:  Regridding   Variable clustering (min/max)  calendar harmonizing", 
            "title": "Goal"
        }, 
        {
            "location": "/#whats-apache-spark", 
            "text": "Apache Spark is an open source cluster computing framework    It was developed at UCB AMPLab, 2014 v1.0, 2016 v.2   Actively developed, 1086 contributors, 19, 788 commits (As of June 5, 2017)     Productive programming interface   6 vs 28 lines of code compare to hadoop mapreduce     Implicit data parallelism   Fault-tolerance     Spark for Data-intensive Computing   Streaming processing  SQL  Machine Learning, MLlib  Graph Processing", 
            "title": "What's Apache Spark?"
        }, 
        {
            "location": "/#porting-apache-spark-software-stack-on-hpc", 
            "text": "Advantages of Porting Spark onto HPC   A more productive API for data-intensive computing  Relieve the users from:  concurrency control  communication  memory management with traditional MPI model    Embarrassingly parallel computing,  data.map(func)  Fault tolerance,  recompute()     HPC applications often rely on hierarchical data formats to organize files and datasets.  However, accessing data stored in HDF/netCDF files is not natively supported in Spark.      Reasons of lack of netCDF/HDF support in Spark include:   Lack of an API in Spark to directly load or sub-select HDF/netCDF datasets into its in-memory data structures.  HDF/netCDF have a deep hierarchy, which cannot simply be treated as a sequence of bytes nor be evenly divided.  GPFS file system is not well tuned for SPARK I/O and vice versa.", 
            "title": "Porting Apache Spark software stack on HPC"
        }, 
        {
            "location": "/#data-in-spark", 
            "text": "Resilient Distributed Datasets(RDDs)  are:    The primary abstraction in Spark   Immutable(Read-Only) once constructed  Spark tracks lineage information to efficiently recompute lost data  Enable operations on collection of elements in parallel     You construct RDDs   by parallelizing existing Python collections (lists)  by transforming an existing RDDs  from files in HDFS or any other storage system (glade in case of Yellowstone and Cheyenne)     The programmer needs to specify the number of partitions for an RDD or the default value is used if unspecified.     Image Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark  There are two types of operations on RDDs:  Transformations  and  Actions .   Transformations  are lazy in a sense that they are not computed immediately  Transformed RDD is executed when action runs on it.  RDDs can be persisted(cached) in memory or disk.   Working with RDDs :   Create an RDD from a data source  Apply transformations to an RDD:  .map (...)  Apply actions to an RDD:  .collect () ,  .count ()     Image Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark", 
            "title": "Data in Spark"
        }, 
        {
            "location": "/#data-in-hdfnetcdf", 
            "text": "Hierarchical Data Format v5", 
            "title": "Data in HDF/netCDF"
        }, 
        {
            "location": "/#support-hdfnetcdf-in-spark-2x-series", 
            "text": "What does Spark have in reading various data formats?   Textfile:  sc.textFile()  or  spark.read.load( filename.txt , format= txt )  Parquet:  spark.read.load( filename.parquet , format= parquet )  Json:  spark.read.load( filename.json , format= json )  csv:  spark.read.load( filename.csv , format= csv)  jdbc:  spark.read.load( filename.jdbc , format= jdbc )  orc, libsvm follow the same pattern.     How about HDF5/netCDF4?:   There is no such thing as  spark.read.load( filename.hdf5 , format= hdf5 )  or  spark.read.load( filename.nc , format= nc )     Challenges: Functionality and Performance   How to transform an netCDF dataset into an RDD?  How to utilize the netCDF I/O libraries in Spark?  How to enable parallel I/O on HPC?  What is the impact of a parallel filesytem striping like  GPFS ?  What is the effect of Caching on I/O in Spark?     Any Solution?   PySpark4Climate Package  is a high level library for parsing netCDF data with Apache Spark, for Spark SQL and DataFrames.", 
            "title": "Support HDF/netCDF in Spark 2.x Series"
        }, 
        {
            "location": "/installation/yellowstone/", 
            "text": "Apache Spark 2.1.1 on Yellowstone\n\n\nThe first task of our research project was to install the newest version of Apache Spark on Yellowstone. \n\n\nAt the time (May/2017), there was an old installation of Apache Spark (Spark 1.6) on Yellowstone. This installation was done by Davide Del Vento.\n\n\nAs of today (June/2017), we've been able to install the newest version of Apache Spark - \nSpark 2.1.1+Hadoop2.7\n.\n\n\nBelow are the steps that we took to get Spark Up and Running on Yellowstone.\n\n\nIf all you need is to run the existing Apache Spark on Yellowstone, just skip to \nSection 2\n of this page.\n\n\n1. Installation\n\n\n1.1 Downloading Spark and Setting up Spark's directory and necessary files\n\n\n\n\nLog into Yellowstone\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nGo to \nApache Spark's official website\n and follow steps 1-4 to get a download link for Spark. Copy the download link and \n\n\n\n\n\n\nGo to \n/glade/p/work/abanihi/\n and download Spark\n\n\n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz\n\n\nUntar \nspark-2.1.1-bin-hadoop2.7.tgz\n with \n\n\ntar -xzf spark-2.1.1-bin-hadoop2.7.tgz\n\n\n\n\n\n\n\n\n\n\n\n\nChange directory to \nspark-2.1.1-bin-hadoop2.7/conf\n and open \nlog4j.properties.template\n\n\n\n\ncd spark-2.1.1-bin-hadoop2.7/conf\n\n\nnano log4j.properties.template\n\n\nGo to the following line: \nlog4j.rootCategory=INFO, console\n and change \nINFO\n to \nERROR\n\n\nExit out of the editor\n\n\n\n\n\n\n\n\nRename \nlog4j.properties.template\n to \nlog4j.properties\n\n\n\n\nmv log4j.properties.template log4.properties\n\n\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nDownload H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this \npage\n.\n\n\n\n\ngit clone https://github.com/valiantljk/h5spark.git\n\n\nThis package will be added to \nPython Path\n in \nspark-cluster.sh\n script.\n\n\n\n\n\n\n\n\n1.2 Scripts\n\n\nNote:\n The scripts in this directory are based on Davide's previous scripts for Spark 1.6.\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi\n\n\n\n\n\n\n\n\nCreate a new directory called \nyellowstone\n and move into it \n\n\n\n\nmkdir yellowstone\n\n\n`cd yellowstone\n\n\nCreate \nspark-cluster-scripts\ndirectory and move into it\n\n\nmkdir spark-cluster-scripts\n\n\ncd spark-cluster-scripts/\n\n\n\n\n\n\n\n\nCreate a new script and name it \nspark-env.sh\n\n\n\n\nnano spark-env.sh\n\n\nspark-env.sh\n should have the following content \n\n\n\n\n\n\n\n\n#!/usr/bin/env bash\n\n\n\nsource\n /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu/5.3.0\nmodule load java\nmodule load python all-python-libs\nmodule load h5py\n\n\nexport\n \nSPARK_WORKER_DIR\n=\n/glade/scratch/\n$USER\n/spark/work\n\nexport\n \nSPARK_LOG_DIR\n=\n/glade/scratch/\n$USER\n/spark/logs\n\nexport\n \nSPARK_LOCAL_DIRS\n=\n/glade/scratch/\n$USER\n/spark/temp\n\nexport\n \nSPARK_LOCAL_IP\n=\n$(\nsed -e \ns/\\([^.]*\\).*$/\\1-ib/\n \n \n$(\nhostname\n))\n\n\n\n\n\n\n\nCreate a new script file and name it \nspark-cluster.sh\n\n\nnano spark-cluster.sh\n\n\n\n\n\n\n\n\n#!/usr/bin/env bash\n\n\nexport\n \nSPARK_HOME\n=\n/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/\n\nexport\n \nPYTHONPATH\n=\n$SPARK_HOME\n/python/:\n$PYTHONPATH\n\n\nexport\n \nPYTHONPATH\n=\n$PYTHONPATH\n:/glade/p/work/abanihi/pyspark4climate/\n\nexport\n \nSPARK_CONF_DIR\n=\n~/yellowstone/spark/conf\n\nexport\n \nSPARK_HOSTFILE\n=\n$SPARK_CONF_DIR\n/spark_hostfile\n\n\n# create temp hostfile\n\n\nexport\n \nSPARK_TEMP_HOSTFILE\n=\n$SPARK_CONF_DIR\n/spark_temp_hostfile\n\nrm \n$SPARK_HOSTFILE\n \n$SPARK_CONF_DIR\n/slaves\n\n\nmpirun.lsf hostname \n|\n grep -v Execute \n|\n sort \n \n$SPARK_TEMP_HOSTFILE\n\n\nsed -i \ns/$/-ib/\n \n$SPARK_TEMP_HOSTFILE\n\ncat \n$SPARK_TEMP_HOSTFILE\n \n|\n sort -u \n \n$SPARK_HOSTFILE\n\ntail -n +2 \n$SPARK_TEMP_HOSTFILE\n \n|\n sort -u \n \n$SPARK_CONF_DIR\n/slaves\ntail -n +2 \n$SPARK_TEMP_HOSTFILE\n \n|\n uniq -c \n \n$SPARK_CONF_DIR\n/temp_ncores_slaves\n\nrm \n$SPARK_TEMP_HOSTFILE\n\n\n\n\nexport\n \nSPARK_MASTER_HOST\n=\n$(\nhead -n \n1\n \n$SPARK_HOSTFILE\n)\n\n\nexport\n \nMASTER\n=\nspark://\n$SPARK_MASTER_HOST\n:7077\n\ncp ~/yellowstone/spark/spark-cluster-scripts/spark-env.sh \n$SPARK_CONF_DIR\n/spark-env.sh\n\nsource\n \n$SPARK_CONF_DIR\n/spark-env.sh\n\n\nif\n \n[\n \n$1\n \n==\n \nstart\n \n]\n;\n \nthen\n\n    \ncmd_master\n=\n$SPARK_HOME\n/sbin/start-master.sh\n\n    \ncmd_slave\n=\n$SPARK_HOME\n/sbin/spark-daemon.sh --config \n$SPARK_CONF_DIR\n start org.apac\n\n\nhe.spark.deploy.worker.Worker 1 \n$MASTER\n\n\nelif\n \n[\n \n$1\n \n==\n \nstop\n \n]\n;\n \nthen\n\n    \ncmd_master\n=\n$SPARK_HOME\n/sbin/stop-master.sh\n\n    \ncmd_slave\n=\n$SPARK_HOME\n/sbin/spark-daemon.sh --config \n$SPARK_CONF_DIR\n stop org.apach\n\n\ne.spark.deploy.worker.Worker 1\n\n\nelse\n\n    \nexit\n \n1\n\n\nfi\n\n\n\n$cmd_master\n\n\n\nwhile\n \nread\n ncore_slave\n\ndo\n\n    \nncore\n=\n$(\necho\n \n$ncore_slave\n \n|\n cut -d\n \n -f1\n)\n\n    \nslave\n=\n$(\necho\n \n$ncore_slave\n \n|\n cut -d\n \n -f2\n)\n\n\n    \nif\n \n[\n \n$slave\n \n==\n \n$SPARK_MASTER_HOST\n \n]\n;\n \nthen\n\n          \necho\n \nOn Master node.  Running: cmd_slave --cores \n$ncore\n\n          \n$cmd_slave\n --cores \n$ncore\n\n     \nelse\n\n          \necho\n \nOn Worker node.  Running: cmd_slave --cores \n$ncore\n\n          ssh \n$slave\n \n$cmd_slave\n --cores \n$ncore\n \n/dev/null \n\n    \nfi\n\n\ndone\n \n$SPARK_CONF_DIR\n/temp_ncores_slaves\n\n\n\n\n\n\nCreate a new script file and name it \nstart-pyspark.sh\n\n\n\n\n#!/usr/bin/env bash\n\n\n\nsource\n ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start\n\n\n$SPARK_HOME\n/bin/pyspark --master \n$MASTER\n\n\n\n\n\n\n\nCreate a new script file and name it \nstart-sparknotebook\n. This script file is an extension of \n/glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook\n script file.\n\n\n\n\n#!/usr/bin/env bash\n\n\n\n#source spark-cluster.sh start\n\n\nsource\n ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start\n\n\n# Add the PySpark classes to the Python path:\n\n\nexport\n \nPATH\n=\n$SPARK_HOME\n:\n$PATH\n\n\nexport\n \nPATH\n=\n$PATH\n:\n$SPARK_HOME\n/bin\n\nexport\n \nPYTHONPATH\n=\n$SPARK_HOME\n/python/:\n$PYTHONPATH\n\n\nexport\n \nPYTHONPATH\n=\n$SPARK_HOME\n/python/lib/py4j-0.10.4-src.zip:\n$PYTHONPATH\n\n\n\n# Create trap to kill notebook when user is done\n\nkill_server\n()\n \n{\n\n    \nif\n \n[[\n \n$JNPID\n !\n=\n -1 \n]]\n;\n \nthen\n\n        \necho\n -en \n\\nKilling Jupyter Notebook Server with PID=\n$JNPID\n ... \n\n        \nkill\n \n$JNPID\n\n        \necho\n \ndone\n\n        \nexit\n \n0\n\n    \nelse\n\n        \nexit\n \n1\n\n    \nfi\n\n\n}\n\n\n\nJNPID\n=\n-1\n\ntrap\n kill_server SIGHUP SIGINT SIGTERM\n\n\n# Begin server creation\n\n\nJNHOST\n=\n$(\nhostname\n)\n\n\nLOGDIR\n=\n/glade/scratch/\n${\nUSER\n}\n/.jupyter-notebook\n\nLOGFILE\n=\n${\nLOGDIR\n}\n/log.\n$(\ndate +%Y%m%dT%H%M%S\n)\n\nmkdir -p \n$LOGDIR\n\n\n\nif\n \n[[\n \n$JNHOST\n \n==\n ch* \n||\n \n$JNHOST\n \n==\n r* \n]]\n;\n \nthen\n\n    \nSTHOST\n=\ncheyenne\n\nelse\n\n    \nSTHOST\n=\nyellowstone\n\nfi\n\n\n\necho\n \nLogging this session in \n$LOGFILE\n\n\n\n# Check if running on login nodes\n\n\nif\n \n[[\n \n$JNHOST\n \n==\n yslogin* \n]]\n;\n \nthen\n\ncat \n EOF\n\n\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\n\n\nnode: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.\n\n\n\nConsider running on Geyser instead by using execgy to start a session. (Run execgy -hel\n\n\np.)\n\n\nEOF\n\n\nelif\n \n[[\n \n$JNHOST\n \n==\n cheyenne* \n]]\n;\n \nthen\n\ncat \n EOF\n\n\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\n\n\nnode: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.\n\n\n\nConsider running in an interactive job instead by using qinteractive. (Run qinterative\n\n\n-help.)\n\n\nEOF\n\n\nfi\n\n\n\njupyter notebook \n$@\n --no-browser --ip\n=\n$JNHOST\n \n$LOGFILE\n \n2\n1\n \n\n\nJNPID\n=\n$!\n\n\n\n\necho\n -en  \n\\nStarting jupyter notebook server, please wait ... \n\n\n\nELAPSED\n=\n0\n\n\nADDRESS\n=\n\n\n\nwhile\n \n[[\n \n$ADDRESS\n !\n=\n *\n${\nJNHOST\n}\n* \n]]\n;\n \ndo\n\n    sleep \n1\n\n    \nELAPSED\n=\n$((\nELAPSED+1\n))\n\n    \nADDRESS\n=\n$(\ntail -n \n1\n \n$LOGFILE\n)\n\n\n    \nif\n \n[[\n \n$ELAPSED\n -gt \n30\n \n]]\n;\n \nthen\n\n        \necho\n -e \nsomething went wrong\\n---\n\n        cat \n$LOGFILE\n\n        \necho\n \n---\n\n\n        kill_server\n    \nfi\n\n\ndone\n\n\n\necho\n -e \ndone\\n---\\n\n\n\n\nADDRESS\n=\n${\nADDRESS\n##*:\n}\n\n\nPORT\n=\n${\nADDRESS\n%/*\n}\n\n\nTOKEN\n=\n${\nADDRESS\n#*=\n}\n\n\ncat \n EOF\n\n\nRun the following command on your desktop or laptop:\n\n\n\n   ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu\n\n\n\nLog in with your YubiKey/Cryptocard (there will be no prompt).\n\n\nThen open a browser and go to http://localhost:8888. The Jupyter web\n\n\ninterface will ask you for a token. Use the following:\n\n\n\n    $TOKEN\n\n\n\nNote that anyone to whom you give the token can access (and modify/delete)\n\n\nfiles in your GLADE spaces, regardless of the file permissions you\n\n\nhave set. SHARE TOKENS RARELY AND WISELY!\n\n\n\nTo stop the server, press Ctrl-C.\n\n\nEOF\n\n\n\n# Wait for user kill command\n\nsleep inf\n\n\n\n\nNote:\n Make the above scripts executable (\nchmod +x script_name.sh\n)", 
            "title": "Yellowstone"
        }, 
        {
            "location": "/installation/yellowstone/#apache-spark-211-on-yellowstone", 
            "text": "The first task of our research project was to install the newest version of Apache Spark on Yellowstone.   At the time (May/2017), there was an old installation of Apache Spark (Spark 1.6) on Yellowstone. This installation was done by Davide Del Vento.  As of today (June/2017), we've been able to install the newest version of Apache Spark -  Spark 2.1.1+Hadoop2.7 .  Below are the steps that we took to get Spark Up and Running on Yellowstone.  If all you need is to run the existing Apache Spark on Yellowstone, just skip to  Section 2  of this page.", 
            "title": "Apache Spark 2.1.1 on Yellowstone"
        }, 
        {
            "location": "/installation/yellowstone/#1-installation", 
            "text": "", 
            "title": "1. Installation"
        }, 
        {
            "location": "/installation/yellowstone/#11-downloading-spark-and-setting-up-sparks-directory-and-necessary-files", 
            "text": "Log into Yellowstone  Change working directory to  /glade/p/work/abanihi  cd /glade/p/work/abanihi/     Go to  Apache Spark's official website  and follow steps 1-4 to get a download link for Spark. Copy the download link and     Go to  /glade/p/work/abanihi/  and download Spark   wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz  Untar  spark-2.1.1-bin-hadoop2.7.tgz  with   tar -xzf spark-2.1.1-bin-hadoop2.7.tgz       Change directory to  spark-2.1.1-bin-hadoop2.7/conf  and open  log4j.properties.template   cd spark-2.1.1-bin-hadoop2.7/conf  nano log4j.properties.template  Go to the following line:  log4j.rootCategory=INFO, console  and change  INFO  to  ERROR  Exit out of the editor     Rename  log4j.properties.template  to  log4j.properties   mv log4j.properties.template log4.properties     Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi/     Download H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this  page .   git clone https://github.com/valiantljk/h5spark.git  This package will be added to  Python Path  in  spark-cluster.sh  script.", 
            "title": "1.1 Downloading Spark and Setting up Spark's directory and necessary files"
        }, 
        {
            "location": "/installation/yellowstone/#12-scripts", 
            "text": "Note:  The scripts in this directory are based on Davide's previous scripts for Spark 1.6.    Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi     Create a new directory called  yellowstone  and move into it    mkdir yellowstone  `cd yellowstone  Create  spark-cluster-scripts directory and move into it  mkdir spark-cluster-scripts  cd spark-cluster-scripts/     Create a new script and name it  spark-env.sh   nano spark-env.sh  spark-env.sh  should have the following content      #!/usr/bin/env bash  source  /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu/5.3.0\nmodule load java\nmodule load python all-python-libs\nmodule load h5py export   SPARK_WORKER_DIR = /glade/scratch/ $USER /spark/work export   SPARK_LOG_DIR = /glade/scratch/ $USER /spark/logs export   SPARK_LOCAL_DIRS = /glade/scratch/ $USER /spark/temp export   SPARK_LOCAL_IP = $( sed -e  s/\\([^.]*\\).*$/\\1-ib/     $( hostname ))    Create a new script file and name it  spark-cluster.sh  nano spark-cluster.sh     #!/usr/bin/env bash  export   SPARK_HOME = /glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/ export   PYTHONPATH = $SPARK_HOME /python/: $PYTHONPATH  export   PYTHONPATH = $PYTHONPATH :/glade/p/work/abanihi/pyspark4climate/ export   SPARK_CONF_DIR = ~/yellowstone/spark/conf export   SPARK_HOSTFILE = $SPARK_CONF_DIR /spark_hostfile # create temp hostfile  export   SPARK_TEMP_HOSTFILE = $SPARK_CONF_DIR /spark_temp_hostfile\n\nrm  $SPARK_HOSTFILE   $SPARK_CONF_DIR /slaves\n\n\nmpirun.lsf hostname  |  grep -v Execute  |  sort    $SPARK_TEMP_HOSTFILE \n\nsed -i  s/$/-ib/   $SPARK_TEMP_HOSTFILE \ncat  $SPARK_TEMP_HOSTFILE   |  sort -u    $SPARK_HOSTFILE \ntail -n +2  $SPARK_TEMP_HOSTFILE   |  sort -u    $SPARK_CONF_DIR /slaves\ntail -n +2  $SPARK_TEMP_HOSTFILE   |  uniq -c    $SPARK_CONF_DIR /temp_ncores_slaves\n\nrm  $SPARK_TEMP_HOSTFILE  export   SPARK_MASTER_HOST = $( head -n  1   $SPARK_HOSTFILE )  export   MASTER = spark:// $SPARK_MASTER_HOST :7077\n\ncp ~/yellowstone/spark/spark-cluster-scripts/spark-env.sh  $SPARK_CONF_DIR /spark-env.sh source   $SPARK_CONF_DIR /spark-env.sh if   [   $1   ==   start   ] ;   then \n     cmd_master = $SPARK_HOME /sbin/start-master.sh \n     cmd_slave = $SPARK_HOME /sbin/spark-daemon.sh --config  $SPARK_CONF_DIR  start org.apac  he.spark.deploy.worker.Worker 1  $MASTER  elif   [   $1   ==   stop   ] ;   then \n     cmd_master = $SPARK_HOME /sbin/stop-master.sh \n     cmd_slave = $SPARK_HOME /sbin/spark-daemon.sh --config  $SPARK_CONF_DIR  stop org.apach  e.spark.deploy.worker.Worker 1  else \n     exit   1  fi  $cmd_master  while   read  ncore_slave do \n     ncore = $( echo   $ncore_slave   |  cut -d    -f1 ) \n     slave = $( echo   $ncore_slave   |  cut -d    -f2 ) \n\n     if   [   $slave   ==   $SPARK_MASTER_HOST   ] ;   then \n           echo   On Master node.  Running: cmd_slave --cores  $ncore \n           $cmd_slave  --cores  $ncore \n      else \n           echo   On Worker node.  Running: cmd_slave --cores  $ncore \n          ssh  $slave   $cmd_slave  --cores  $ncore   /dev/null  \n     fi  done   $SPARK_CONF_DIR /temp_ncores_slaves   Create a new script file and name it  start-pyspark.sh   #!/usr/bin/env bash  source  ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start $SPARK_HOME /bin/pyspark --master  $MASTER    Create a new script file and name it  start-sparknotebook . This script file is an extension of  /glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook  script file.   #!/usr/bin/env bash  #source spark-cluster.sh start  source  ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start # Add the PySpark classes to the Python path:  export   PATH = $SPARK_HOME : $PATH  export   PATH = $PATH : $SPARK_HOME /bin export   PYTHONPATH = $SPARK_HOME /python/: $PYTHONPATH  export   PYTHONPATH = $SPARK_HOME /python/lib/py4j-0.10.4-src.zip: $PYTHONPATH  # Create trap to kill notebook when user is done \nkill_server ()   { \n     if   [[   $JNPID  ! =  -1  ]] ;   then \n         echo  -en  \\nKilling Jupyter Notebook Server with PID= $JNPID  ...  \n         kill   $JNPID \n         echo   done \n         exit   0 \n     else \n         exit   1 \n     fi  }  JNPID = -1 trap  kill_server SIGHUP SIGINT SIGTERM # Begin server creation  JNHOST = $( hostname )  LOGDIR = /glade/scratch/ ${ USER } /.jupyter-notebook LOGFILE = ${ LOGDIR } /log. $( date +%Y%m%dT%H%M%S ) \nmkdir -p  $LOGDIR  if   [[   $JNHOST   ==  ch*  ||   $JNHOST   ==  r*  ]] ;   then \n     STHOST = cheyenne else \n     STHOST = yellowstone fi  echo   Logging this session in  $LOGFILE  # Check if running on login nodes  if   [[   $JNHOST   ==  yslogin*  ]] ;   then \ncat   EOF  See  Use of login nodes  here before running Jupyter Notebook on this  node: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.  Consider running on Geyser instead by using execgy to start a session. (Run execgy -hel  p.)  EOF  elif   [[   $JNHOST   ==  cheyenne*  ]] ;   then \ncat   EOF  See  Use of login nodes  here before running Jupyter Notebook on this  node: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.  Consider running in an interactive job instead by using qinteractive. (Run qinterative  -help.)  EOF  fi \n\n\njupyter notebook  $@  --no-browser --ip = $JNHOST   $LOGFILE   2 1    JNPID = $!  echo  -en   \\nStarting jupyter notebook server, please wait ...   ELAPSED = 0  ADDRESS =  while   [[   $ADDRESS  ! =  * ${ JNHOST } *  ]] ;   do \n    sleep  1 \n     ELAPSED = $(( ELAPSED+1 )) \n     ADDRESS = $( tail -n  1   $LOGFILE ) \n\n     if   [[   $ELAPSED  -gt  30   ]] ;   then \n         echo  -e  something went wrong\\n--- \n        cat  $LOGFILE \n         echo   --- \n\n        kill_server\n     fi  done  echo  -e  done\\n---\\n  ADDRESS = ${ ADDRESS ##*: }  PORT = ${ ADDRESS %/* }  TOKEN = ${ ADDRESS #*= } \n\ncat   EOF  Run the following command on your desktop or laptop:     ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu  Log in with your YubiKey/Cryptocard (there will be no prompt).  Then open a browser and go to http://localhost:8888. The Jupyter web  interface will ask you for a token. Use the following:      $TOKEN  Note that anyone to whom you give the token can access (and modify/delete)  files in your GLADE spaces, regardless of the file permissions you  have set. SHARE TOKENS RARELY AND WISELY!  To stop the server, press Ctrl-C.  EOF  # Wait for user kill command \nsleep inf  Note:  Make the above scripts executable ( chmod +x script_name.sh )", 
            "title": "1.2 Scripts"
        }, 
        {
            "location": "/installation/cheyenne/", 
            "text": "Apache Spark 2.1.1 on Cheyenne\n\n\nThe Second task of our research project was to install the newest version of Apache Spark on Cheyenne. \n\n\nAs of today (June/2017), we've been able to install the newest version of Apache Spark - \nSpark 2.1.1+Hadoop2.7\n.\n\n\nBelow are the steps that we took to get Spark Up and Running on Cheyenne.\n\n\nIf all you need is to run the existing Apache Spark on Cheyenne, just skip to \nSection 2\n of this page.\n\n\n1. Installation\n\n\n1.1 Downloading Spark and Setting up Spark's directory and necessary files\n\n\nThe steps described in this section are similar to those for Yellowstone. Since both Yellowstone and Cheyenne have access to the same parallel filesytem, we decide to use the same downloaded Spark binaries.\n\n\nThe following section gives details on how Apache Spark would be installed on Cheyenne.\n\n\n\n\nLog into Cheyenne\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nGo to \nApache Spark's official website\n and follow steps 1-4 to get a download link for Spark. Copy the download link and \n\n\n\n\n\n\nGo to \n/glade/p/work/abanihi/\n and download Spark\n\n\n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz\n\n\nUntar \nspark-2.1.1-bin-hadoop2.7.tgz\n with \n\n\ntar -xzf spark-2.1.1-bin-hadoop2.7.tgz\n\n\n\n\n\n\n\n\n\n\n\n\nChange directory to \nspark-2.1.1-bin-hadoop2.7/conf\n and open \nlog4j.properties.template\n\n\n\n\ncd spark-2.1.1-bin-hadoop2.7/conf\n\n\nnano log4j.properties.template\n\n\nGo to the following line: \nlog4j.rootCategory=INFO, console\n and change \nINFO\n to \nERROR\n\n\nExit out of the editor\n\n\n\n\n\n\n\n\nRename \nlog4j.properties.template\n to \nlog4j.properties\n\n\n\n\nmv log4j.properties.template log4.properties\n\n\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nDownload H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this \npage\n.\n\n\n\n\ngit clone https://github.com/valiantljk/h5spark.git\n\n\nThis package will be added to \nPython Path\n in \nspark-cluster.sh\n script.\n\n\n\n\n\n\n\n\n1.2 Scripts\n\n\nEven though the scripts for Yellowstone and Cheyenne have so much in common, there are some differences.\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi\n\n\n\n\n\n\n\n\nCreate a new directory called \ncheyenne\n and move into it \n\n\n\n\nmkdir cheyenne\n\n\n`cd cheyenne\n\n\nCreate \nspark-cluster-scripts\ndirectory and move into it\n\n\nmkdir spark-cluster-scripts\n\n\ncd spark-cluster-scripts/\n\n\n\n\n\n\n\n\nCreate a new script and name it \nspark-env.sh\n\n\n\n\nnano spark-env.sh\n\n\nspark-env.sh\n should have the following content \n\n\n\n\n\n\n\n\n##!/usr/bin/env bash\n\n\n\nsource\n /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu\n\nexport\n \nMODULEPATH\n=\n/glade/p/work/bdobbins/Modules:\n${\nMODULEPATH\n}\n\nmodule load java\nml python\nml numpy\nml jupyter\nml scipy\nml h5py\nml bottleneck\nml numexpr\nml pandas\nml pyside\nml matplotlib\nml pyngl\nml scikit-learn\nml netcdf4-python\nml cf_units\nml xarray\n\n\nexport\n \nSPARK_WORKER_DIR\n=\n/glade/scratch/\n$USER\n/spark/work\n\nexport\n \nSPARK_LOG_DIR\n=\n/glade/scratch/\n$USER\n/spark/logs\n\nexport\n \nSPARK_LOCAL_DIRS\n=\n/glade/scratch/\n$USER\n/spark/temp\n\nexport\n \nSPARK_LOCAL_IP\n=\n$(\nsed -e \ns/\\([^.]*\\).*$/\\1/\n \n \n$(\nhostname\n))\n\n\n\n\n\n\n\nCreate a new script file and name it \nspark-cluster.sh\n\n\nnano spark-cluster.sh\n\n\n\n\n\n\n\n\n#!/usr/bin/env bash\n\n\nexport\n \nSPARK_HOME\n=\n/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/\n\nexport\n \nPYTHONPATH\n=\n$SPARK_HOME\n/python/:\n$PYTHONPATH\n\n\nexport\n \nPYTHONPATH\n=\n$PYTHONPATH\n:/glade/p/work/abanihi/h5spark/src/main/python/h5spark/:\n$PYTHONPATH\n\n\nexport\n \nSPARK_CONF_DIR\n=\n~/cheyenne/spark/conf\n\nexport\n \nSPARK_HOSTFILE\n=\n$SPARK_CONF_DIR\n/spark_hostfile\n\n\n# create temp hostfile\n\n\nexport\n \nSPARK_TEMP_HOSTFILE\n=\n$SPARK_CONF_DIR\n/spark_temp_hostfile\n\nrm \n$SPARK_HOSTFILE\n \n$SPARK_CONF_DIR\n/slaves\n\n\nexport\n \nMPI_SHEPHERD\n=\ntrue\n\nmpiexec_mpt hostname \n|\n grep -v Execute \n|\n sort \n \n$SPARK_TEMP_HOSTFILE\n\n\n\n#sed -i \ns/$/-ib/\n $SPARK_TEMP_HOSTFILE\n\ncat \n$SPARK_TEMP_HOSTFILE\n \n|\n sort -u \n \n$SPARK_HOSTFILE\n\ntail -n +2 \n$SPARK_TEMP_HOSTFILE\n \n|\n sort -u \n \n$SPARK_CONF_DIR\n/slaves\ntail -n +2 \n$SPARK_TEMP_HOSTFILE\n \n|\n uniq -c \n temp_ncores_slaves\n\nrm \n$SPARK_TEMP_HOSTFILE\n\n\n\n\nexport\n \nSPARK_MASTER_HOST\n=\n$(\nhead -n \n1\n \n$SPARK_HOSTFILE\n)\n\n\nexport\n \nMASTER\n=\nspark://\n$SPARK_MASTER_HOST\n:7077\n\ncp spark-env.sh \n$SPARK_CONF_DIR\n/spark-env.sh\n\nsource\n \n$SPARK_CONF_DIR\n/spark-env.sh\n\n\nif\n \n[\n \n$1\n \n==\n \nstart\n \n]\n;\n \nthen\n\n    \ncmd_master\n=\n$SPARK_HOME\n/sbin/start-master.sh\n\n    \ncmd_slave\n=\n$SPARK_HOME\n/sbin/spark-daemon.sh --config \n$SPARK_CONF_DIR\n start org.apache.spark.deploy.worker.Worker 1 \n$MASTER\n\n\nelif\n \n[\n \n$1\n \n==\n \nstop\n \n]\n;\n \nthen\n\n    \ncmd_master\n=\n$SPARK_HOME\n/sbin/stop-master.sh\n\n    \ncmd_slave\n=\n$SPARK_HOME\n/sbin/spark-daemon.sh --config \n$SPARK_CONF_DIR\n stop org.apache.spark.deploy.worker.Worker 1\n\n\nelse\n\n    \nexit\n \n1\n\n\nfi\n\n\n\n$cmd_master\n\n\n\nwhile\n \nread\n ncore_slave\n\ndo\n\n    \nncore\n=\n$(\necho\n \n$ncore_slave\n \n|\n cut -d\n \n -f1\n)\n\n    \nslave\n=\n$(\necho\n \n$ncore_slave\n \n|\n cut -d\n \n -f2\n)\n\n\n    \nif\n \n[\n \n$slave\n \n==\n \n$SPARK_MASTER_HOST\n \n]\n;\n \nthen\n\n          \necho\n \nOn Master node.  Running: cmd_slave --cores \n$ncore\n\n          \n$cmd_slave\n --cores \n$ncore\n\n     \nelse\n\n          \necho\n \nOn Worker node.  Running: cmd_slave --cores \n$ncore\n\n          ssh \n$slave\n \n$cmd_slave\n --cores \n$ncore\n \n /dev/null\n\n    \nfi\n\n\ndone\n \ntemp_ncores_slaves\n\n\n\n\n\n\nCreate a new script file and name it \nstart-pyspark.sh\n\n\n\n\n#!/usr/bin/env bash\n\n\n\nsource\n spark-cluster.sh start\n\n$SPARK_HOME\n/bin/pyspark --master \n$MASTER\n\n\n\n\n\n\n\nCreate a new script file and name it \nstart-sparknotebook\n. This script file is an extension of \n/glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook\n script file.\n\n\n\n\n#!/usr/bin/env bash\n\n\n\nsource\n spark-cluster.sh start\n\n\n# Add the PySpark classes to the Python path:\n\n\nexport\n \nPATH\n=\n$SPARK_HOME\n:\n$PATH\n\n\nexport\n \nPATH\n=\n$PATH\n:\n$SPARK_HOME\n/bin\n\nexport\n \nPYTHONPATH\n=\n$SPARK_HOME\n/python/:\n$PYTHONPATH\n\n\nexport\n \nPYTHONPATH\n=\n$SPARK_HOME\n/python/lib/py4j-0.10.4-src.zip:\n$PYTHONPATH\n\n\n\n# Create trap to kill notebook when user is done\n\nkill_server\n()\n \n{\n\n    \nif\n \n[[\n \n$JNPID\n !\n=\n -1 \n]]\n;\n \nthen\n\n        \necho\n -en \n\\nKilling Jupyter Notebook Server with PID=\n$JNPID\n ... \n\n        \nkill\n \n$JNPID\n\n        \necho\n \ndone\n\n        \nexit\n \n0\n\n    \nelse\n\n        \nexit\n \n1\n\n    \nfi\n\n\n}\n\n\n\nJNPID\n=\n-1\n\ntrap\n kill_server SIGHUP SIGINT SIGTERM\n\n\n# Begin server creation\n\n\nJNHOST\n=\n$(\nhostname\n)\n\n\nLOGDIR\n=\n/glade/scratch/\n${\nUSER\n}\n/.jupyter-notebook\n\nLOGFILE\n=\n${\nLOGDIR\n}\n/log.\n$(\ndate +%Y%m%dT%H%M%S\n)\n\nmkdir -p \n$LOGDIR\n\n\n\nif\n \n[[\n \n$JNHOST\n \n==\n ch* \n||\n \n$JNHOST\n \n==\n r* \n]]\n;\n \nthen\n\n    \nSTHOST\n=\ncheyenne\n\nelse\n\n    \nSTHOST\n=\nyellowstone\n\nfi\n\n\n\necho\n \nLogging this session in \n$LOGFILE\n\n\n\n# Check if running on login nodes\n\n\nif\n \n[[\n \n$JNHOST\n \n==\n yslogin* \n]]\n;\n \nthen\n\ncat \n EOF\n\n\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\n\n\nnode: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.\n\n\n\nConsider running on Geyser instead by using execgy to start a session. (Run execgy -help.)\n\n\nEOF\n\n\nelif\n \n[[\n \n$JNHOST\n \n==\n cheyenne* \n]]\n;\n \nthen\n\ncat \n EOF\n\n\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\n\n\nnode: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.\n\n\n\nConsider running in an interactive job instead by using qinteractive. (Run qinterative -help.)\n\n\nEOF\n\n\nfi\n\n\n\njupyter notebook \n$@\n --no-browser --ip\n=\n$JNHOST\n \n$LOGFILE\n \n2\n1\n \n\n\nJNPID\n=\n$!\n\n\n\n\necho\n -en  \n\\nStarting jupyter notebook server, please wait ... \n\n\n\nELAPSED\n=\n0\n\n\nADDRESS\n=\n\n\n\nwhile\n \n[[\n \n$ADDRESS\n !\n=\n *\n${\nJNHOST\n}\n* \n]]\n;\n \ndo\n\n    sleep \n1\n\n    \nELAPSED\n=\n$((\nELAPSED+1\n))\n\n    \nADDRESS\n=\n$(\ntail -n \n1\n \n$LOGFILE\n)\n\n\n    \nif\n \n[[\n \n$ELAPSED\n -gt \n30\n \n]]\n;\n \nthen\n\n        \necho\n -e \nsomething went wrong\\n---\n\n        cat \n$LOGFILE\n\n        \necho\n \n---\n\n\n        kill_server\n    \nfi\n\n\ndone\n\n\n\necho\n -e \ndone\\n---\\n\n\n\n\nADDRESS\n=\n${\nADDRESS\n##*:\n}\n\n\nPORT\n=\n${\nADDRESS\n%/*\n}\n\n\nTOKEN\n=\n${\nADDRESS\n#*=\n}\n\n\ncat \n EOF\n\n\nRun the following command on your desktop or laptop:\n\n\n\n   ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu\n\n\n\nLog in with your YubiKey/Cryptocard (there will be no prompt).\n\n\nThen open a browser and go to http://localhost:8888. The Jupyter web\n\n\ninterface will ask you for a token. Use the following:\n\n\n\n    $TOKEN\n\n\n\nNote that anyone to whom you give the token can access (and modify/delete)\n\n\nfiles in your GLADE spaces, regardless of the file permissions you\n\n\nhave set. SHARE TOKENS RARELY AND WISELY!\n\n\n\nRun the following commands on your destop or latptop:\n\n\n\n   ssh -N -l $USER -L 8080:${JNHOST}:8080 ${STHOST}.ucar.edu\n\n\n   ssh -N -l $USER -L 4040:${JNHOST}:4040 ${STHOST}.ucar.edu\n\n\n\nLog in with your YubiKey/Cryptocard(there will be no prompt).\n\n\nThen open a browser and go to http://localhost:8080 to access the Spark Master UI.\n\n\n\nFinally open a browser and go to http://localhost:4040 to access the Spark Master UI jobs\n\n\nhistory.\n\n\n\nTo stop the server, press Ctrl-C.\n\n\nEOF\n\n\n\n# Wait for user kill command\n\nsleep inf\n\n\n\n\nNote:\n Make the above scripts executable by running (\nchmod +x script_name.sh\n)\n\n\n2. Running Existing Cheyenne Spark Installation\n\n\n\n\nLog into Cheyenne\n\n\n\n\nCreate a working directory in your home and move into it:\n\n\n\n\nmkdir cheyenne\n\n\n`cd cheyenne\n\n\nmkdir spark\n\n\ncd spark\n\n\n\n\n\n\n\n\nCopy \nspark-cluster-scripts\n directory to \nspark\n directory\n\n\n\n\ncp -r /glade/p/work/abanihi/cheyenne/spark-cluster-scripts .\n\n\n\n\n\n\n\n\nIn the created \nspark\n directory, create \nconf\n directory:\n\n\n\n\nmkdir conf\n\n\n\n\n\n\n\n\nSchedule your job to run on the Cheyenne, by submitting your job through \npbs scheduler\n\n\n\n\nExample: \nqsub -I -l select=4:ncpus=1:mpiprocs=1 -l walltime=00:30:00 -q regular -A ProjectID\n\n\n\n\n\n\n\n\nChange current directory to \nspark-cluster-scripts\n\n\n\n\ncd spark-cluster-scripts\n\n\n\n\n\n\n\n\n2.1. Run PySpark Shell\n\n\n\n\nTo run PySpark shell, run \nstart-pyspark.sh\n by running \n./start-pyspark.sh\n. You should get something similar to this:\n\n\n\n\n2.2. Run PySpark in a Jupyter notebook\n\n\n\n\n\n\nTo run PySpark in a Jupyter notebook, make sure you that your current directory is \nspark-cluster-scripts\n and \n\n\n\n\nrun \nstart-sparknotebook\n by typing \n./start-sparknotebook\n and follow the instructions given.\n\n\n\n\n\n\n\n\nThere are two notebooks in the \nspark-cluster-scripts/\n directory. Run the \nSpark-Essentials\n notebook to test that Spark is running and that you have access to a cluster of nodes.", 
            "title": "Cheyenne"
        }, 
        {
            "location": "/installation/cheyenne/#apache-spark-211-on-cheyenne", 
            "text": "The Second task of our research project was to install the newest version of Apache Spark on Cheyenne.   As of today (June/2017), we've been able to install the newest version of Apache Spark -  Spark 2.1.1+Hadoop2.7 .  Below are the steps that we took to get Spark Up and Running on Cheyenne.  If all you need is to run the existing Apache Spark on Cheyenne, just skip to  Section 2  of this page.", 
            "title": "Apache Spark 2.1.1 on Cheyenne"
        }, 
        {
            "location": "/installation/cheyenne/#1-installation", 
            "text": "", 
            "title": "1. Installation"
        }, 
        {
            "location": "/installation/cheyenne/#11-downloading-spark-and-setting-up-sparks-directory-and-necessary-files", 
            "text": "The steps described in this section are similar to those for Yellowstone. Since both Yellowstone and Cheyenne have access to the same parallel filesytem, we decide to use the same downloaded Spark binaries.  The following section gives details on how Apache Spark would be installed on Cheyenne.   Log into Cheyenne  Change working directory to  /glade/p/work/abanihi  cd /glade/p/work/abanihi/     Go to  Apache Spark's official website  and follow steps 1-4 to get a download link for Spark. Copy the download link and     Go to  /glade/p/work/abanihi/  and download Spark   wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz  Untar  spark-2.1.1-bin-hadoop2.7.tgz  with   tar -xzf spark-2.1.1-bin-hadoop2.7.tgz       Change directory to  spark-2.1.1-bin-hadoop2.7/conf  and open  log4j.properties.template   cd spark-2.1.1-bin-hadoop2.7/conf  nano log4j.properties.template  Go to the following line:  log4j.rootCategory=INFO, console  and change  INFO  to  ERROR  Exit out of the editor     Rename  log4j.properties.template  to  log4j.properties   mv log4j.properties.template log4.properties     Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi/     Download H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this  page .   git clone https://github.com/valiantljk/h5spark.git  This package will be added to  Python Path  in  spark-cluster.sh  script.", 
            "title": "1.1 Downloading Spark and Setting up Spark's directory and necessary files"
        }, 
        {
            "location": "/installation/cheyenne/#12-scripts", 
            "text": "Even though the scripts for Yellowstone and Cheyenne have so much in common, there are some differences.    Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi     Create a new directory called  cheyenne  and move into it    mkdir cheyenne  `cd cheyenne  Create  spark-cluster-scripts directory and move into it  mkdir spark-cluster-scripts  cd spark-cluster-scripts/     Create a new script and name it  spark-env.sh   nano spark-env.sh  spark-env.sh  should have the following content      ##!/usr/bin/env bash  source  /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu export   MODULEPATH = /glade/p/work/bdobbins/Modules: ${ MODULEPATH } \nmodule load java\nml python\nml numpy\nml jupyter\nml scipy\nml h5py\nml bottleneck\nml numexpr\nml pandas\nml pyside\nml matplotlib\nml pyngl\nml scikit-learn\nml netcdf4-python\nml cf_units\nml xarray export   SPARK_WORKER_DIR = /glade/scratch/ $USER /spark/work export   SPARK_LOG_DIR = /glade/scratch/ $USER /spark/logs export   SPARK_LOCAL_DIRS = /glade/scratch/ $USER /spark/temp export   SPARK_LOCAL_IP = $( sed -e  s/\\([^.]*\\).*$/\\1/     $( hostname ))    Create a new script file and name it  spark-cluster.sh  nano spark-cluster.sh     #!/usr/bin/env bash  export   SPARK_HOME = /glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/ export   PYTHONPATH = $SPARK_HOME /python/: $PYTHONPATH  export   PYTHONPATH = $PYTHONPATH :/glade/p/work/abanihi/h5spark/src/main/python/h5spark/: $PYTHONPATH  export   SPARK_CONF_DIR = ~/cheyenne/spark/conf export   SPARK_HOSTFILE = $SPARK_CONF_DIR /spark_hostfile # create temp hostfile  export   SPARK_TEMP_HOSTFILE = $SPARK_CONF_DIR /spark_temp_hostfile\n\nrm  $SPARK_HOSTFILE   $SPARK_CONF_DIR /slaves export   MPI_SHEPHERD = true \nmpiexec_mpt hostname  |  grep -v Execute  |  sort    $SPARK_TEMP_HOSTFILE  #sed -i  s/$/-ib/  $SPARK_TEMP_HOSTFILE \ncat  $SPARK_TEMP_HOSTFILE   |  sort -u    $SPARK_HOSTFILE \ntail -n +2  $SPARK_TEMP_HOSTFILE   |  sort -u    $SPARK_CONF_DIR /slaves\ntail -n +2  $SPARK_TEMP_HOSTFILE   |  uniq -c   temp_ncores_slaves\n\nrm  $SPARK_TEMP_HOSTFILE  export   SPARK_MASTER_HOST = $( head -n  1   $SPARK_HOSTFILE )  export   MASTER = spark:// $SPARK_MASTER_HOST :7077\n\ncp spark-env.sh  $SPARK_CONF_DIR /spark-env.sh source   $SPARK_CONF_DIR /spark-env.sh if   [   $1   ==   start   ] ;   then \n     cmd_master = $SPARK_HOME /sbin/start-master.sh \n     cmd_slave = $SPARK_HOME /sbin/spark-daemon.sh --config  $SPARK_CONF_DIR  start org.apache.spark.deploy.worker.Worker 1  $MASTER  elif   [   $1   ==   stop   ] ;   then \n     cmd_master = $SPARK_HOME /sbin/stop-master.sh \n     cmd_slave = $SPARK_HOME /sbin/spark-daemon.sh --config  $SPARK_CONF_DIR  stop org.apache.spark.deploy.worker.Worker 1  else \n     exit   1  fi  $cmd_master  while   read  ncore_slave do \n     ncore = $( echo   $ncore_slave   |  cut -d    -f1 ) \n     slave = $( echo   $ncore_slave   |  cut -d    -f2 ) \n\n     if   [   $slave   ==   $SPARK_MASTER_HOST   ] ;   then \n           echo   On Master node.  Running: cmd_slave --cores  $ncore \n           $cmd_slave  --cores  $ncore \n      else \n           echo   On Worker node.  Running: cmd_slave --cores  $ncore \n          ssh  $slave   $cmd_slave  --cores  $ncore    /dev/null\n\n     fi  done   temp_ncores_slaves   Create a new script file and name it  start-pyspark.sh   #!/usr/bin/env bash  source  spark-cluster.sh start $SPARK_HOME /bin/pyspark --master  $MASTER    Create a new script file and name it  start-sparknotebook . This script file is an extension of  /glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook  script file.   #!/usr/bin/env bash  source  spark-cluster.sh start # Add the PySpark classes to the Python path:  export   PATH = $SPARK_HOME : $PATH  export   PATH = $PATH : $SPARK_HOME /bin export   PYTHONPATH = $SPARK_HOME /python/: $PYTHONPATH  export   PYTHONPATH = $SPARK_HOME /python/lib/py4j-0.10.4-src.zip: $PYTHONPATH  # Create trap to kill notebook when user is done \nkill_server ()   { \n     if   [[   $JNPID  ! =  -1  ]] ;   then \n         echo  -en  \\nKilling Jupyter Notebook Server with PID= $JNPID  ...  \n         kill   $JNPID \n         echo   done \n         exit   0 \n     else \n         exit   1 \n     fi  }  JNPID = -1 trap  kill_server SIGHUP SIGINT SIGTERM # Begin server creation  JNHOST = $( hostname )  LOGDIR = /glade/scratch/ ${ USER } /.jupyter-notebook LOGFILE = ${ LOGDIR } /log. $( date +%Y%m%dT%H%M%S ) \nmkdir -p  $LOGDIR  if   [[   $JNHOST   ==  ch*  ||   $JNHOST   ==  r*  ]] ;   then \n     STHOST = cheyenne else \n     STHOST = yellowstone fi  echo   Logging this session in  $LOGFILE  # Check if running on login nodes  if   [[   $JNHOST   ==  yslogin*  ]] ;   then \ncat   EOF  See  Use of login nodes  here before running Jupyter Notebook on this  node: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.  Consider running on Geyser instead by using execgy to start a session. (Run execgy -help.)  EOF  elif   [[   $JNHOST   ==  cheyenne*  ]] ;   then \ncat   EOF  See  Use of login nodes  here before running Jupyter Notebook on this  node: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.  Consider running in an interactive job instead by using qinteractive. (Run qinterative -help.)  EOF  fi \n\n\njupyter notebook  $@  --no-browser --ip = $JNHOST   $LOGFILE   2 1    JNPID = $!  echo  -en   \\nStarting jupyter notebook server, please wait ...   ELAPSED = 0  ADDRESS =  while   [[   $ADDRESS  ! =  * ${ JNHOST } *  ]] ;   do \n    sleep  1 \n     ELAPSED = $(( ELAPSED+1 )) \n     ADDRESS = $( tail -n  1   $LOGFILE ) \n\n     if   [[   $ELAPSED  -gt  30   ]] ;   then \n         echo  -e  something went wrong\\n--- \n        cat  $LOGFILE \n         echo   --- \n\n        kill_server\n     fi  done  echo  -e  done\\n---\\n  ADDRESS = ${ ADDRESS ##*: }  PORT = ${ ADDRESS %/* }  TOKEN = ${ ADDRESS #*= } \n\ncat   EOF  Run the following command on your desktop or laptop:     ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu  Log in with your YubiKey/Cryptocard (there will be no prompt).  Then open a browser and go to http://localhost:8888. The Jupyter web  interface will ask you for a token. Use the following:      $TOKEN  Note that anyone to whom you give the token can access (and modify/delete)  files in your GLADE spaces, regardless of the file permissions you  have set. SHARE TOKENS RARELY AND WISELY!  Run the following commands on your destop or latptop:     ssh -N -l $USER -L 8080:${JNHOST}:8080 ${STHOST}.ucar.edu     ssh -N -l $USER -L 4040:${JNHOST}:4040 ${STHOST}.ucar.edu  Log in with your YubiKey/Cryptocard(there will be no prompt).  Then open a browser and go to http://localhost:8080 to access the Spark Master UI.  Finally open a browser and go to http://localhost:4040 to access the Spark Master UI jobs  history.  To stop the server, press Ctrl-C.  EOF  # Wait for user kill command \nsleep inf  Note:  Make the above scripts executable by running ( chmod +x script_name.sh )", 
            "title": "1.2 Scripts"
        }, 
        {
            "location": "/installation/cheyenne/#2-running-existing-cheyenne-spark-installation", 
            "text": "Log into Cheyenne   Create a working directory in your home and move into it:   mkdir cheyenne  `cd cheyenne  mkdir spark  cd spark     Copy  spark-cluster-scripts  directory to  spark  directory   cp -r /glade/p/work/abanihi/cheyenne/spark-cluster-scripts .     In the created  spark  directory, create  conf  directory:   mkdir conf     Schedule your job to run on the Cheyenne, by submitting your job through  pbs scheduler   Example:  qsub -I -l select=4:ncpus=1:mpiprocs=1 -l walltime=00:30:00 -q regular -A ProjectID     Change current directory to  spark-cluster-scripts   cd spark-cluster-scripts", 
            "title": "2. Running Existing Cheyenne Spark Installation"
        }, 
        {
            "location": "/installation/cheyenne/#21-run-pyspark-shell", 
            "text": "To run PySpark shell, run  start-pyspark.sh  by running  ./start-pyspark.sh . You should get something similar to this:", 
            "title": "2.1. Run PySpark Shell"
        }, 
        {
            "location": "/installation/cheyenne/#22-run-pyspark-in-a-jupyter-notebook", 
            "text": "To run PySpark in a Jupyter notebook, make sure you that your current directory is  spark-cluster-scripts  and    run  start-sparknotebook  by typing  ./start-sparknotebook  and follow the instructions given.     There are two notebooks in the  spark-cluster-scripts/  directory. Run the  Spark-Essentials  notebook to test that Spark is running and that you have access to a cluster of nodes.", 
            "title": "2.2. Run PySpark in a Jupyter notebook"
        }, 
        {
            "location": "/usage/yellowstone/", 
            "text": "Installation and Use\n\n\nSpark 2.1.1+Hadoop2.7 is already installed and ready to use on both Cheyenne and Yellowstone.\n\n\nPrerequisites:\n \n\n\n\n\nIf this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go \nhere\n.\n\n\n\n\n1. Yellowstone\n\n\n\n\n\n\nLog into Yellowstone\n\n\n\n\n\n\nCopy \nyellowstone\n directory into your home directory by running \n\n\n\n\n\n\ncp -r /glade/p/work/abanihi/yellowstone/ .\n\n\n\n\n\n\nSchedule your job to run on the Yellowstone, by submitting your job through \nlsf scheduler\n\n\n\n\nExample: \nbsub -Is -W 01:00 -q small -P ProjectID -R \nspan[ptile=1]\n -n 4 bash\n\n\n\n\n\n\n\n\n1.1. Run PySpark Shell\n\n\n\n\nTo run PySpark shell, run \n~/yellowstone/spark/spark-cluster-scripts/start-pyspark.sh\n\n\n\n\nIf everything is well setup, you should get something similar to this:\n\n\n\n\nWhen you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize \nSparkSession\n and \nSparkContext\n before starting to use Spark.\n\n\n\n\n# Import SparkSession\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\n# Initialize SparkSession and attach a sparkContext to the created sparkSession\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\nappName\n(\npyspark\n)\n.\ngetOrCreate\n()\n\n\nsc\n \n=\n \nspark\n.\nsparkContext\n\n\n\n\n\n\n\nIf you need to use \nSpark Master WebUI\n, consider running spark on Cheyenne. As of now, Spark Master WebUI is not available on Yellowstone.\n\n\n\n\n1.2. Run PySpark in a Jupyter notebook\n\n\n\n\n\n\nTo run PySpark in a Jupyter notebook:\n\n\n\n\nrun \n~/yellowstone/spark/spark-cluster-scripts/start-sparknotebook\n and follow the instructions given.\n\n\n\n\n\n\n\n\nThere are two notebooks in the \nspark-cluster-scripts/\n directory. Run the \nSpark-Essentials\n notebook to test that Spark is running and that you have access to a cluster of nodes.\n\n\n\n\n\n\nNOTE:\n We've not been able to get SparkUI feature working on Yellowstone yet!", 
            "title": "Yellowstone"
        }, 
        {
            "location": "/usage/yellowstone/#installation-and-use", 
            "text": "Spark 2.1.1+Hadoop2.7 is already installed and ready to use on both Cheyenne and Yellowstone.  Prerequisites:     If this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go  here .", 
            "title": "Installation and Use"
        }, 
        {
            "location": "/usage/yellowstone/#1-yellowstone", 
            "text": "Log into Yellowstone    Copy  yellowstone  directory into your home directory by running     cp -r /glade/p/work/abanihi/yellowstone/ .    Schedule your job to run on the Yellowstone, by submitting your job through  lsf scheduler   Example:  bsub -Is -W 01:00 -q small -P ProjectID -R  span[ptile=1]  -n 4 bash", 
            "title": "1. Yellowstone"
        }, 
        {
            "location": "/usage/yellowstone/#11-run-pyspark-shell", 
            "text": "To run PySpark shell, run  ~/yellowstone/spark/spark-cluster-scripts/start-pyspark.sh   If everything is well setup, you should get something similar to this:   When you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize  SparkSession  and  SparkContext  before starting to use Spark.   # Import SparkSession  from   pyspark.sql   import   SparkSession  # Initialize SparkSession and attach a sparkContext to the created sparkSession  spark   =   SparkSession . builder . appName ( pyspark ) . getOrCreate ()  sc   =   spark . sparkContext    If you need to use  Spark Master WebUI , consider running spark on Cheyenne. As of now, Spark Master WebUI is not available on Yellowstone.", 
            "title": "1.1. Run PySpark Shell"
        }, 
        {
            "location": "/usage/yellowstone/#12-run-pyspark-in-a-jupyter-notebook", 
            "text": "To run PySpark in a Jupyter notebook:   run  ~/yellowstone/spark/spark-cluster-scripts/start-sparknotebook  and follow the instructions given.     There are two notebooks in the  spark-cluster-scripts/  directory. Run the  Spark-Essentials  notebook to test that Spark is running and that you have access to a cluster of nodes.    NOTE:  We've not been able to get SparkUI feature working on Yellowstone yet!", 
            "title": "1.2. Run PySpark in a Jupyter notebook"
        }, 
        {
            "location": "/usage/cheyenne/", 
            "text": "Quick Start: Cheyenne\n\n\nPrerequisites:\n \n\n\n\n\nIf this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go \nhere\n.\n\n\n\n\nLogging in\n\n\n\n\n\n\nLog in to the Cheyenne system from the terminal\n\n\n\n\nssh -X -l username cheyenne.ucar.edu\n\n\n\n\n\n\n\n\nCopy \ncheyenne\n directory into your home directory by running \n\n\n\n\ncp -r /glade/p/work/abanihi/cheyenne/ .\n\n\n\n\n\n\n\n\nSubmitting jobs\n\n\nSpark can be run interactively ( via ipython shell, jupyter notebook) or in batch mode. \n\n\n1. Interactive jobs\n\n\nTo start an interactive job, use the qsub command with the necessary options.\n\n\n\n\nqsub -I -l select=1:ncpus=36:mpiprocs=36 -l walltime=01:00 -q small -A project_code\n\n\n\n\n1.1 Load IPython shell with PySpark\n\n\n\n\n\n\nTo start IPython shell with PySpark, run the following:\n\n\n\n\n~/cheyenne/spark/spark-cluster-scripts/start-pyspark.sh\n\n\n\n\n\n\n\n\n\n\n1.2. Run PySpark in a Jupyter notebook\n\n\n\n\nTo run PySpark in a Jupyter notebook, run the following:\n\n\n~/cheyenne/spark/spark-cluster-scripts/start-sparknotebook\n and follow the instructions given.\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\nWhen you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize \nSparkSession\n and \nSparkContext\n before starting to use Spark.\n\n\n# Import SparkSession\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\n# Initialize SparkSession and attach a sparkContext to the created sparkSession\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\nappName\n(\npyspark\n)\n.\ngetOrCreate\n()\n\n\nsc\n \n=\n \nspark\n.\nsparkContext\n\n\n\n\n\n2. Batch jobs\n\n\nTo submit a Spark batch job, use the qsub command followed by the name of your PBS batch script file.\n- \nqsub script_name\n\n\n2.1. Spark job script example\n\n\nBatch script to run a Spark job:\n\n\n\n\nspark-test.sh\n\n\n\n\n#!/usr/bin/env bash\n\n\n#PBS -A project_code\n\n\n#PBS -j oe\n\n\n#PBS -m abe\n\n\n#PBS -M email_address\n\n\n#PBS -q queue_name\n\n\n#PBS -l walltime=01:00\n\n\n#PBS -l select=1:ncpus=4:mpiprocs=4\n\n\n\nsource\n ~/cheyenne/spark/spark-cluster-scripts/spark-cluster.sh start\n\n$SPARK_HOME\n/bin/spark-submit --master \n$MASTER\n ~/cheyenne-jobs/spark-test.py\n\n\n\n\n\n\nspark-test.py\n\n\n\n\nfrom\n \n__future__\n \nimport\n \nprint_function\n\n\nfrom\n \nread\n \nimport\n \nRDD\n,\n \nDataFrame\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\nappName\n(\nspark-test\n)\n.\ngetOrCreate\n()\n\n\nsc\n \n=\n \nspark\n.\nsparkContext\n\n\nsc\n.\naddPyFile\n(\n/glade/p/work/abanihi/pyspark4climate/read.py\n)\n\n\nfilepath\n \n=\n \n/glade/u/home/abanihi/data/pres_monthly_1948-2008.nc\n\n\nvar\n \n=\n \npres\n\n\ndata_df\n \n=\n \nDataFrame\n(\nsc\n,\n \n(\nfilepath\n,\n \nvar\n),\n \nsingle\n)\n\n\ndf\n \n=\n \ndata_df\n.\ndf\n\n\nprint\n(\ndf\n.\nshow\n())\n\n\n\n\n\nTo run this spark job, run:\n\n\n\n\nqsub spark-test.sh", 
            "title": "Cheyenne"
        }, 
        {
            "location": "/usage/cheyenne/#quick-start-cheyenne", 
            "text": "Prerequisites:     If this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go  here .", 
            "title": "Quick Start: Cheyenne"
        }, 
        {
            "location": "/usage/cheyenne/#logging-in", 
            "text": "Log in to the Cheyenne system from the terminal   ssh -X -l username cheyenne.ucar.edu     Copy  cheyenne  directory into your home directory by running    cp -r /glade/p/work/abanihi/cheyenne/ .", 
            "title": "Logging in"
        }, 
        {
            "location": "/usage/cheyenne/#submitting-jobs", 
            "text": "Spark can be run interactively ( via ipython shell, jupyter notebook) or in batch mode.", 
            "title": "Submitting jobs"
        }, 
        {
            "location": "/usage/cheyenne/#1-interactive-jobs", 
            "text": "To start an interactive job, use the qsub command with the necessary options.   qsub -I -l select=1:ncpus=36:mpiprocs=36 -l walltime=01:00 -q small -A project_code", 
            "title": "1. Interactive jobs"
        }, 
        {
            "location": "/usage/cheyenne/#11-load-ipython-shell-with-pyspark", 
            "text": "To start IPython shell with PySpark, run the following:   ~/cheyenne/spark/spark-cluster-scripts/start-pyspark.sh", 
            "title": "1.1 Load IPython shell with PySpark"
        }, 
        {
            "location": "/usage/cheyenne/#12-run-pyspark-in-a-jupyter-notebook", 
            "text": "To run PySpark in a Jupyter notebook, run the following:  ~/cheyenne/spark/spark-cluster-scripts/start-sparknotebook  and follow the instructions given.      Note:  When you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize  SparkSession  and  SparkContext  before starting to use Spark.  # Import SparkSession  from   pyspark.sql   import   SparkSession  # Initialize SparkSession and attach a sparkContext to the created sparkSession  spark   =   SparkSession . builder . appName ( pyspark ) . getOrCreate ()  sc   =   spark . sparkContext", 
            "title": "1.2. Run PySpark in a Jupyter notebook"
        }, 
        {
            "location": "/usage/cheyenne/#2-batch-jobs", 
            "text": "To submit a Spark batch job, use the qsub command followed by the name of your PBS batch script file.\n-  qsub script_name", 
            "title": "2. Batch jobs"
        }, 
        {
            "location": "/usage/cheyenne/#21-spark-job-script-example", 
            "text": "Batch script to run a Spark job:   spark-test.sh   #!/usr/bin/env bash  #PBS -A project_code  #PBS -j oe  #PBS -m abe  #PBS -M email_address  #PBS -q queue_name  #PBS -l walltime=01:00  #PBS -l select=1:ncpus=4:mpiprocs=4  source  ~/cheyenne/spark/spark-cluster-scripts/spark-cluster.sh start $SPARK_HOME /bin/spark-submit --master  $MASTER  ~/cheyenne-jobs/spark-test.py   spark-test.py   from   __future__   import   print_function  from   read   import   RDD ,   DataFrame  from   pyspark.sql   import   SparkSession  spark   =   SparkSession . builder . appName ( spark-test ) . getOrCreate ()  sc   =   spark . sparkContext  sc . addPyFile ( /glade/p/work/abanihi/pyspark4climate/read.py )  filepath   =   /glade/u/home/abanihi/data/pres_monthly_1948-2008.nc  var   =   pres  data_df   =   DataFrame ( sc ,   ( filepath ,   var ),   single )  df   =   data_df . df  print ( df . show ())   To run this spark job, run:   qsub spark-test.sh", 
            "title": "2.1. Spark job script example"
        }, 
        {
            "location": "/pyspark4climate/intro/", 
            "text": "PySpark4Climate Package\n\n\nPySpark4Climate is a high level library for parsing netCDF data with Apache Spark, for Spark SQL and DataFrames.\n\n\nRequirements\n\n\nThis library requires:\n\n\n\n\nSpark 2.0+\n\n\nnetcdf4-python 1.2.8+\n\n\n\n\nFeatures\n\n\nThis package allows reading netCDF files in local or distributed filesystem a:\n\n\n\n\n\n\nSpark DataFrames\n\n\n\n\n\n\nSpark RDD\n\n\n\n\n\n\nWhen reading files the API accepts several options:\n\n\n\n\nsc\n          : by default \nsc\n is \nNone\n, but sc should be set to SparkContext\n\n\nfile_list\n   : list of tuples. Each tuple is of the form (filepath, variable_name). By default \nfile_list\n is None. \n\n\nmode\n        : string. By default \nmode\n is \nmulti\n for multiple files. To read a single a file, \nmode\n is set to \nsingle\n.\n\n\n\n\npartitions\n  : number of partitions to be used by Spark. By default \npartitions\n is None. When \npartitions\n is \nNone\n, Spark uses the number of partitions computed using information from the dataset metadata.\n\n\n\n\n\n\nPros of H5Spark\n\n\n\n\nParallel I/O is transparently handled without user's interaction\n\n\nH5Spark's I/O is an \nMPI-Like\n independent I/O and this means that:\n\n\neach executor will issue the I/O independently without communicating with other executors.\n\n\n\n\n\n\n\n\n\n\n\n\nScala/Python implementation\n\n\n\n\nSpark favors Scala and Python\n\n\nH5Spark uses HDF5 Python library\n\n\nUnderneath is HDF5 C posix library\n\n\nNo MPIIO support \n\n\n\n\n\n\n\n\n\n\nAs it's shown in the figure below, \nH5Spark\n was designed for and tested on \nLustre parallel filesytem\n. \n\n\n\n\nTODO:\n Test H5Spark on NCAR's GPFS filesytem.\n\n\n**Currently Supported Fe\n* [x] Introduction\n* [x] Usage\n* [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est\n* [ ] Vestibulum convallis sit amet nisi a tincidunt\n    * [x] In hac habitasse platea dictumst\n    * [x] In scelerisque nibh non dolor mollis congue sed et metus\n    * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis\n    * [ ] Praesent sed risus massa\n* [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque\n* [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi", 
            "title": "Introduction"
        }, 
        {
            "location": "/pyspark4climate/intro/#pyspark4climate-package", 
            "text": "PySpark4Climate is a high level library for parsing netCDF data with Apache Spark, for Spark SQL and DataFrames.", 
            "title": "PySpark4Climate Package"
        }, 
        {
            "location": "/pyspark4climate/intro/#requirements", 
            "text": "This library requires:   Spark 2.0+  netcdf4-python 1.2.8+", 
            "title": "Requirements"
        }, 
        {
            "location": "/pyspark4climate/intro/#features", 
            "text": "This package allows reading netCDF files in local or distributed filesystem a:    Spark DataFrames    Spark RDD    When reading files the API accepts several options:   sc           : by default  sc  is  None , but sc should be set to SparkContext  file_list    : list of tuples. Each tuple is of the form (filepath, variable_name). By default  file_list  is None.   mode         : string. By default  mode  is  multi  for multiple files. To read a single a file,  mode  is set to  single .   partitions   : number of partitions to be used by Spark. By default  partitions  is None. When  partitions  is  None , Spark uses the number of partitions computed using information from the dataset metadata.    Pros of H5Spark   Parallel I/O is transparently handled without user's interaction  H5Spark's I/O is an  MPI-Like  independent I/O and this means that:  each executor will issue the I/O independently without communicating with other executors.       Scala/Python implementation   Spark favors Scala and Python  H5Spark uses HDF5 Python library  Underneath is HDF5 C posix library  No MPIIO support       As it's shown in the figure below,  H5Spark  was designed for and tested on  Lustre parallel filesytem .    TODO:  Test H5Spark on NCAR's GPFS filesytem.  **Currently Supported Fe\n* [x] Introduction\n* [x] Usage\n* [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est\n* [ ] Vestibulum convallis sit amet nisi a tincidunt\n    * [x] In hac habitasse platea dictumst\n    * [x] In scelerisque nibh non dolor mollis congue sed et metus\n    * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis\n    * [ ] Praesent sed risus massa\n* [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque\n* [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi", 
            "title": "Features"
        }, 
        {
            "location": "/pyspark4climate/getting-started/", 
            "text": "", 
            "title": "Getting Started"
        }
    ]
}