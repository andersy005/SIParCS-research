{
    "docs": [
        {
            "location": "/", 
            "text": "PySpark for \"Big\" Atmospheric \n Oceanic Data Analysis\n\n\nBackground\n\n\n\n\nClimate and Weather directly impact all aspects of society. Understanding these processeses provide important information for policy - and decision - makers. To understand the average climate conditions and extreme weather events, Earth science and climate change researchers need data from climate models and observations. As such in the typical work flow of climate change and atmospheric research, much time is wasted waiting to reformat and regrid data to homogeneous formats. Additionally, because of the data volume, in order to compute metrics, perform analysis, and visualize data / generate plots, multi-stage processes with repeated I/O are used - the root cause of performance bottlenecks and the resulting user\u2019s frustrations related to time inefficiencies.\n\nNASA-SciSpark project\n\n\n\n\n1\n: Spark is a cluster computing paradigm based on the MapReduce paradigm that has garnered many scientific analysis workflows and are very well suited for Spark.  As a result of this lack of great deal of interest for its power and ease of use in analyzing \u201cbig data\u201d in the commercial and computer science sectors.  In much of the scientific sector, however --- and specifically in the atmospheric and oceanic sciences --- Spark has not captured the interest of scientists for analyzing their data, even though their datasets may be larger than many commercial datasets interest, there are very few platforms on which scientists can experiment with and learn about using Hadoop and/or Spark for their scientific research.  Additionally, there are very few resources to teach and educate scientists on how or why to use Hadoop or Spark for their analysis.\n\n\nGoal\n\n\nPySpark for Big Atmospheric \n Oceanic Data Analysis\n is a \nCISL/SIParCS research project\n that seeks to explore the realm of distributed parallel computing on NCAR's Yellowstone and Cheyenne supercomputers by taking advantage of: \n\n\n\n\n\n\nApache Spark's potential to offer speed-up and advancements of nearly 1000x in-memory\n\n\n\n\n\n\nThe increasing growing community around Spark\n\n\n\n\n\n\nSpark's notion of Resilient Distributed Datasets(RDDs). RDDs represent immutable dataset that can be: \n\n\n\n\nreused across multi-stage operations.\n\n\npartitioned across multiple machines.\n\n\nautomatically reconstructed if a partition is lost.\n\n\n\n\nto address the pain points that scientists and researchers endure during model evaluation processes. \n\n\nExamples of these pain points include:\n\n\n\n\nTemporal and Zonal averaging of data\n\n\nComputation of climatologies\n\n\nPre-processing of CMIP data such as:\n\n\nRegridding \n\n\nVariable clustering (min/max)\n\n\ncalendar harmonizing", 
            "title": "Home"
        }, 
        {
            "location": "/#pyspark-for-big-atmospheric-oceanic-data-analysis", 
            "text": "", 
            "title": "PySpark for \"Big\" Atmospheric &amp; Oceanic Data Analysis"
        }, 
        {
            "location": "/#background", 
            "text": "Climate and Weather directly impact all aspects of society. Understanding these processeses provide important information for policy - and decision - makers. To understand the average climate conditions and extreme weather events, Earth science and climate change researchers need data from climate models and observations. As such in the typical work flow of climate change and atmospheric research, much time is wasted waiting to reformat and regrid data to homogeneous formats. Additionally, because of the data volume, in order to compute metrics, perform analysis, and visualize data / generate plots, multi-stage processes with repeated I/O are used - the root cause of performance bottlenecks and the resulting user\u2019s frustrations related to time inefficiencies. NASA-SciSpark project   1 : Spark is a cluster computing paradigm based on the MapReduce paradigm that has garnered many scientific analysis workflows and are very well suited for Spark.  As a result of this lack of great deal of interest for its power and ease of use in analyzing \u201cbig data\u201d in the commercial and computer science sectors.  In much of the scientific sector, however --- and specifically in the atmospheric and oceanic sciences --- Spark has not captured the interest of scientists for analyzing their data, even though their datasets may be larger than many commercial datasets interest, there are very few platforms on which scientists can experiment with and learn about using Hadoop and/or Spark for their scientific research.  Additionally, there are very few resources to teach and educate scientists on how or why to use Hadoop or Spark for their analysis.", 
            "title": "Background"
        }, 
        {
            "location": "/#goal", 
            "text": "PySpark for Big Atmospheric   Oceanic Data Analysis  is a  CISL/SIParCS research project  that seeks to explore the realm of distributed parallel computing on NCAR's Yellowstone and Cheyenne supercomputers by taking advantage of:     Apache Spark's potential to offer speed-up and advancements of nearly 1000x in-memory    The increasing growing community around Spark    Spark's notion of Resilient Distributed Datasets(RDDs). RDDs represent immutable dataset that can be:    reused across multi-stage operations.  partitioned across multiple machines.  automatically reconstructed if a partition is lost.   to address the pain points that scientists and researchers endure during model evaluation processes.   Examples of these pain points include:   Temporal and Zonal averaging of data  Computation of climatologies  Pre-processing of CMIP data such as:  Regridding   Variable clustering (min/max)  calendar harmonizing", 
            "title": "Goal"
        }, 
        {
            "location": "/installation/yellowstone/", 
            "text": "Apache Spark 2.1.1 on Yellowstone\n\n\nThe first task of our research project was to install the newest version of Apache Spark on Yellowstone. \n\n\nAt the time (May/2017), there was an old installation of Apache Spark (Spark 1.6) on Yellowstone. This installation was done by Davide Del Vento.\n\n\nAs of today (June/2017), we've been able to install the newest version of Apache Spark - \nSpark 2.1.1+Hadoop2.7\n.\n\n\nBelow are the steps that we took to get Spark Up and Running on Yellowstone.\n\n\nIf all you need is to run the existing Apache Spark on Yellowstone, just skip to \nSection 2\n of this page.\n\n\n1. Installation\n\n\n1.1 Downloading Spark and Setting up Spark's directory and necessary files\n\n\n\n\nLog into Yellowstone\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nGo to \nApache Spark's official website\n and follow steps 1-4 to get a download link for Spark. Copy the download link and \n\n\n\n\n\n\nGo to \n/glade/p/work/abanihi/\n and download Spark\n\n\n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz\n\n\nUntar \nspark-2.1.1-bin-hadoop2.7.tgz\n with \n\n\ntar -xzf spark-2.1.1-bin-hadoop2.7.tgz\n\n\n\n\n\n\n\n\n\n\n\n\nChange directory to \nspark-2.1.1-bin-hadoop2.7/conf\n and open \nlog4j.properties.template\n\n\n\n\ncd spark-2.1.1-bin-hadoop2.7/conf\n\n\nnano log4j.properties.template\n\n\nGo to the following line: \nlog4j.rootCategory=INFO, console\n and change \nINFO\n to \nERROR\n\n\nExit out of the editor\n\n\n\n\n\n\n\n\nRename \nlog4j.properties.template\n to \nlog4j.properties\n\n\n\n\nmv log4j.properties.template log4.properties\n\n\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nDownload H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this \npage\n.\n\n\n\n\ngit clone https://github.com/valiantljk/h5spark.git\n\n\nThis package will be added to \nPython Path\n in \nspark-cluster.sh\n script.\n\n\n\n\n\n\n\n\n1.2 Scripts\n\n\nNote:\n The scripts in this directory are based on Davide's previous scripts for Spark 1.6.\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi\n\n\n\n\n\n\n\n\nCreate a new directory called \nyellowstone\n and move into it \n\n\n\n\nmkdir yellowstone\n\n\n`cd yellowstone\n\n\nCreate \nspark-cluster-scripts\ndirectory and move into it\n\n\nmkdir spark-cluster-scripts\n\n\ncd spark-cluster-scripts/\n\n\n\n\n\n\n\n\nCreate a new script and name it \nspark-env.sh\n\n\n\n\nnano spark-env.sh\n\n\nspark-env.sh\n should have the following content \n\n\n\n\n\n\n\n\n#!/usr/bin/env bash\n\nsource /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu/5.3.0\nmodule load java\nmodule load python all-python-libs\nmodule load h5py\n\nexport SPARK_WORKER_DIR=/glade/scratch/$USER/spark/work\nexport SPARK_LOG_DIR=/glade/scratch/$USER/spark/logs\nexport SPARK_LOCAL_DIRS=/glade/scratch/$USER/spark/temp\nexport SPARK_LOCAL_IP=$(sed -e 's/\\([^.]*\\).*$/\\1-ib/' \n $(hostname))\n\n\n\n\n\n\n\nCreate a new script file and name it \nspark-cluster.sh\n\n\nnano spark-cluster.sh\n\n\n\n\n\n\n\n\n#!/usr/bin/env bash\nexport SPARK_HOME=/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$PYTHONPATH:/glade/p/work/abanihi/pyspark4climate/\nexport SPARK_CONF_DIR=~/yellowstone/spark/conf\nexport SPARK_HOSTFILE=$SPARK_CONF_DIR/spark_hostfile\n\n# create temp hostfile\nexport SPARK_TEMP_HOSTFILE=$SPARK_CONF_DIR/spark_temp_hostfile\n\nrm $SPARK_HOSTFILE $SPARK_CONF_DIR/slaves\n\n\nmpirun.lsf hostname | grep -v Execute | sort \n $SPARK_TEMP_HOSTFILE\n\nsed -i 's/$/-ib/' $SPARK_TEMP_HOSTFILE\ncat $SPARK_TEMP_HOSTFILE | sort -u \n $SPARK_HOSTFILE\ntail -n +2 $SPARK_TEMP_HOSTFILE | sort -u \n $SPARK_CONF_DIR/slaves\ntail -n +2 $SPARK_TEMP_HOSTFILE | uniq -c \n $SPARK_CONF_DIR/temp_ncores_slaves\n\nrm $SPARK_TEMP_HOSTFILE\n\n\nexport SPARK_MASTER_HOST=$(head -n 1 $SPARK_HOSTFILE)\nexport MASTER=spark://$SPARK_MASTER_HOST:7077\n\ncp ~/yellowstone/spark/spark-cluster-scripts/spark-env.sh $SPARK_CONF_DIR/spark-env.sh\nsource $SPARK_CONF_DIR/spark-env.sh\n\nif [ \n$1\n == \nstart\n ]; then\n    cmd_master=\n$SPARK_HOME/sbin/start-master.sh\n\n    cmd_slave=\n$SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR start org.apac\nhe.spark.deploy.worker.Worker 1 $MASTER\n\nelif [ \n$1\n == \nstop\n ]; then\n    cmd_master=\n$SPARK_HOME/sbin/stop-master.sh\n\n    cmd_slave=\n$SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR stop org.apach\ne.spark.deploy.worker.Worker 1\n\nelse\n    exit 1\nfi\n\n$cmd_master\n\nwhile read ncore_slave\ndo\n    ncore=$(echo $ncore_slave | cut -d' ' -f1)\n    slave=$(echo $ncore_slave | cut -d' ' -f2)\n\n    if [ \n$slave\n == \n$SPARK_MASTER_HOST\n ]; then\n          echo \nOn Master node.  Running: cmd_slave --cores $ncore\n\n          $cmd_slave --cores $ncore\n     else\n          echo \nOn Worker node.  Running: cmd_slave --cores $ncore\n\n          ssh $slave \n$cmd_slave\n --cores $ncore \n/dev/null \n\n    fi\ndone \n$SPARK_CONF_DIR/temp_ncores_slaves\n\n\n\n\n\n\n\nCreate a new script file and name it \nstart-pyspark.sh\n\n\n\n\n#!/usr/bin/env bash\n\nsource ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start\n\n$SPARK_HOME/bin/pyspark --master $MASTER\n\n\n\n\n\n\n\nCreate a new script file and name it \nstart-sparknotebook\n. This script file is an extension of \n/glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook\n script file.\n\n\n\n\n#!/usr/bin/env bash\n\n#source spark-cluster.sh start\nsource ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start\n\n# Add the PySpark classes to the Python path:\nexport PATH=\n$SPARK_HOME:$PATH\n\nexport PATH=$PATH:$SPARK_HOME/bin\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n\n# Create trap to kill notebook when user is done\nkill_server() {\n    if [[ \n$JNPID\n != -1 ]]; then\n        echo -en \n\\nKilling Jupyter Notebook Server with PID=$JNPID ... \n\n        kill \n$JNPID\n\n        echo \ndone\n\n        exit 0\n    else\n        exit 1\n    fi\n}\n\nJNPID=-1\ntrap kill_server SIGHUP SIGINT SIGTERM\n\n# Begin server creation\nJNHOST=$(hostname)\nLOGDIR=/glade/scratch/${USER}/.jupyter-notebook\nLOGFILE=${LOGDIR}/log.$(date +%Y%m%dT%H%M%S)\nmkdir -p \n$LOGDIR\n\n\nif [[ $JNHOST == ch* || $JNHOST == r* ]]; then\n    STHOST=cheyenne\nelse\n    STHOST=yellowstone\nfi\n\necho \nLogging this session in $LOGFILE\n\n\n# Check if running on login nodes\nif [[ $JNHOST == yslogin* ]]; then\ncat \n EOF\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.\n\nConsider running on Geyser instead by using execgy to start a session. (Run execgy -hel\np.)\nEOF\nelif [[ $JNHOST == cheyenne* ]]; then\ncat \n EOF\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.\n\nConsider running in an interactive job instead by using qinteractive. (Run qinterative\n-help.)\nEOF\nfi\n\n\njupyter notebook \n$@\n --no-browser --ip=\n$JNHOST\n \n$LOGFILE\n 2\n1 \n\nJNPID=$!\n\n\necho -en  \n\\nStarting jupyter notebook server, please wait ... \n\n\nELAPSED=0\nADDRESS=\n\nwhile [[ $ADDRESS != *\n${JNHOST}\n* ]]; do\n    sleep 1\n    ELAPSED=$((ELAPSED+1))\n    ADDRESS=$(tail -n 1 \n$LOGFILE\n)\n\n    if [[ $ELAPSED -gt 30 ]]; then\n        echo -e \nsomething went wrong\\n---\n\n        cat \n$LOGFILE\n\n        echo \n---\n\n\n        kill_server\n    fi\ndone\n\necho -e \ndone\\n---\\n\n\n\nADDRESS=${ADDRESS##*:}\nPORT=${ADDRESS%/*}\nTOKEN=${ADDRESS#*=}\n\ncat \n EOF\nRun the following command on your desktop or laptop:\n\n   ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu\n\nLog in with your YubiKey/Cryptocard (there will be no prompt).\nThen open a browser and go to http://localhost:8888. The Jupyter web\ninterface will ask you for a token. Use the following:\n\n    $TOKEN\n\nNote that anyone to whom you give the token can access (and modify/delete)\nfiles in your GLADE spaces, regardless of the file permissions you\nhave set. SHARE TOKENS RARELY AND WISELY!\n\nTo stop the server, press Ctrl-C.\nEOF\n\n# Wait for user kill command\nsleep inf\n\n\n\n\n\nNote:\n Make the above scripts executable (\nchmod +x script_name.sh\n)", 
            "title": "Yellowstone"
        }, 
        {
            "location": "/installation/yellowstone/#apache-spark-211-on-yellowstone", 
            "text": "The first task of our research project was to install the newest version of Apache Spark on Yellowstone.   At the time (May/2017), there was an old installation of Apache Spark (Spark 1.6) on Yellowstone. This installation was done by Davide Del Vento.  As of today (June/2017), we've been able to install the newest version of Apache Spark -  Spark 2.1.1+Hadoop2.7 .  Below are the steps that we took to get Spark Up and Running on Yellowstone.  If all you need is to run the existing Apache Spark on Yellowstone, just skip to  Section 2  of this page.", 
            "title": "Apache Spark 2.1.1 on Yellowstone"
        }, 
        {
            "location": "/installation/yellowstone/#1-installation", 
            "text": "", 
            "title": "1. Installation"
        }, 
        {
            "location": "/installation/yellowstone/#11-downloading-spark-and-setting-up-sparks-directory-and-necessary-files", 
            "text": "Log into Yellowstone  Change working directory to  /glade/p/work/abanihi  cd /glade/p/work/abanihi/     Go to  Apache Spark's official website  and follow steps 1-4 to get a download link for Spark. Copy the download link and     Go to  /glade/p/work/abanihi/  and download Spark   wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz  Untar  spark-2.1.1-bin-hadoop2.7.tgz  with   tar -xzf spark-2.1.1-bin-hadoop2.7.tgz       Change directory to  spark-2.1.1-bin-hadoop2.7/conf  and open  log4j.properties.template   cd spark-2.1.1-bin-hadoop2.7/conf  nano log4j.properties.template  Go to the following line:  log4j.rootCategory=INFO, console  and change  INFO  to  ERROR  Exit out of the editor     Rename  log4j.properties.template  to  log4j.properties   mv log4j.properties.template log4.properties     Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi/     Download H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this  page .   git clone https://github.com/valiantljk/h5spark.git  This package will be added to  Python Path  in  spark-cluster.sh  script.", 
            "title": "1.1 Downloading Spark and Setting up Spark's directory and necessary files"
        }, 
        {
            "location": "/installation/yellowstone/#12-scripts", 
            "text": "Note:  The scripts in this directory are based on Davide's previous scripts for Spark 1.6.    Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi     Create a new directory called  yellowstone  and move into it    mkdir yellowstone  `cd yellowstone  Create  spark-cluster-scripts directory and move into it  mkdir spark-cluster-scripts  cd spark-cluster-scripts/     Create a new script and name it  spark-env.sh   nano spark-env.sh  spark-env.sh  should have the following content      #!/usr/bin/env bash\n\nsource /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu/5.3.0\nmodule load java\nmodule load python all-python-libs\nmodule load h5py\n\nexport SPARK_WORKER_DIR=/glade/scratch/$USER/spark/work\nexport SPARK_LOG_DIR=/glade/scratch/$USER/spark/logs\nexport SPARK_LOCAL_DIRS=/glade/scratch/$USER/spark/temp\nexport SPARK_LOCAL_IP=$(sed -e 's/\\([^.]*\\).*$/\\1-ib/'   $(hostname))   Create a new script file and name it  spark-cluster.sh  nano spark-cluster.sh     #!/usr/bin/env bash\nexport SPARK_HOME=/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$PYTHONPATH:/glade/p/work/abanihi/pyspark4climate/\nexport SPARK_CONF_DIR=~/yellowstone/spark/conf\nexport SPARK_HOSTFILE=$SPARK_CONF_DIR/spark_hostfile\n\n# create temp hostfile\nexport SPARK_TEMP_HOSTFILE=$SPARK_CONF_DIR/spark_temp_hostfile\n\nrm $SPARK_HOSTFILE $SPARK_CONF_DIR/slaves\n\n\nmpirun.lsf hostname | grep -v Execute | sort   $SPARK_TEMP_HOSTFILE\n\nsed -i 's/$/-ib/' $SPARK_TEMP_HOSTFILE\ncat $SPARK_TEMP_HOSTFILE | sort -u   $SPARK_HOSTFILE\ntail -n +2 $SPARK_TEMP_HOSTFILE | sort -u   $SPARK_CONF_DIR/slaves\ntail -n +2 $SPARK_TEMP_HOSTFILE | uniq -c   $SPARK_CONF_DIR/temp_ncores_slaves\n\nrm $SPARK_TEMP_HOSTFILE\n\n\nexport SPARK_MASTER_HOST=$(head -n 1 $SPARK_HOSTFILE)\nexport MASTER=spark://$SPARK_MASTER_HOST:7077\n\ncp ~/yellowstone/spark/spark-cluster-scripts/spark-env.sh $SPARK_CONF_DIR/spark-env.sh\nsource $SPARK_CONF_DIR/spark-env.sh\n\nif [  $1  ==  start  ]; then\n    cmd_master= $SPARK_HOME/sbin/start-master.sh \n    cmd_slave= $SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR start org.apac\nhe.spark.deploy.worker.Worker 1 $MASTER \nelif [  $1  ==  stop  ]; then\n    cmd_master= $SPARK_HOME/sbin/stop-master.sh \n    cmd_slave= $SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR stop org.apach\ne.spark.deploy.worker.Worker 1 \nelse\n    exit 1\nfi\n\n$cmd_master\n\nwhile read ncore_slave\ndo\n    ncore=$(echo $ncore_slave | cut -d' ' -f1)\n    slave=$(echo $ncore_slave | cut -d' ' -f2)\n\n    if [  $slave  ==  $SPARK_MASTER_HOST  ]; then\n          echo  On Master node.  Running: cmd_slave --cores $ncore \n          $cmd_slave --cores $ncore\n     else\n          echo  On Worker node.  Running: cmd_slave --cores $ncore \n          ssh $slave  $cmd_slave  --cores $ncore  /dev/null  \n    fi\ndone  $SPARK_CONF_DIR/temp_ncores_slaves   Create a new script file and name it  start-pyspark.sh   #!/usr/bin/env bash\n\nsource ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start\n\n$SPARK_HOME/bin/pyspark --master $MASTER   Create a new script file and name it  start-sparknotebook . This script file is an extension of  /glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook  script file.   #!/usr/bin/env bash\n\n#source spark-cluster.sh start\nsource ~/yellowstone/spark/spark-cluster-scripts/spark-cluster.sh start\n\n# Add the PySpark classes to the Python path:\nexport PATH= $SPARK_HOME:$PATH \nexport PATH=$PATH:$SPARK_HOME/bin\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n\n# Create trap to kill notebook when user is done\nkill_server() {\n    if [[  $JNPID  != -1 ]]; then\n        echo -en  \\nKilling Jupyter Notebook Server with PID=$JNPID ...  \n        kill  $JNPID \n        echo  done \n        exit 0\n    else\n        exit 1\n    fi\n}\n\nJNPID=-1\ntrap kill_server SIGHUP SIGINT SIGTERM\n\n# Begin server creation\nJNHOST=$(hostname)\nLOGDIR=/glade/scratch/${USER}/.jupyter-notebook\nLOGFILE=${LOGDIR}/log.$(date +%Y%m%dT%H%M%S)\nmkdir -p  $LOGDIR \n\nif [[ $JNHOST == ch* || $JNHOST == r* ]]; then\n    STHOST=cheyenne\nelse\n    STHOST=yellowstone\nfi\n\necho  Logging this session in $LOGFILE \n\n# Check if running on login nodes\nif [[ $JNHOST == yslogin* ]]; then\ncat   EOF\n\nSee  Use of login nodes  here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.\n\nConsider running on Geyser instead by using execgy to start a session. (Run execgy -hel\np.)\nEOF\nelif [[ $JNHOST == cheyenne* ]]; then\ncat   EOF\n\nSee  Use of login nodes  here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.\n\nConsider running in an interactive job instead by using qinteractive. (Run qinterative\n-help.)\nEOF\nfi\n\n\njupyter notebook  $@  --no-browser --ip= $JNHOST   $LOGFILE  2 1  \nJNPID=$!\n\n\necho -en   \\nStarting jupyter notebook server, please wait ...  \n\nELAPSED=0\nADDRESS=\n\nwhile [[ $ADDRESS != * ${JNHOST} * ]]; do\n    sleep 1\n    ELAPSED=$((ELAPSED+1))\n    ADDRESS=$(tail -n 1  $LOGFILE )\n\n    if [[ $ELAPSED -gt 30 ]]; then\n        echo -e  something went wrong\\n--- \n        cat  $LOGFILE \n        echo  --- \n\n        kill_server\n    fi\ndone\n\necho -e  done\\n---\\n \n\nADDRESS=${ADDRESS##*:}\nPORT=${ADDRESS%/*}\nTOKEN=${ADDRESS#*=}\n\ncat   EOF\nRun the following command on your desktop or laptop:\n\n   ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu\n\nLog in with your YubiKey/Cryptocard (there will be no prompt).\nThen open a browser and go to http://localhost:8888. The Jupyter web\ninterface will ask you for a token. Use the following:\n\n    $TOKEN\n\nNote that anyone to whom you give the token can access (and modify/delete)\nfiles in your GLADE spaces, regardless of the file permissions you\nhave set. SHARE TOKENS RARELY AND WISELY!\n\nTo stop the server, press Ctrl-C.\nEOF\n\n# Wait for user kill command\nsleep inf  Note:  Make the above scripts executable ( chmod +x script_name.sh )", 
            "title": "1.2 Scripts"
        }, 
        {
            "location": "/installation/cheyenne/", 
            "text": "Apache Spark 2.1.1 on Cheyenne\n\n\nThe Second task of our research project was to install the newest version of Apache Spark on Cheyenne. \n\n\nAs of today (June/2017), we've been able to install the newest version of Apache Spark - \nSpark 2.1.1+Hadoop2.7\n.\n\n\nBelow are the steps that we took to get Spark Up and Running on Cheyenne.\n\n\nIf all you need is to run the existing Apache Spark on Cheyenne, just skip to \nSection 2\n of this page.\n\n\n1. Installation\n\n\n1.1 Downloading Spark and Setting up Spark's directory and necessary files\n\n\nThe steps described in this section are similar to those for Yellowstone. Since both Yellowstone and Cheyenne have access to the same parallel filesytem, we decide to use the same downloaded Spark binaries.\n\n\nThe following section gives details on how Apache Spark would be installed on Cheyenne.\n\n\n\n\nLog into Cheyenne\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nGo to \nApache Spark's official website\n and follow steps 1-4 to get a download link for Spark. Copy the download link and \n\n\n\n\n\n\nGo to \n/glade/p/work/abanihi/\n and download Spark\n\n\n\n\nwget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz\n\n\nUntar \nspark-2.1.1-bin-hadoop2.7.tgz\n with \n\n\ntar -xzf spark-2.1.1-bin-hadoop2.7.tgz\n\n\n\n\n\n\n\n\n\n\n\n\nChange directory to \nspark-2.1.1-bin-hadoop2.7/conf\n and open \nlog4j.properties.template\n\n\n\n\ncd spark-2.1.1-bin-hadoop2.7/conf\n\n\nnano log4j.properties.template\n\n\nGo to the following line: \nlog4j.rootCategory=INFO, console\n and change \nINFO\n to \nERROR\n\n\nExit out of the editor\n\n\n\n\n\n\n\n\nRename \nlog4j.properties.template\n to \nlog4j.properties\n\n\n\n\nmv log4j.properties.template log4.properties\n\n\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi/\n\n\n\n\n\n\n\n\nDownload H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this \npage\n.\n\n\n\n\ngit clone https://github.com/valiantljk/h5spark.git\n\n\nThis package will be added to \nPython Path\n in \nspark-cluster.sh\n script.\n\n\n\n\n\n\n\n\n1.2 Scripts\n\n\nEven though the scripts for Yellowstone and Cheyenne have so much in common, there are some differences.\n\n\n\n\n\n\nChange working directory to \n/glade/p/work/abanihi\n\n\n\n\ncd /glade/p/work/abanihi\n\n\n\n\n\n\n\n\nCreate a new directory called \ncheyenne\n and move into it \n\n\n\n\nmkdir cheyenne\n\n\n`cd cheyenne\n\n\nCreate \nspark-cluster-scripts\ndirectory and move into it\n\n\nmkdir spark-cluster-scripts\n\n\ncd spark-cluster-scripts/\n\n\n\n\n\n\n\n\nCreate a new script and name it \nspark-env.sh\n\n\n\n\nnano spark-env.sh\n\n\nspark-env.sh\n should have the following content \n\n\n\n\n\n\n\n\n##!/usr/bin/env bash\n\nsource /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu\nexport MODULEPATH=/glade/p/work/bdobbins/Modules:${MODULEPATH}\nmodule load java\nml python\nml numpy\nml jupyter\nml scipy\nml h5py\nml bottleneck\nml numexpr\nml pandas\nml pyside\nml matplotlib\nml pyngl\nml scikit-learn\nml netcdf4-python\nml cf_units\nml xarray\n\nexport SPARK_WORKER_DIR=/glade/scratch/$USER/spark/work\nexport SPARK_LOG_DIR=/glade/scratch/$USER/spark/logs\nexport SPARK_LOCAL_DIRS=/glade/scratch/$USER/spark/temp\nexport SPARK_LOCAL_IP=$(sed -e 's/\\([^.]*\\).*$/\\1/' \n $(hostname))\n\n\n\n\n\n\n\nCreate a new script file and name it \nspark-cluster.sh\n\n\nnano spark-cluster.sh\n\n\n\n\n\n\n\n\n#!/usr/bin/env bash\nexport SPARK_HOME=/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$PYTHONPATH:/glade/p/work/abanihi/h5spark/src/main/python/h5spark/:$PYTHONPATH\nexport SPARK_CONF_DIR=~/cheyenne/spark/conf\nexport SPARK_HOSTFILE=$SPARK_CONF_DIR/spark_hostfile\n\n# create temp hostfile\nexport SPARK_TEMP_HOSTFILE=$SPARK_CONF_DIR/spark_temp_hostfile\n\nrm $SPARK_HOSTFILE $SPARK_CONF_DIR/slaves\n\nexport MPI_SHEPHERD=true\nmpiexec_mpt hostname | grep -v Execute | sort \n $SPARK_TEMP_HOSTFILE\n\n#sed -i 's/$/-ib/' $SPARK_TEMP_HOSTFILE\ncat $SPARK_TEMP_HOSTFILE | sort -u \n $SPARK_HOSTFILE\ntail -n +2 $SPARK_TEMP_HOSTFILE | sort -u \n $SPARK_CONF_DIR/slaves\ntail -n +2 $SPARK_TEMP_HOSTFILE | uniq -c \n temp_ncores_slaves\n\nrm $SPARK_TEMP_HOSTFILE\n\n\nexport SPARK_MASTER_HOST=$(head -n 1 $SPARK_HOSTFILE)\nexport MASTER=spark://$SPARK_MASTER_HOST:7077\n\ncp spark-env.sh $SPARK_CONF_DIR/spark-env.sh\nsource $SPARK_CONF_DIR/spark-env.sh\n\nif [ \n$1\n == \nstart\n ]; then\n    cmd_master=\n$SPARK_HOME/sbin/start-master.sh\n\n    cmd_slave=\n$SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker 1 $MASTER\n\nelif [ \n$1\n == \nstop\n ]; then\n    cmd_master=\n$SPARK_HOME/sbin/stop-master.sh\n\n    cmd_slave=\n$SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker 1\n\nelse\n    exit 1\nfi\n\n$cmd_master\n\nwhile read ncore_slave\ndo\n    ncore=$(echo $ncore_slave | cut -d' ' -f1)\n    slave=$(echo $ncore_slave | cut -d' ' -f2)\n\n    if [ \n$slave\n == \n$SPARK_MASTER_HOST\n ]; then\n          echo \nOn Master node.  Running: cmd_slave --cores $ncore\n\n          $cmd_slave --cores $ncore\n     else\n          echo \nOn Worker node.  Running: cmd_slave --cores $ncore\n\n          ssh $slave \n$cmd_slave\n --cores $ncore \n /dev/null\n\n    fi\ndone \ntemp_ncores_slaves\n\n\n\n\n\n\nCreate a new script file and name it \nstart-pyspark.sh\n\n\n\n\n#!/usr/bin/env bash\n\nsource spark-cluster.sh start\n$SPARK_HOME/bin/pyspark --master $MASTER\n\n\n\n\n\n\n\nCreate a new script file and name it \nstart-sparknotebook\n. This script file is an extension of \n/glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook\n script file.\n\n\n\n\n#!/usr/bin/env bash\n\nsource spark-cluster.sh start\n\n# Add the PySpark classes to the Python path:\nexport PATH=\n$SPARK_HOME:$PATH\n\nexport PATH=$PATH:$SPARK_HOME/bin\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n\n# Create trap to kill notebook when user is done\nkill_server() {\n    if [[ \n$JNPID\n != -1 ]]; then\n        echo -en \n\\nKilling Jupyter Notebook Server with PID=$JNPID ... \n\n        kill \n$JNPID\n\n        echo \ndone\n\n        exit 0\n    else\n        exit 1\n    fi\n}\n\nJNPID=-1\ntrap kill_server SIGHUP SIGINT SIGTERM\n\n# Begin server creation\nJNHOST=$(hostname)\nLOGDIR=/glade/scratch/${USER}/.jupyter-notebook\nLOGFILE=${LOGDIR}/log.$(date +%Y%m%dT%H%M%S)\nmkdir -p \n$LOGDIR\n\n\nif [[ $JNHOST == ch* || $JNHOST == r* ]]; then\n    STHOST=cheyenne\nelse\n    STHOST=yellowstone\nfi\n\necho \nLogging this session in $LOGFILE\n\n\n# Check if running on login nodes\nif [[ $JNHOST == yslogin* ]]; then\ncat \n EOF\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.\n\nConsider running on Geyser instead by using execgy to start a session. (Run execgy -help.)\nEOF\nelif [[ $JNHOST == cheyenne* ]]; then\ncat \n EOF\n\nSee \nUse of login nodes\n here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.\n\nConsider running in an interactive job instead by using qinteractive. (Run qinterative -help.)\nEOF\nfi\n\n\njupyter notebook \n$@\n --no-browser --ip=\n$JNHOST\n \n$LOGFILE\n 2\n1 \n\nJNPID=$!\n\n\necho -en  \n\\nStarting jupyter notebook server, please wait ... \n\n\nELAPSED=0\nADDRESS=\n\nwhile [[ $ADDRESS != *\n${JNHOST}\n* ]]; do\n    sleep 1\n    ELAPSED=$((ELAPSED+1))\n    ADDRESS=$(tail -n 1 \n$LOGFILE\n)\n\n    if [[ $ELAPSED -gt 30 ]]; then\n        echo -e \nsomething went wrong\\n---\n\n        cat \n$LOGFILE\n\n        echo \n---\n\n\n        kill_server\n    fi\ndone\n\necho -e \ndone\\n---\\n\n\n\nADDRESS=${ADDRESS##*:}\nPORT=${ADDRESS%/*}\nTOKEN=${ADDRESS#*=}\n\ncat \n EOF\nRun the following command on your desktop or laptop:\n\n   ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu\n\nLog in with your YubiKey/Cryptocard (there will be no prompt).\nThen open a browser and go to http://localhost:8888. The Jupyter web\ninterface will ask you for a token. Use the following:\n\n    $TOKEN\n\nNote that anyone to whom you give the token can access (and modify/delete)\nfiles in your GLADE spaces, regardless of the file permissions you\nhave set. SHARE TOKENS RARELY AND WISELY!\n\nRun the following commands on your destop or latptop:\n\n   ssh -N -l $USER -L 8080:${JNHOST}:8080 ${STHOST}.ucar.edu\n   ssh -N -l $USER -L 4040:${JNHOST}:4040 ${STHOST}.ucar.edu\n\nLog in with your YubiKey/Cryptocard(there will be no prompt).\nThen open a browser and go to http://localhost:8080 to access the Spark Master UI.\n\nFinally open a browser and go to http://localhost:4040 to access the Spark Master UI jobs\nhistory.\n\nTo stop the server, press Ctrl-C.\nEOF\n\n# Wait for user kill command\nsleep inf\n\n\n\n\nNote:\n Make the above scripts executable by running (\nchmod +x script_name.sh\n)\n\n\n2. Running Existing Cheyenne Spark Installation\n\n\n\n\nLog into Cheyenne\n\n\n\n\nCreate a working directory in your home and move into it:\n\n\n\n\nmkdir cheyenne\n\n\n`cd cheyenne\n\n\nmkdir spark\n\n\ncd spark\n\n\n\n\n\n\n\n\nCopy \nspark-cluster-scripts\n directory to \nspark\n directory\n\n\n\n\ncp -r /glade/p/work/abanihi/cheyenne/spark-cluster-scripts .\n\n\n\n\n\n\n\n\nIn the created \nspark\n directory, create \nconf\n directory:\n\n\n\n\nmkdir conf\n\n\n\n\n\n\n\n\nSchedule your job to run on the Cheyenne, by submitting your job through \npbs scheduler\n\n\n\n\nExample: \nqsub -I -l select=4:ncpus=1:mpiprocs=1 -l walltime=00:30:00 -q regular -A ProjectID\n\n\n\n\n\n\n\n\nChange current directory to \nspark-cluster-scripts\n\n\n\n\ncd spark-cluster-scripts\n\n\n\n\n\n\n\n\n2.1. Run PySpark Shell\n\n\n\n\nTo run PySpark shell, run \nstart-pyspark.sh\n by running \n./start-pyspark.sh\n. You should get something similar to this:\n\n\n\n\n2.2. Run PySpark in a Jupyter notebook\n\n\n\n\n\n\nTo run PySpark in a Jupyter notebook, make sure you that your current directory is \nspark-cluster-scripts\n and \n\n\n\n\nrun \nstart-sparknotebook\n by typing \n./start-sparknotebook\n and follow the instructions given.\n\n\n\n\n\n\n\n\nThere are two notebooks in the \nspark-cluster-scripts/\n directory. Run the \nSpark-Essentials\n notebook to test that Spark is running and that you have access to a cluster of nodes.", 
            "title": "Cheyenne"
        }, 
        {
            "location": "/installation/cheyenne/#apache-spark-211-on-cheyenne", 
            "text": "The Second task of our research project was to install the newest version of Apache Spark on Cheyenne.   As of today (June/2017), we've been able to install the newest version of Apache Spark -  Spark 2.1.1+Hadoop2.7 .  Below are the steps that we took to get Spark Up and Running on Cheyenne.  If all you need is to run the existing Apache Spark on Cheyenne, just skip to  Section 2  of this page.", 
            "title": "Apache Spark 2.1.1 on Cheyenne"
        }, 
        {
            "location": "/installation/cheyenne/#1-installation", 
            "text": "", 
            "title": "1. Installation"
        }, 
        {
            "location": "/installation/cheyenne/#11-downloading-spark-and-setting-up-sparks-directory-and-necessary-files", 
            "text": "The steps described in this section are similar to those for Yellowstone. Since both Yellowstone and Cheyenne have access to the same parallel filesytem, we decide to use the same downloaded Spark binaries.  The following section gives details on how Apache Spark would be installed on Cheyenne.   Log into Cheyenne  Change working directory to  /glade/p/work/abanihi  cd /glade/p/work/abanihi/     Go to  Apache Spark's official website  and follow steps 1-4 to get a download link for Spark. Copy the download link and     Go to  /glade/p/work/abanihi/  and download Spark   wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz  Untar  spark-2.1.1-bin-hadoop2.7.tgz  with   tar -xzf spark-2.1.1-bin-hadoop2.7.tgz       Change directory to  spark-2.1.1-bin-hadoop2.7/conf  and open  log4j.properties.template   cd spark-2.1.1-bin-hadoop2.7/conf  nano log4j.properties.template  Go to the following line:  log4j.rootCategory=INFO, console  and change  INFO  to  ERROR  Exit out of the editor     Rename  log4j.properties.template  to  log4j.properties   mv log4j.properties.template log4.properties     Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi/     Download H5Spark Package: This package supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark. For more details, please see this  page .   git clone https://github.com/valiantljk/h5spark.git  This package will be added to  Python Path  in  spark-cluster.sh  script.", 
            "title": "1.1 Downloading Spark and Setting up Spark's directory and necessary files"
        }, 
        {
            "location": "/installation/cheyenne/#12-scripts", 
            "text": "Even though the scripts for Yellowstone and Cheyenne have so much in common, there are some differences.    Change working directory to  /glade/p/work/abanihi   cd /glade/p/work/abanihi     Create a new directory called  cheyenne  and move into it    mkdir cheyenne  `cd cheyenne  Create  spark-cluster-scripts directory and move into it  mkdir spark-cluster-scripts  cd spark-cluster-scripts/     Create a new script and name it  spark-env.sh   nano spark-env.sh  spark-env.sh  should have the following content      ##!/usr/bin/env bash\n\nsource /etc/profile.d/modules.sh\n\nmodule restore system\nmodule swap intel gnu\nexport MODULEPATH=/glade/p/work/bdobbins/Modules:${MODULEPATH}\nmodule load java\nml python\nml numpy\nml jupyter\nml scipy\nml h5py\nml bottleneck\nml numexpr\nml pandas\nml pyside\nml matplotlib\nml pyngl\nml scikit-learn\nml netcdf4-python\nml cf_units\nml xarray\n\nexport SPARK_WORKER_DIR=/glade/scratch/$USER/spark/work\nexport SPARK_LOG_DIR=/glade/scratch/$USER/spark/logs\nexport SPARK_LOCAL_DIRS=/glade/scratch/$USER/spark/temp\nexport SPARK_LOCAL_IP=$(sed -e 's/\\([^.]*\\).*$/\\1/'   $(hostname))   Create a new script file and name it  spark-cluster.sh  nano spark-cluster.sh     #!/usr/bin/env bash\nexport SPARK_HOME=/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$PYTHONPATH:/glade/p/work/abanihi/h5spark/src/main/python/h5spark/:$PYTHONPATH\nexport SPARK_CONF_DIR=~/cheyenne/spark/conf\nexport SPARK_HOSTFILE=$SPARK_CONF_DIR/spark_hostfile\n\n# create temp hostfile\nexport SPARK_TEMP_HOSTFILE=$SPARK_CONF_DIR/spark_temp_hostfile\n\nrm $SPARK_HOSTFILE $SPARK_CONF_DIR/slaves\n\nexport MPI_SHEPHERD=true\nmpiexec_mpt hostname | grep -v Execute | sort   $SPARK_TEMP_HOSTFILE\n\n#sed -i 's/$/-ib/' $SPARK_TEMP_HOSTFILE\ncat $SPARK_TEMP_HOSTFILE | sort -u   $SPARK_HOSTFILE\ntail -n +2 $SPARK_TEMP_HOSTFILE | sort -u   $SPARK_CONF_DIR/slaves\ntail -n +2 $SPARK_TEMP_HOSTFILE | uniq -c   temp_ncores_slaves\n\nrm $SPARK_TEMP_HOSTFILE\n\n\nexport SPARK_MASTER_HOST=$(head -n 1 $SPARK_HOSTFILE)\nexport MASTER=spark://$SPARK_MASTER_HOST:7077\n\ncp spark-env.sh $SPARK_CONF_DIR/spark-env.sh\nsource $SPARK_CONF_DIR/spark-env.sh\n\nif [  $1  ==  start  ]; then\n    cmd_master= $SPARK_HOME/sbin/start-master.sh \n    cmd_slave= $SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker 1 $MASTER \nelif [  $1  ==  stop  ]; then\n    cmd_master= $SPARK_HOME/sbin/stop-master.sh \n    cmd_slave= $SPARK_HOME/sbin/spark-daemon.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker 1 \nelse\n    exit 1\nfi\n\n$cmd_master\n\nwhile read ncore_slave\ndo\n    ncore=$(echo $ncore_slave | cut -d' ' -f1)\n    slave=$(echo $ncore_slave | cut -d' ' -f2)\n\n    if [  $slave  ==  $SPARK_MASTER_HOST  ]; then\n          echo  On Master node.  Running: cmd_slave --cores $ncore \n          $cmd_slave --cores $ncore\n     else\n          echo  On Worker node.  Running: cmd_slave --cores $ncore \n          ssh $slave  $cmd_slave  --cores $ncore   /dev/null\n\n    fi\ndone  temp_ncores_slaves   Create a new script file and name it  start-pyspark.sh   #!/usr/bin/env bash\n\nsource spark-cluster.sh start\n$SPARK_HOME/bin/pyspark --master $MASTER   Create a new script file and name it  start-sparknotebook . This script file is an extension of  /glade/apps/opt/jupyter/5.0.0/gnu/4.8.2/bin/start-notebook  script file.   #!/usr/bin/env bash\n\nsource spark-cluster.sh start\n\n# Add the PySpark classes to the Python path:\nexport PATH= $SPARK_HOME:$PATH \nexport PATH=$PATH:$SPARK_HOME/bin\nexport PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\nexport PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n\n# Create trap to kill notebook when user is done\nkill_server() {\n    if [[  $JNPID  != -1 ]]; then\n        echo -en  \\nKilling Jupyter Notebook Server with PID=$JNPID ...  \n        kill  $JNPID \n        echo  done \n        exit 0\n    else\n        exit 1\n    fi\n}\n\nJNPID=-1\ntrap kill_server SIGHUP SIGINT SIGTERM\n\n# Begin server creation\nJNHOST=$(hostname)\nLOGDIR=/glade/scratch/${USER}/.jupyter-notebook\nLOGFILE=${LOGDIR}/log.$(date +%Y%m%dT%H%M%S)\nmkdir -p  $LOGDIR \n\nif [[ $JNHOST == ch* || $JNHOST == r* ]]; then\n    STHOST=cheyenne\nelse\n    STHOST=yellowstone\nfi\n\necho  Logging this session in $LOGFILE \n\n# Check if running on login nodes\nif [[ $JNHOST == yslogin* ]]; then\ncat   EOF\n\nSee  Use of login nodes  here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/yellowstone/using_resources.\n\nConsider running on Geyser instead by using execgy to start a session. (Run execgy -help.)\nEOF\nelif [[ $JNHOST == cheyenne* ]]; then\ncat   EOF\n\nSee  Use of login nodes  here before running Jupyter Notebook on this\nnode: https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne/running-jobs.\n\nConsider running in an interactive job instead by using qinteractive. (Run qinterative -help.)\nEOF\nfi\n\n\njupyter notebook  $@  --no-browser --ip= $JNHOST   $LOGFILE  2 1  \nJNPID=$!\n\n\necho -en   \\nStarting jupyter notebook server, please wait ...  \n\nELAPSED=0\nADDRESS=\n\nwhile [[ $ADDRESS != * ${JNHOST} * ]]; do\n    sleep 1\n    ELAPSED=$((ELAPSED+1))\n    ADDRESS=$(tail -n 1  $LOGFILE )\n\n    if [[ $ELAPSED -gt 30 ]]; then\n        echo -e  something went wrong\\n--- \n        cat  $LOGFILE \n        echo  --- \n\n        kill_server\n    fi\ndone\n\necho -e  done\\n---\\n \n\nADDRESS=${ADDRESS##*:}\nPORT=${ADDRESS%/*}\nTOKEN=${ADDRESS#*=}\n\ncat   EOF\nRun the following command on your desktop or laptop:\n\n   ssh -N -l $USER -L 8888:${JNHOST}:$PORT ${STHOST}.ucar.edu\n\nLog in with your YubiKey/Cryptocard (there will be no prompt).\nThen open a browser and go to http://localhost:8888. The Jupyter web\ninterface will ask you for a token. Use the following:\n\n    $TOKEN\n\nNote that anyone to whom you give the token can access (and modify/delete)\nfiles in your GLADE spaces, regardless of the file permissions you\nhave set. SHARE TOKENS RARELY AND WISELY!\n\nRun the following commands on your destop or latptop:\n\n   ssh -N -l $USER -L 8080:${JNHOST}:8080 ${STHOST}.ucar.edu\n   ssh -N -l $USER -L 4040:${JNHOST}:4040 ${STHOST}.ucar.edu\n\nLog in with your YubiKey/Cryptocard(there will be no prompt).\nThen open a browser and go to http://localhost:8080 to access the Spark Master UI.\n\nFinally open a browser and go to http://localhost:4040 to access the Spark Master UI jobs\nhistory.\n\nTo stop the server, press Ctrl-C.\nEOF\n\n# Wait for user kill command\nsleep inf  Note:  Make the above scripts executable by running ( chmod +x script_name.sh )", 
            "title": "1.2 Scripts"
        }, 
        {
            "location": "/installation/cheyenne/#2-running-existing-cheyenne-spark-installation", 
            "text": "Log into Cheyenne   Create a working directory in your home and move into it:   mkdir cheyenne  `cd cheyenne  mkdir spark  cd spark     Copy  spark-cluster-scripts  directory to  spark  directory   cp -r /glade/p/work/abanihi/cheyenne/spark-cluster-scripts .     In the created  spark  directory, create  conf  directory:   mkdir conf     Schedule your job to run on the Cheyenne, by submitting your job through  pbs scheduler   Example:  qsub -I -l select=4:ncpus=1:mpiprocs=1 -l walltime=00:30:00 -q regular -A ProjectID     Change current directory to  spark-cluster-scripts   cd spark-cluster-scripts", 
            "title": "2. Running Existing Cheyenne Spark Installation"
        }, 
        {
            "location": "/installation/cheyenne/#21-run-pyspark-shell", 
            "text": "To run PySpark shell, run  start-pyspark.sh  by running  ./start-pyspark.sh . You should get something similar to this:", 
            "title": "2.1. Run PySpark Shell"
        }, 
        {
            "location": "/installation/cheyenne/#22-run-pyspark-in-a-jupyter-notebook", 
            "text": "To run PySpark in a Jupyter notebook, make sure you that your current directory is  spark-cluster-scripts  and    run  start-sparknotebook  by typing  ./start-sparknotebook  and follow the instructions given.     There are two notebooks in the  spark-cluster-scripts/  directory. Run the  Spark-Essentials  notebook to test that Spark is running and that you have access to a cluster of nodes.", 
            "title": "2.2. Run PySpark in a Jupyter notebook"
        }, 
        {
            "location": "/usage/yellowstone/", 
            "text": "Installation and Use\n\n\nSpark 2.1.1+Hadoop2.7 is already installed and ready to use on both Cheyenne and Yellowstone.\n\n\nPrerequisites:\n \n\n\n\n\nIf this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go \nhere\n.\n\n\n\n\n1. Yellowstone\n\n\n\n\n\n\nLog into Yellowstone\n\n\n\n\n\n\nCopy \nyellowstone\n directory into your home directory by running \n\n\n\n\n\n\ncp -r /glade/p/work/abanihi/yellowstone/ .\n\n\n\n\n\n\nSchedule your job to run on the Yellowstone, by submitting your job through \nlsf scheduler\n\n\n\n\nExample: \nbsub -Is -W 01:00 -q small -P ProjectID -R \"span[ptile=1]\" -n 4 bash\n\n\n\n\n\n\n\n\n1.1. Run PySpark Shell\n\n\n\n\nTo run PySpark shell, run \n~/yellowstone/spark/spark-cluster-scripts/start-pyspark.sh\n\n\n\n\nIf everything is well setup, you should get something similar to this:\n\n\n\n\nWhen you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize \nSparkSession\n and \nSparkContext\n before starting to use Spark.\n\n\n\n\n# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Initialize SparkSession and attach a sparkContext to the created sparkSession\nspark = SparkSession.builder.appName(\npyspark\n).getOrCreate()\nsc = spark.sparkContext\n\n\n\n\n\n\n\nIf you need to use \nSpark Master WebUI\n, consider running spark on Cheyenne. As of now, Spark Master WebUI is not available on Yellowstone.\n\n\n\n\n1.2. Run PySpark in a Jupyter notebook\n\n\n\n\n\n\nTo run PySpark in a Jupyter notebook:\n\n\n\n\nrun \n~/yellowstone/spark/spark-cluster-scripts/start-sparknotebook\n and follow the instructions given.\n\n\n\n\n\n\n\n\nThere are two notebooks in the \nspark-cluster-scripts/\n directory. Run the \nSpark-Essentials\n notebook to test that Spark is running and that you have access to a cluster of nodes.\n\n\n\n\n\n\nNOTE:\n We've not been able to get SparkUI feature working on Yellowstone yet!", 
            "title": "Yellowstone"
        }, 
        {
            "location": "/usage/yellowstone/#installation-and-use", 
            "text": "Spark 2.1.1+Hadoop2.7 is already installed and ready to use on both Cheyenne and Yellowstone.  Prerequisites:     If this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go  here .", 
            "title": "Installation and Use"
        }, 
        {
            "location": "/usage/yellowstone/#1-yellowstone", 
            "text": "Log into Yellowstone    Copy  yellowstone  directory into your home directory by running     cp -r /glade/p/work/abanihi/yellowstone/ .    Schedule your job to run on the Yellowstone, by submitting your job through  lsf scheduler   Example:  bsub -Is -W 01:00 -q small -P ProjectID -R \"span[ptile=1]\" -n 4 bash", 
            "title": "1. Yellowstone"
        }, 
        {
            "location": "/usage/yellowstone/#11-run-pyspark-shell", 
            "text": "To run PySpark shell, run  ~/yellowstone/spark/spark-cluster-scripts/start-pyspark.sh   If everything is well setup, you should get something similar to this:   When you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize  SparkSession  and  SparkContext  before starting to use Spark.   # Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Initialize SparkSession and attach a sparkContext to the created sparkSession\nspark = SparkSession.builder.appName( pyspark ).getOrCreate()\nsc = spark.sparkContext   If you need to use  Spark Master WebUI , consider running spark on Cheyenne. As of now, Spark Master WebUI is not available on Yellowstone.", 
            "title": "1.1. Run PySpark Shell"
        }, 
        {
            "location": "/usage/yellowstone/#12-run-pyspark-in-a-jupyter-notebook", 
            "text": "To run PySpark in a Jupyter notebook:   run  ~/yellowstone/spark/spark-cluster-scripts/start-sparknotebook  and follow the instructions given.     There are two notebooks in the  spark-cluster-scripts/  directory. Run the  Spark-Essentials  notebook to test that Spark is running and that you have access to a cluster of nodes.    NOTE:  We've not been able to get SparkUI feature working on Yellowstone yet!", 
            "title": "1.2. Run PySpark in a Jupyter notebook"
        }, 
        {
            "location": "/usage/cheyenne/", 
            "text": "Quick Start: Cheyenne\n\n\nPrerequisites:\n \n\n\n\n\nIf this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go \nhere\n.\n\n\n\n\nLogging in\n\n\n\n\n\n\nLog in to the Cheyenne system from the terminal\n\n\n\n\nssh -X -l username cheyenne.ucar.edu\n\n\n\n\n\n\n\n\nCopy \ncheyenne\n directory into your home directory by running \n\n\n\n\ncp -r /glade/p/work/abanihi/cheyenne/ .\n\n\n\n\n\n\n\n\nSubmitting jobs\n\n\nSpark can be run interactively ( via ipython shell, jupyter notebook) or in batch mode. \n\n\n1. Interactive jobs\n\n\nTo start an interactive job, use the qsub command with the necessary options.\n\n\n\n\nqsub -I -l select=1:ncpus=36:mpiprocs=36 -l walltime=01:00 -q small -A project_code\n\n\n\n\n1.1 Load IPython shell with PySpark\n\n\n\n\n\n\nTo start IPython shell with PySpark, run the following:\n\n\n\n\n~/cheyenne/spark/spark-cluster-scripts/start-pyspark.sh\n\n\n\n\n\n\n\n\n\n\n1.2. Run PySpark in a Jupyter notebook\n\n\n\n\nTo run PySpark in a Jupyter notebook, run the following:\n\n\n~/cheyenne/spark/spark-cluster-scripts/start-sparknotebook\n and follow the instructions given.\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\nWhen you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize \nSparkSession\n and \nSparkContext\n before starting to use Spark.\n\n\n# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Initialize SparkSession and attach a sparkContext to the created sparkSession\nspark = SparkSession.builder.appName(\npyspark\n).getOrCreate()\nsc = spark.sparkContext\n\n\n\n\n\n2. Batch jobs\n\n\nTo submit a Spark batch job, use the qsub command followed by the name of your PBS batch script file.\n- \nqsub script_name\n\n\n2. 1 Spark job script example\n\n\nBatch script to run a Spark job:\n\n\n\n\nspark-test.sh\n\n\n\n\n#!/usr/bin/env bash\n#PBS -A project_code\n#PBS -j oe\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00\n#PBS -l select=1:ncpus=4:mpiprocs=4\n\nsource ~/cheyenne/spark/spark-cluster-scripts/spark-cluster.sh start\n$SPARK_HOME/bin/spark-submit --master $MASTER ~/cheyenne-jobs/spark-test.py\n\n\n\n\n\n\nspark-test.py\n\n\n\n\nfrom __future__ import print_function\nfrom read import RDD, DataFrame\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('spark-test').getOrCreate()\nsc = spark.sparkContext\nsc.addPyFile(\n/glade/p/work/abanihi/pyspark4climate/read.py\n)\nfilepath = '/glade/u/home/abanihi/data/pres_monthly_1948-2008.nc'\nvar = 'pres'\ndata_df = DataFrame(sc, (filepath, var), 'single')\ndf = data_df.df\nprint(df.show())\n\n\n\n\nTo run this spark job, run:\n\n\n\n\nqsub spark-test.sh", 
            "title": "Cheyenne"
        }, 
        {
            "location": "/usage/cheyenne/#quick-start-cheyenne", 
            "text": "Prerequisites:     If this is your first time running interactive jobs on multiple nodes, or if you've never installed SSH keys in your yellowstone/cheyenne user environment, installing SSH keys on Yellowstone/Cheyenne will simplify the process of running Spark jobs. For more details on how to install SSH keys go  here .", 
            "title": "Quick Start: Cheyenne"
        }, 
        {
            "location": "/usage/cheyenne/#logging-in", 
            "text": "Log in to the Cheyenne system from the terminal   ssh -X -l username cheyenne.ucar.edu     Copy  cheyenne  directory into your home directory by running    cp -r /glade/p/work/abanihi/cheyenne/ .", 
            "title": "Logging in"
        }, 
        {
            "location": "/usage/cheyenne/#submitting-jobs", 
            "text": "Spark can be run interactively ( via ipython shell, jupyter notebook) or in batch mode.", 
            "title": "Submitting jobs"
        }, 
        {
            "location": "/usage/cheyenne/#1-interactive-jobs", 
            "text": "To start an interactive job, use the qsub command with the necessary options.   qsub -I -l select=1:ncpus=36:mpiprocs=36 -l walltime=01:00 -q small -A project_code", 
            "title": "1. Interactive jobs"
        }, 
        {
            "location": "/usage/cheyenne/#11-load-ipython-shell-with-pyspark", 
            "text": "To start IPython shell with PySpark, run the following:   ~/cheyenne/spark/spark-cluster-scripts/start-pyspark.sh", 
            "title": "1.1 Load IPython shell with PySpark"
        }, 
        {
            "location": "/usage/cheyenne/#12-run-pyspark-in-a-jupyter-notebook", 
            "text": "To run PySpark in a Jupyter notebook, run the following:  ~/cheyenne/spark/spark-cluster-scripts/start-sparknotebook  and follow the instructions given.      Note:  When you run PySpark shell, SparkSession (single point of entry to interact with underlying Spark functionality) is created for you. This is not the case for the Jupyter notebook. Once the jupyter notebook is running, you will need to create and Initialize  SparkSession  and  SparkContext  before starting to use Spark.  # Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Initialize SparkSession and attach a sparkContext to the created sparkSession\nspark = SparkSession.builder.appName( pyspark ).getOrCreate()\nsc = spark.sparkContext", 
            "title": "1.2. Run PySpark in a Jupyter notebook"
        }, 
        {
            "location": "/usage/cheyenne/#2-batch-jobs", 
            "text": "To submit a Spark batch job, use the qsub command followed by the name of your PBS batch script file.\n-  qsub script_name", 
            "title": "2. Batch jobs"
        }, 
        {
            "location": "/usage/cheyenne/#2-1-spark-job-script-example", 
            "text": "Batch script to run a Spark job:   spark-test.sh   #!/usr/bin/env bash\n#PBS -A project_code\n#PBS -j oe\n#PBS -m abe\n#PBS -M email_address\n#PBS -q queue_name\n#PBS -l walltime=01:00\n#PBS -l select=1:ncpus=4:mpiprocs=4\n\nsource ~/cheyenne/spark/spark-cluster-scripts/spark-cluster.sh start\n$SPARK_HOME/bin/spark-submit --master $MASTER ~/cheyenne-jobs/spark-test.py   spark-test.py   from __future__ import print_function\nfrom read import RDD, DataFrame\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('spark-test').getOrCreate()\nsc = spark.sparkContext\nsc.addPyFile( /glade/p/work/abanihi/pyspark4climate/read.py )\nfilepath = '/glade/u/home/abanihi/data/pres_monthly_1948-2008.nc'\nvar = 'pres'\ndata_df = DataFrame(sc, (filepath, var), 'single')\ndf = data_df.df\nprint(df.show())  To run this spark job, run:   qsub spark-test.sh", 
            "title": "2. 1 Spark job script example"
        }, 
        {
            "location": "/pyspark4climate/", 
            "text": "PySpark4Climate Package\n\n\n1. Introduction\n\n\n\n\n\n\nApache Spark is an open source cluster computing framework\n\n\n\n\n\n\nIt was developed at UCB AMPLab, 2014 v1.0, 2016 v.2\n\n\n\n\nActively developed, 1086 contributors, 19, 788 commits (As of June 5, 2017)\n\n\n\n\n\n\n\n\nProductive programming interface\n\n\n\n\n6 vs 28 lines of code compare to hadoop mapreduce\n\n\n\n\n\n\n\n\nImplicit data parallelism\n\n\n\n\nFault-tolerance\n\n\n\n\n\n\n\n\nSpark for Data-intensive Computing\n\n\n\n\nStreaming processing\n\n\nSQL\n\n\nMachine Learning, MLlib\n\n\nGraph Processing\n\n\n\n\n\n\n\n\n2. Porting Apache Spark software stack on HPC\n\n\n\n\n\n\nAdvantages of Porting Spark onto HPC\n\n\n\n\nA more productive API for data-intensive computing\n\n\nRelieve the users from:\n\n\nconcurrency control\n\n\ncommunication\n\n\nmemory management with traditional MPI model\n\n\n\n\n\n\nEmbarrassingly parallel computing, \ndata.map(func)\n\n\nFault tolerance, \nrecompute()\n\n\n\n\n\n\n\n\nHPC applications often rely on hierarchical data formats to organize files and datasets. \nHowever, accessing data stored in HDF5/netCDF4 files is not natively supported in Spark.\n \n\n\n\n\n\n\nReasons of lack of netCDF/HDF5 support in Spark include:\n\n\n\n\nLack of an API in Spark to directly load or sub-select HDF5/netCDF datasets into its in-memory data structures.\n\n\nHDF5/netCDF have a deep hierarchy, which cannot simply be treated as a sequence of bytes nor be evenly divided.\n\n\nGPFS file system is not well tuned for SPARK I/O and vice versa.\n\n\n\n\n\n\n\n\n3. Data in Spark\n\n\nResilient Distributed Datasets(RDDs)\n are:\n\n\n\n\n\n\nThe primary abstraction in Spark\n\n\n\n\nImmutable(Read-Only) once constructed\n\n\nSpark tracks lineage information to efficiently recompute lost data\n\n\nEnable operations on collection of elements in parallel\n\n\n\n\n\n\n\n\nYou construct RDDs\n\n\n\n\nby parallelizing existing Python collections (lists)\n\n\nby transforming an existing RDDs\n\n\nfrom files in HDFS or any other storage system (glade in case of Yellowstone and Cheyenne)\n\n\n\n\n\n\n\n\nThe programmer needs to specify the number of partitions for an RDD or the default value is used if unspecified.\n\n\n\n\n\n\n\n\nImage Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark\n\n\nThere are two types of operations on RDDs: \nTransformations\n and \nActions\n.\n\n\n\n\nTransformations\n are lazy in a sense that they are not computed immediately\n\n\nTransformed RDD is executed when action runs on it.\n\n\nRDDs can be persisted(cached) in memory or disk.\n\n\n\n\nWorking with RDDs\n:\n\n\n\n\nCreate an RDD from a data source\n\n\nApply transformations to an RDD: \n.map(...)\n\n\nApply actions to an RDD: \n.collect()\n, \n.count()\n\n\n\n\n\n\n\n\nImage Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark\n\n\n4. Data in HDF5/netCDF4\n\n\n\n\nHierarchical Data Format v5\n\n\n\n\n\n5. Support HDF5/netCDF4 in Spark 2.x Series\n\n\n\n\n\n\nWhat does Spark have in reading various data formats?\n\n\n\n\nTextfile: \nsc.textFile()\n or \nspark.read.load(\"filename.txt\", format=\"txt\")\n\n\nParquet: \nspark.read.load(\"filename.parquet\", format=\"parquet\")\n\n\nJson: \nspark.read.load(\"filename.json\", format=\"json\")\n\n\ncsv: \nspark.read.load(\"filename.csv\", format=\"csv)\n\n\njdbc: \nspark.read.load(\"filename.jdbc\", format=\"jdbc\")\n\n\norc, libsvm follow the same pattern.\n\n\n\n\n\n\n\n\nHow about HDF5/netCDF4?:\n\n\n\n\nThere was no such thing as \nspark.read.load(\"filename.hdf5\", format=\"hdf5\")\n or \nspark.read.load(\"filename.nc\", format=\"nc\")\n until \nH5Spark Package\n came along!!\n\n\n\n\n\n\n\n\nChallenges: Functionality and Performance\n\n\n\n\nHow to transform an HDF5 dataset into an RDD?\n\n\nHow to utilize the HDF5 I/O libraries in Spark?\n\n\nHow to enable parallel I/O on HPC?\n\n\nWhat is the impact of a parallel filesytem striping like \nGPFS\n?\n\n\nWhat is the effect of Caching on I/O in Spark?\n\n\n\n\n\n\n\n\nH5Spark\n is a Spark Package that:\n- Supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark.\n\n\n\n\nOptimizes I/O performance on HPC with Lustre Filesytems Tuning.\n\n\n\n\n6. H5Spark Design\n\n\nH5Spark has 4 major components:\n\n\n1. Meta data analyzer \n2. RDD seeder\n3. Hyperslab partitioner\n4. RDD constructor\n\n\n\n\n\n\n\nFor loading a single HDF5 file:\n\n\n\n\n\n\nH5Spark metadata analyzer takes the user's input filename and dataset name and triggers \nfirst\n I/O call to the file system to fetch the HDF5 file metadata info\n\n\n\n\n\n\nI/O calls \nh5fopen\n and \nh5dopen\n are called to return size of each dimension of queried dataset.\n\n\n\n\n\n\nSpark partition parameter is used to control the degree of parallelism.\n\n\n\n\nE.g: If the user chooses 10 partitions, then there will be at most 10 parallel tasks to process the RDD.\n\n\n\n\n\n\n\n\n\n\n\n\nPros of H5Spark\n\n\n\n\nParallel I/O is transparently handled without user's interaction\n\n\nH5Spark's I/O is an \nMPI-Like\n independent I/O and this means that:\n\n\neach executor will issue the I/O independently without communicating with other executors.\n\n\n\n\n\n\n\n\n\n\n\n\nScala/Python implementation\n\n\n\n\nSpark favors Scala and Python\n\n\nH5Spark uses HDF5 Python library\n\n\nUnderneath is HDF5 C posix library\n\n\nNo MPIIO support \n\n\n\n\n\n\n\n\n\n\nAs it's shown in the figure below, \nH5Spark\n was designed for and tested on \nLustre parallel filesytem\n. \n\n\n\n\nTODO:\n Test H5Spark on NCAR's GPFS filesytem.\n\n\nReferences:\n\n\n\n\n\n\nJ.L. Liu, E. Racah, Q. Koziol, R. S. Canon, A. Gittens, L. Gerhardt, S. Byna, M. F. Ringenburg, Prabhat. \"H5Spark: Bridging the I/O Gap between Spark and Scientific Data Formats on HPC Systems\", Cray User Group, 2016, (\nPaper\n)\n\n\n\n\n\n\nH5Spark GitHub Repo", 
            "title": "PySpark4Climate Package"
        }, 
        {
            "location": "/pyspark4climate/#pyspark4climate-package", 
            "text": "", 
            "title": "PySpark4Climate Package"
        }, 
        {
            "location": "/pyspark4climate/#1-introduction", 
            "text": "Apache Spark is an open source cluster computing framework    It was developed at UCB AMPLab, 2014 v1.0, 2016 v.2   Actively developed, 1086 contributors, 19, 788 commits (As of June 5, 2017)     Productive programming interface   6 vs 28 lines of code compare to hadoop mapreduce     Implicit data parallelism   Fault-tolerance     Spark for Data-intensive Computing   Streaming processing  SQL  Machine Learning, MLlib  Graph Processing", 
            "title": "1. Introduction"
        }, 
        {
            "location": "/pyspark4climate/#2-porting-apache-spark-software-stack-on-hpc", 
            "text": "Advantages of Porting Spark onto HPC   A more productive API for data-intensive computing  Relieve the users from:  concurrency control  communication  memory management with traditional MPI model    Embarrassingly parallel computing,  data.map(func)  Fault tolerance,  recompute()     HPC applications often rely on hierarchical data formats to organize files and datasets.  However, accessing data stored in HDF5/netCDF4 files is not natively supported in Spark.      Reasons of lack of netCDF/HDF5 support in Spark include:   Lack of an API in Spark to directly load or sub-select HDF5/netCDF datasets into its in-memory data structures.  HDF5/netCDF have a deep hierarchy, which cannot simply be treated as a sequence of bytes nor be evenly divided.  GPFS file system is not well tuned for SPARK I/O and vice versa.", 
            "title": "2. Porting Apache Spark software stack on HPC"
        }, 
        {
            "location": "/pyspark4climate/#3-data-in-spark", 
            "text": "Resilient Distributed Datasets(RDDs)  are:    The primary abstraction in Spark   Immutable(Read-Only) once constructed  Spark tracks lineage information to efficiently recompute lost data  Enable operations on collection of elements in parallel     You construct RDDs   by parallelizing existing Python collections (lists)  by transforming an existing RDDs  from files in HDFS or any other storage system (glade in case of Yellowstone and Cheyenne)     The programmer needs to specify the number of partitions for an RDD or the default value is used if unspecified.     Image Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark  There are two types of operations on RDDs:  Transformations  and  Actions .   Transformations  are lazy in a sense that they are not computed immediately  Transformed RDD is executed when action runs on it.  RDDs can be persisted(cached) in memory or disk.   Working with RDDs :   Create an RDD from a data source  Apply transformations to an RDD:  .map(...)  Apply actions to an RDD:  .collect() ,  .count()     Image Courtesy: BerkeleyX-CS100.1x-Big-Data-with-Apache-Spark", 
            "title": "3. Data in Spark"
        }, 
        {
            "location": "/pyspark4climate/#4-data-in-hdf5netcdf4", 
            "text": "Hierarchical Data Format v5", 
            "title": "4. Data in HDF5/netCDF4"
        }, 
        {
            "location": "/pyspark4climate/#5-support-hdf5netcdf4-in-spark-2x-series", 
            "text": "What does Spark have in reading various data formats?   Textfile:  sc.textFile()  or  spark.read.load(\"filename.txt\", format=\"txt\")  Parquet:  spark.read.load(\"filename.parquet\", format=\"parquet\")  Json:  spark.read.load(\"filename.json\", format=\"json\")  csv:  spark.read.load(\"filename.csv\", format=\"csv)  jdbc:  spark.read.load(\"filename.jdbc\", format=\"jdbc\")  orc, libsvm follow the same pattern.     How about HDF5/netCDF4?:   There was no such thing as  spark.read.load(\"filename.hdf5\", format=\"hdf5\")  or  spark.read.load(\"filename.nc\", format=\"nc\")  until  H5Spark Package  came along!!     Challenges: Functionality and Performance   How to transform an HDF5 dataset into an RDD?  How to utilize the HDF5 I/O libraries in Spark?  How to enable parallel I/O on HPC?  What is the impact of a parallel filesytem striping like  GPFS ?  What is the effect of Caching on I/O in Spark?     H5Spark  is a Spark Package that:\n- Supports Hierarchical Data Format, HDF5/netCDF4 and Rich Parallel I/O interface in Spark.   Optimizes I/O performance on HPC with Lustre Filesytems Tuning.", 
            "title": "5. Support HDF5/netCDF4 in Spark 2.x Series"
        }, 
        {
            "location": "/pyspark4climate/#6-h5spark-design", 
            "text": "H5Spark has 4 major components:  1. Meta data analyzer \n2. RDD seeder\n3. Hyperslab partitioner\n4. RDD constructor    For loading a single HDF5 file:    H5Spark metadata analyzer takes the user's input filename and dataset name and triggers  first  I/O call to the file system to fetch the HDF5 file metadata info    I/O calls  h5fopen  and  h5dopen  are called to return size of each dimension of queried dataset.    Spark partition parameter is used to control the degree of parallelism.   E.g: If the user chooses 10 partitions, then there will be at most 10 parallel tasks to process the RDD.       Pros of H5Spark   Parallel I/O is transparently handled without user's interaction  H5Spark's I/O is an  MPI-Like  independent I/O and this means that:  each executor will issue the I/O independently without communicating with other executors.       Scala/Python implementation   Spark favors Scala and Python  H5Spark uses HDF5 Python library  Underneath is HDF5 C posix library  No MPIIO support       As it's shown in the figure below,  H5Spark  was designed for and tested on  Lustre parallel filesytem .    TODO:  Test H5Spark on NCAR's GPFS filesytem.  References:    J.L. Liu, E. Racah, Q. Koziol, R. S. Canon, A. Gittens, L. Gerhardt, S. Byna, M. F. Ringenburg, Prabhat. \"H5Spark: Bridging the I/O Gap between Spark and Scientific Data Formats on HPC Systems\", Cray User Group, 2016, ( Paper )    H5Spark GitHub Repo", 
            "title": "6. H5Spark Design"
        }
    ]
}