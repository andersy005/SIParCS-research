{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PySpark4Climate I/O primer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces some of the functionalities supported by PySpark4Climate ```read module``` and how to use PySpark4Climate ```read module``` in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hide warnings if there are any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import read\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-read-test2\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's direct Spark to make **pyspark4climate read module** available to all executors by using ```sc.addPyFiles()``` function option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(\"/glade/p/work/abanihi/pyspark4climate/read.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module read:\n",
      "\n",
      "NAME\n",
      "    read\n",
      "\n",
      "FILE\n",
      "    /glade/p/work/abanihi/pyspark4climate/read.py\n",
      "\n",
      "DESCRIPTION\n",
      "    This module ingests netCDF file formats into Spark as:\n",
      "        - a resilient distributed dataset(RDD)\n",
      "        - a distributed dataframe\n",
      "    \n",
      "    Attributes:\n",
      "        PARTITIONS (int): default number of partitions to be used by Spark.\n",
      "    \n",
      "    TODO:\n",
      "        * Support multiple files reading\n",
      "        * Convert time_indices from numbers to dates\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        dataset\n",
      "    \n",
      "    class dataset(__builtin__.object)\n",
      "     |  Defines and initializes netCDF file attributes needed by Spark.\n",
      "     |  Attributes:\n",
      "     |      filepath                 (str)   :  path for the file to be read\n",
      "     |      variable_name            (str)   :  variable name\n",
      "     |      dims                     (tuple) :  dimensions (excluding time dimension) of the variable of interest\n",
      "     |      ndims                    (int)   :  size of dims tuple\n",
      "     |      partitions               (int)   :  number of partitions to be used by spark\n",
      "     |      other_dims_values_tuple  (list)  :  list of tuples containing cartesian product of all dims values\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples:\n",
      "     |      >>> dset = dataset(('ta_Amon_CCSM4_historical_r1i1p1_185001-189912.nc', 'ta'))\n",
      "     |      >>> print(dset.partitions)\n",
      "     |      75\n",
      "     |      >>> print(dset.variable_name)\n",
      "     |      ta\n",
      "     |      >>> print(dset.ndims)\n",
      "     |      3\n",
      "     |      >>> print(dset.dims)\n",
      "     |      (u'plev', u'lat', u'lon')\n",
      "     |      >>> print(dset.other_dims_values_tuple[:2])\n",
      "     |      [(100000.0, -90.0, 0.0), (100000.0, -90.0, 1.25)]\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, filepath_variable_tuple=None)\n",
      "     |      Args:\n",
      "     |          filepath_variable_tuple (tuple): tuple containing (filepath, 'variable')\n",
      "     |  \n",
      "     |  generate_cartesian_product(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    create_rdd(sc, file_list_or_txt_file, mode='multi', partitions=None)\n",
      "        Create an RDD from a file_list or tuple of (filepath, variable) and Returns the RDD.\n",
      "        \n",
      "        Args:\n",
      "            sc                     (object)       : sparkContext Object\n",
      "            file_list_or_txt_file  (list or tuple : list of tuples or a tuple of the format (filepath, variable)\n",
      "            mode                   (str)          : If reading multiple files (multi), otherwise(single)\n",
      "            partitions             (int)          : number of partitions\n",
      "    \n",
      "    dataframe(sc, file_list_or_txt_file, mode='multi', partitions=None)\n",
      "        Creates a distributed dataframe from a netCDF file.\n",
      "        \n",
      "        Args:\n",
      "            sc                     (object)       : sparkContext Object\n",
      "            file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
      "            partitions             (int)          : number of partitions\n",
      "            mode                   (str)          : (multi) if reading multiple files, otherwise(single)\n",
      "        Returns:\n",
      "            df                  (dataframe)          : Spark's distributed data frame\n",
      "    \n",
      "    flatten_data(line)\n",
      "        Flattens numpy array and return a tuple of each value\n",
      "        and its corresponding lat_lon coordinates together with other dimensions.\n",
      "        \n",
      "        Args:\n",
      "            line (tuple) :  an rdd element in the form of a tuple (data, idx) where data is\n",
      "                            a numpy array and idx correspond to time index.\n",
      "        \n",
      "        Returns:\n",
      "             results (tuple): a transformed rdd element in the form\n",
      "                               of a tuple (idx, dim1_value, dim_value2, ..., data_value)\n",
      "    \n",
      "    rdd_to_df(rdd)\n",
      "        Function that converts an RDD into a Spark data frame.\n",
      "        Arguments:\n",
      "            - rdd: (rdd)\n",
      "        \n",
      "        Returns:\n",
      "            - df: Spark dataframe\n",
      "    \n",
      "    read_nc_single_chunked(sc, filepath_variable_tuple, partitions=None)\n",
      "        Generates an RDD using the information passed by create_rdd function.\n",
      "         Args:\n",
      "            sc                     (object)       : sparkContext Object\n",
      "            file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
      "            partitions             (int)          : number of partitions\n",
      "        \n",
      "        Returns:\n",
      "            rdd_                   (rdd)          : Spark's resilient distributed dataset\n",
      "    \n",
      "    readonep(filepath_, variable_, start_idx, chunk_size)\n",
      "        Read a slice from one file.\n",
      "        \n",
      "        Args:\n",
      "            filepath_    (str): string containing the file path\n",
      "            variable_    (str): variable name\n",
      "            start_idx    (int): starting index\n",
      "            chunk_size   (int): the chunk size to be read at a time.\n",
      "        \n",
      "        Returns:\n",
      "            list:   list of the chunk read\n",
      "    \n",
      "    row_transform(line)\n",
      "        Transforms a a tuple (idx, dim1_value, dim_value2, ..., data_value) into a Spark sql\n",
      "           Row object.\n",
      "        \n",
      "        Args:\n",
      "            line (tuple): a tuple of the form (idx, dim1_value, dim_value2, ..., data_value)\n",
      "        \n",
      "        Returns:\n",
      "            row(*line) : Spark Row object with arbitray number of items depending on the size of\n",
      "                         the tuple in line.\n",
      "        \n",
      "        Examples:\n",
      "            >>> print(line)\n",
      "            (0, 100000.0, -90.0, 0.0, 257.8)\n",
      "            >>> row(*line)\n",
      "            Row(time=0, plev=100000.0, lat=-90.0, lon=0.0, ta=257.8)\n",
      "\n",
      "DATA\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.Kryoserializer.buffer.max.mb=4096\n",
      "spark.app.name=spark-read-test2\n",
      "spark.driver.maxResultSize=10g\n",
      "spark.driver.memory=20g\n",
      "spark.executor.memory=15g\n",
      "spark.master=spark://r1i6n24.ib0.cheyenne.ucar.edu:7077\n",
      "spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
      "spark.speculation=True\n",
      "spark.submit.deployMode=client\n"
     ]
    }
   ],
   "source": [
    "# Print some information about Spark's configuration\n",
    "print(SparkConf().toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For this tutorial we will be using the following dataset.\n",
    "- ```/glade/p/CMIP/CMIP5/output1/NCAR/CCSM4/historical/3hr/atmos/3hr/r6i1p1/files/tas_20120514/tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100.nc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netcdf tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100 {\r\n",
      "dimensions:\r\n",
      "\ttime = UNLIMITED ; // (2920 currently)\r\n",
      "\tlat = 192 ;\r\n",
      "\tlon = 288 ;\r\n",
      "\tbnds = 2 ;\r\n",
      "variables:\r\n",
      "\tdouble time(time) ;\r\n",
      "\t\ttime:units = \"days since 1850-01-01 00:00:00\" ;\r\n",
      "\t\ttime:calendar = \"noleap\" ;\r\n",
      "\t\ttime:axis = \"T\" ;\r\n",
      "\t\ttime:long_name = \"time\" ;\r\n",
      "\t\ttime:standard_name = \"time\" ;\r\n",
      "\tdouble lat(lat) ;\r\n",
      "\t\tlat:bounds = \"lat_bnds\" ;\r\n",
      "\t\tlat:units = \"degrees_north\" ;\r\n",
      "\t\tlat:axis = \"Y\" ;\r\n",
      "\t\tlat:long_name = \"latitude\" ;\r\n",
      "\t\tlat:standard_name = \"latitude\" ;\r\n",
      "\tdouble lat_bnds(lat, bnds) ;\r\n",
      "\tdouble lon(lon) ;\r\n",
      "\t\tlon:bounds = \"lon_bnds\" ;\r\n",
      "\t\tlon:units = \"degrees_east\" ;\r\n",
      "\t\tlon:axis = \"X\" ;\r\n",
      "\t\tlon:long_name = \"longitude\" ;\r\n",
      "\t\tlon:standard_name = \"longitude\" ;\r\n",
      "\tdouble lon_bnds(lon, bnds) ;\r\n",
      "\tdouble height ;\r\n",
      "\t\theight:units = \"m\" ;\r\n",
      "\t\theight:axis = \"Z\" ;\r\n",
      "\t\theight:positive = \"up\" ;\r\n",
      "\t\theight:long_name = \"height\" ;\r\n",
      "\t\theight:standard_name = \"height\" ;\r\n",
      "\tfloat tas(time, lat, lon) ;\r\n",
      "\t\ttas:standard_name = \"air_temperature\" ;\r\n",
      "\t\ttas:long_name = \"Air Temperature\" ;\r\n",
      "\t\ttas:comment = \"TREFHT no change, CMIP5_table_comment: This is sampled synoptically.\" ;\r\n",
      "\t\ttas:units = \"K\" ;\r\n",
      "\t\ttas:original_name = \"TREFHT\" ;\r\n",
      "\t\ttas:cell_methods = \"time: point (interval: 3 hours)\" ;\r\n",
      "\t\ttas:cell_measures = \"area: areacella\" ;\r\n",
      "\t\ttas:history = \"2012-05-09T20:49:36Z altered by CMOR: Treated scalar dimension: \\'height\\'. 2012-05-09T20:49:36Z altered by CMOR: replaced missing value flag (-1e+32) with standard missing value (1e+20).\" ;\r\n",
      "\t\ttas:coordinates = \"height\" ;\r\n",
      "\t\ttas:missing_value = 1.e+20f ;\r\n",
      "\t\ttas:_FillValue = 1.e+20f ;\r\n",
      "\t\ttas:associated_files = \"baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation gridspecFile: gridspec_atmos_fx_CCSM4_historical_r0i0p0.nc areacella: areacella_fx_CCSM4_historical_r0i0p0.nc\" ;\r\n",
      "\r\n",
      "// global attributes:\r\n",
      "\t\t:institution = \"NCAR (National Center for Atmospheric Research) Boulder, CO, USA\" ;\r\n",
      "\t\t:institute_id = \"NCAR\" ;\r\n",
      "\t\t:experiment_id = \"historical\" ;\r\n",
      "\t\t:source = \"CCSM4\" ;\r\n",
      "\t\t:model_id = \"CCSM4\" ;\r\n",
      "\t\t:forcing = \"Sl GHG Vl SS Ds SD BC MD OC Oz AA LU\" ;\r\n",
      "\t\t:parent_experiment_id = \"piControl\" ;\r\n",
      "\t\t:parent_experiment_rip = \"r1i1p1\" ;\r\n",
      "\t\t:branch_time = 9. ;\r\n",
      "\t\t:contact = \"cesm_data@ucar.edu\" ;\r\n",
      "\t\t:comment = \"CESM home page: http://www.cesm.ucar.edu\" ;\r\n",
      "\t\t:references = \"Gent P. R., et.al. 2011: The Community Climate System Model version 4. J. Climate, doi: 10.1175/2011JCLI4083.1\" ;\r\n",
      "\t\t:initialization_method = 1 ;\r\n",
      "\t\t:physics_version = 1 ;\r\n",
      "\t\t:tracking_id = \"26a93b87-fd66-467e-914b-ae8d686ccb8b\" ;\r\n",
      "\t\t:acknowledgements = \"The CESM project is supported by the National Science Foundation and the Office of Science (BER) of the U.S. Department of Energy. NCAR is sponsored by the National Science Foundation. Computing resources were provided by the Climate Simulation Laboratory at the NCAR Computational and Information Systems Laboratory (CISL), sponsored by the National Science Foundation and other agencies.\" ;\r\n",
      "\t\t:cesm_casename = \"b40.20th.track1.1deg.012\" ;\r\n",
      "\t\t:cesm_repotag = \"ccsm4_0_beta53\" ;\r\n",
      "\t\t:cesm_compset = \"B20TRCN\" ;\r\n",
      "\t\t:resolution = \"f09_g16 (0.9x1.25_gx1v6)\" ;\r\n",
      "\t\t:forcing_note = \"Additional information on the external forcings used in this experiment can be found at http://www.cesm.ucar.edu/CMIP5/forcing_information\" ;\r\n",
      "\t\t:processed_by = \"strandwg on mirage1 at 20120509  -144936.028\" ;\r\n",
      "\t\t:processing_code_information = \"Last Changed Rev: 757 Last Changed Date: 2012-05-09 13:01:12 -0600 (Wed, 09 May 2012) Repository UUID: d2181dbe-5796-6825-dc7f-cbd98591f93d\" ;\r\n",
      "\t\t:product = \"output\" ;\r\n",
      "\t\t:experiment = \"historical\" ;\r\n",
      "\t\t:frequency = \"3hr\" ;\r\n",
      "\t\t:creation_date = \"2012-05-09T20:54:32Z\" ;\r\n",
      "\t\t:history = \"2012-05-09T20:51:00Z CMOR rewrote data to comply with CF standards and CMIP5 requirements.\" ;\r\n",
      "\t\t:Conventions = \"CF-1.4\" ;\r\n",
      "\t\t:project_id = \"CMIP5\" ;\r\n",
      "\t\t:table_id = \"Table 3hr (12 January 2012) 7eb57b250935fac136ab277443a9144f\" ;\r\n",
      "\t\t:title = \"CCSM4 model output prepared for CMIP5 historical\" ;\r\n",
      "\t\t:parent_experiment = \"pre-industrial control\" ;\r\n",
      "\t\t:modeling_realm = \"atmos\" ;\r\n",
      "\t\t:realization = 6 ;\r\n",
      "\t\t:cmor_version = \"2.8.1\" ;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!ncdump -h /glade/p/CMIP/CMIP5/output1/NCAR/CCSM4/historical/3hr/atmos/3hr/r6i1p1/files/tas_20120514/tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616M\t/glade/p/CMIP/CMIP5/output1/NCAR/CCSM4/historical/3hr/atmos/3hr/r6i1p1/files/tas_20120514/tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100.nc\r\n"
     ]
    }
   ],
   "source": [
    "!du -lh /glade/p/CMIP/CMIP5/output1/NCAR/CCSM4/historical/3hr/atmos/3hr/r6i1p1/files/tas_20120514/tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/glade/p/CMIP/CMIP5/output1/NCAR/CCSM4/historical/3hr/atmos/3hr/r6i1p1/files/tas_20120514/tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Initialize ```dataset``` class available in ```read module```\n",
    "\n",
    "To initialize this class, we need to pass as an argument of a tuple containing ```(filepath, variable)```. In this case we are interested in ```tas variable```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = read.dataset((filepath, 'tas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'lat', u'lon')\n",
      "/glade/p/CMIP/CMIP5/output1/NCAR/CCSM4/historical/3hr/atmos/3hr/r6i1p1/files/tas_20120514/tas_3hr_CCSM4_historical_r6i1p1_200501010000-200512312100.nc\n",
      "tas\n",
      "365\n",
      "[(-90.0, 0.0), (-90.0, 1.25), (-90.0, 2.5), (-90.0, 3.75), (-90.0, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "print(dset.dims)\n",
    "print(dset.filepath)\n",
    "print(dset.variable_name)\n",
    "print(dset.partitions)\n",
    "print(dset.other_dims_values_tuple[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Use spark to broadcast the following dataset attributes to all the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_dims_values_tuple = sc.broadcast(dset.other_dims_values_tuple) \n",
    "variable_name = sc.broadcast(dset.variable_name)\n",
    "dims = sc.broadcast(dset.dims)\n",
    "ndims = sc.broadcast(dset.ndims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create an RDD using ```read.create_rdd()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tas_rdd = read.create_rdd(sc, (filepath, 'tas'), mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2920"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tas_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 252.29956055,  252.49697876,  252.51231384, ...,  252.30375671,\n",
       "           252.2875061 ,  252.70848083],\n",
       "         [ 254.22868347,  253.75341797,  253.87719727, ...,  254.39428711,\n",
       "           254.04812622,  253.65689087],\n",
       "         [ 255.30435181,  255.51652527,  255.38240051, ...,  255.73982239,\n",
       "           255.53344727,  255.60620117],\n",
       "         ..., \n",
       "         [ 242.31686401,  242.32778931,  242.32933044, ...,  242.25997925,\n",
       "           242.2774353 ,  242.29290771],\n",
       "         [ 242.40519714,  242.40791321,  242.41052246, ...,  242.39332581,\n",
       "           242.40063477,  242.40325928],\n",
       "         [ 243.58824158,  243.58340454,  243.57896423, ...,  243.60612488,\n",
       "           243.59954834,  243.59364319]], dtype=float32), 0),\n",
       " (array([[ 252.1300354 ,  252.33421326,  252.34989929, ...,  252.13648987,\n",
       "           252.11824036,  252.54826355],\n",
       "         [ 254.00390625,  253.53767395,  253.66960144, ...,  254.15505981,\n",
       "           253.8092041 ,  253.41487122],\n",
       "         [ 254.95761108,  255.1879425 ,  255.07904053, ...,  255.29229736,\n",
       "           255.12776184,  255.22019958],\n",
       "         ..., \n",
       "         [ 242.3522644 ,  242.40306091,  242.44438171, ...,  242.19378662,\n",
       "           242.24511719,  242.29232788],\n",
       "         [ 242.3762207 ,  242.38302612,  242.38989258, ...,  242.34638977,\n",
       "           242.36167908,  242.36978149],\n",
       "         [ 243.35525513,  243.35069275,  243.34649658, ...,  243.37216187,\n",
       "           243.36595154,  243.36036682]], dtype=float32), 1),\n",
       " (array([[ 252.06655884,  252.27651978,  252.29927063, ...,  252.07402039,\n",
       "           252.05484009,  252.49511719],\n",
       "         [ 253.98602295,  253.51002502,  253.64990234, ...,  254.11132812,\n",
       "           253.78323364,  253.38105774],\n",
       "         [ 254.95227051,  255.27322388,  255.16915894, ...,  255.35211182,\n",
       "           255.15501404,  255.29963684],\n",
       "         ..., \n",
       "         [ 242.96403503,  243.04924011,  243.12261963, ...,  242.71261597,\n",
       "           242.79588318,  242.87156677],\n",
       "         [ 242.52996826,  242.53330994,  242.53683472, ...,  242.50021362,\n",
       "           242.51942444,  242.52636719],\n",
       "         [ 243.17915344,  243.17582703,  243.17280579, ...,  243.19143677,\n",
       "           243.18690491,  243.18283081]], dtype=float32), 2),\n",
       " (array([[ 252.07104492,  252.28721619,  252.31240845, ...,  252.07917786,\n",
       "           252.05929565,  252.51626587],\n",
       "         [ 254.15396118,  253.66638184,  253.81361389, ...,  254.2666626 ,\n",
       "           253.93405151,  253.52474976],\n",
       "         [ 255.44013977,  255.71736145,  255.62550354, ...,  255.83700562,\n",
       "           255.66171265,  255.78208923],\n",
       "         ..., \n",
       "         [ 244.04873657,  244.1191864 ,  244.17851257, ...,  243.82414246,\n",
       "           243.90306091,  243.97123718],\n",
       "         [ 242.89906311,  242.90884399,  242.91908264, ...,  242.84408569,\n",
       "           242.87269592,  242.88806152],\n",
       "         [ 243.23457336,  243.23313904,  243.23182678, ...,  243.23999023,\n",
       "           243.23799133,  243.23620605]], dtype=float32), 3),\n",
       " (array([[ 252.18556213,  252.4122467 ,  252.43078613, ...,  252.19343567,\n",
       "           252.17376709,  252.63987732],\n",
       "         [ 254.53742981,  254.05484009,  254.20874023, ...,  254.64906311,\n",
       "           254.31124878,  253.90481567],\n",
       "         [ 256.04418945,  256.28839111,  256.20141602, ...,  256.38824463,\n",
       "           256.23779297,  256.35797119],\n",
       "         ..., \n",
       "         [ 244.85041809,  244.88867188,  244.91990662, ...,  244.74307251,\n",
       "           244.78244019,  244.81266785],\n",
       "         [ 243.43415833,  243.44104004,  243.44787598, ...,  243.38026428,\n",
       "           243.41163635,  243.42570496],\n",
       "         [ 243.63981628,  243.63827515,  243.63685608, ...,  243.64559937,\n",
       "           243.64346313,  243.64155579]], dtype=float32), 4),\n",
       " (array([[ 252.26622009,  252.49554443,  252.51319885, ...,  252.27435303,\n",
       "           252.25460815,  252.72297668],\n",
       "         [ 254.65777588,  254.1711731 ,  254.31639099, ...,  254.77207947,\n",
       "           254.43536377,  254.03083801],\n",
       "         [ 255.93943787,  256.22540283,  256.11401367, ...,  256.37869263,\n",
       "           256.18338013,  256.30923462],\n",
       "         ..., \n",
       "         [ 245.75389099,  245.78530884,  245.81141663, ...,  245.69668579,\n",
       "           245.71620178,  245.72944641],\n",
       "         [ 244.14529419,  244.14634705,  244.14685059, ...,  244.10993958,\n",
       "           244.13371277,  244.14219666],\n",
       "         [ 243.89561462,  243.89561462,  243.89561462, ...,  243.89572144,\n",
       "           243.89567566,  243.8956604 ]], dtype=float32), 5),\n",
       " (array([[ 252.46824646,  252.69876099,  252.71588135, ...,  252.4755249 ,\n",
       "           252.45654297,  252.92713928],\n",
       "         [ 254.61341858,  254.11465454,  254.25378418, ...,  254.75701904,\n",
       "           254.40531921,  253.9912262 ],\n",
       "         [ 255.61532593,  255.91764832,  255.78912354, ...,  256.10861206,\n",
       "           255.88017273,  256.009552  ],\n",
       "         ..., \n",
       "         [ 246.47283936,  246.47090149,  246.46347046, ...,  246.49021912,\n",
       "           246.48724365,  246.47770691],\n",
       "         [ 244.60725403,  244.6022644 ,  244.59602356, ...,  244.59289551,\n",
       "           244.60765076,  244.60987854],\n",
       "         [ 244.04454041,  244.04588318,  244.04708862, ...,  244.03971863,\n",
       "           244.04148865,  244.04310608]], dtype=float32), 6),\n",
       " (array([[ 252.59346008,  252.8203125 ,  252.84411621, ...,  252.59266663,\n",
       "           252.57362366,  253.05180359],\n",
       "         [ 254.32777405,  253.81692505,  253.9552002 , ...,  254.47677612,\n",
       "           254.11695862,  253.69927979],\n",
       "         [ 255.06062317,  255.31602478,  255.18400574, ...,  255.51701355,\n",
       "           255.28910828,  255.38722229],\n",
       "         ..., \n",
       "         [ 246.74369812,  246.67608643,  246.60391235, ...,  246.92720032,\n",
       "           246.87095642,  246.80923462],\n",
       "         [ 244.68927002,  244.67538452,  244.65901184, ...,  244.70324707,\n",
       "           244.70759583,  244.70069885],\n",
       "         [ 244.01124573,  244.01106262,  244.01086426, ...,  244.01197815,\n",
       "           244.01171875,  244.01148987]], dtype=float32), 7),\n",
       " (array([[ 252.56170654,  252.79495239,  252.81524658, ...,  252.55976868,\n",
       "           252.54095459,  253.02227783],\n",
       "         [ 253.96316528,  253.45010376,  253.58703613, ...,  254.11875916,\n",
       "           253.75498962,  253.33811951],\n",
       "         [ 254.50013733,  254.74928284,  254.61891174, ...,  254.91732788,\n",
       "           254.70379639,  254.78457642],\n",
       "         ..., \n",
       "         [ 246.5209198 ,  246.42579651,  246.32798767, ...,  246.77967834,\n",
       "           246.69825745,  246.61445618],\n",
       "         [ 244.43183899,  244.4342041 ,  244.43554688, ...,  244.41448975,\n",
       "           244.42095947,  244.42723083],\n",
       "         [ 243.76911926,  243.76800537,  243.76695251, ...,  243.77323914,\n",
       "           243.77172852,  243.77038574]], dtype=float32), 8),\n",
       " (array([[ 252.20622253,  252.44624329,  252.46479797, ...,  252.20657349,\n",
       "           252.18527222,  252.67703247],\n",
       "         [ 253.29289246,  252.78025818,  252.92236328, ...,  253.42953491,\n",
       "           253.07443237,  252.65531921],\n",
       "         [ 253.88299561,  254.13305664,  254.02180481, ...,  254.21292114,\n",
       "           254.052948  ,  254.12675476],\n",
       "         ..., \n",
       "         [ 245.54354858,  245.41671753,  245.28845215, ...,  245.90463257,\n",
       "           245.7875061 ,  245.66822815],\n",
       "         [ 244.09434509,  244.08569336,  244.07328796, ...,  244.10997009,\n",
       "           244.10678101,  244.10139465],\n",
       "         [ 243.38772583,  243.38809204,  243.38838196, ...,  243.38642883,\n",
       "           243.38691711,  243.38737488]], dtype=float32), 9)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tas_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/51l0O4r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create a DataFrame using ```read.dataframe()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /glade/p/work/abanihi/pyspark4climate/read.py\n",
    "\"\"\"\n",
    "This module ingests netCDF file formats into Spark as:\n",
    "    - a resilient distributed dataset(RDD)\n",
    "    - a distributed dataframe\n",
    "\n",
    "Attributes:\n",
    "    PARTITIONS (int): default number of partitions to be used by Spark.\n",
    "\n",
    "TODO:\n",
    "    * Support multiple files reading\n",
    "    * Convert time_indices from numbers to dates\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from netCDF4 import Dataset\n",
    "from netCDF4 import MFDataset\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "global PARTITIONS\n",
    "\n",
    "\n",
    "class dataset(object):\n",
    "    \"\"\"Defines and initializes netCDF file attributes needed by Spark.\n",
    "    Attributes:\n",
    "        filepath                 (str)   :  path for the file to be read\n",
    "        variable_name            (str)   :  variable name\n",
    "        dims                     (tuple) :  dimensions (excluding time dimension) of the variable of interest\n",
    "        ndims                    (int)   :  size of dims tuple\n",
    "        partitions               (int)   :  number of partitions to be used by spark\n",
    "        other_dims_values_tuple  (list)  :  list of tuples containing cartesian product of all dims values\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> dset = dataset(('ta_Amon_CCSM4_historical_r1i1p1_185001-189912.nc', 'ta'))\n",
    "        >>> print(dset.partitions)\n",
    "        75\n",
    "        >>> print(dset.variable_name)\n",
    "        ta\n",
    "        >>> print(dset.ndims)\n",
    "        3\n",
    "        >>> print(dset.dims)\n",
    "        (u'plev', u'lat', u'lon')\n",
    "        >>> print(dset.other_dims_values_tuple[:2])\n",
    "        [(100000.0, -90.0, 0.0), (100000.0, -90.0, 1.25)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath_variable_tuple=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            filepath_variable_tuple (tuple): tuple containing (filepath, 'variable')\n",
    "        \"\"\"\n",
    "\n",
    "        if filepath_variable_tuple is not None:\n",
    "            self.filepath = filepath_variable_tuple[0]\n",
    "            self.variable_name = filepath_variable_tuple[1]\n",
    "            self.dims = None\n",
    "            self.ndims = None\n",
    "            self.partitions = None\n",
    "            self.other_dims_values_tuple = self.generate_cartesian_product()\n",
    "\n",
    "    def generate_cartesian_product(self):\n",
    "        f = Dataset(self.filepath, 'r')\n",
    "        dset = f.variables[self.variable_name]\n",
    "        self.partitions = dset.shape[0] / 8\n",
    "        global PARTITIONS\n",
    "        PARTITIONS = self.partitions\n",
    "        self.dims = dset.dimensions[1:]\n",
    "        self.ndims = len(self.dims)\n",
    "        values = [f.variables[dim][:].tolist() for dim in self.dims]\n",
    "        f.close()\n",
    "        return [element for element in itertools.product(*values)]\n",
    "\n",
    "\n",
    "def create_rdd(sc, file_list_or_txt_file, mode='multi', partitions=None):\n",
    "    \"\"\"Create an RDD from a file_list or tuple of (filepath, variable) and Returns the RDD.\n",
    "\n",
    "    Args:\n",
    "        sc                     (object)       : sparkContext Object\n",
    "        file_list_or_txt_file  (list or tuple : list of tuples or a tuple of the format (filepath, variable)\n",
    "        mode                   (str)          : If reading multiple files (multi), otherwise(single)\n",
    "        partitions             (int)          : number of partitions\n",
    "\n",
    "    \"\"\"\n",
    "    if mode == 'multi':\n",
    "        return read_nc_multi(sc, file_list_or_txt_file, partitions=partitions)\n",
    "\n",
    "    elif mode == 'single':\n",
    "        return read_nc_single_chunked(sc, file_list_or_txt_file, partitions=partitions)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"You specified a mode that is not implemented.\")\n",
    "\n",
    "\n",
    "def read_nc_single_chunked(sc, filepath_variable_tuple, partitions=None):\n",
    "\n",
    "    \"\"\" Generates an RDD using the information passed by create_rdd function.\n",
    "     Args:\n",
    "        sc                     (object)       : sparkContext Object\n",
    "        file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
    "        partitions             (int)          : number of partitions\n",
    "\n",
    "    Returns:\n",
    "        rdd_                   (rdd)          : Spark's resilient distributed dataset\n",
    "    \"\"\"\n",
    "    assert isinstance(filepath_variable_tuple, tuple), \"For single file mode, you must must input a tuple\"\n",
    "    dset = dataset(filepath_variable_tuple)\n",
    "    filepath_ = dset.filepath\n",
    "    variable_ = dset.variable_name\n",
    "    rows = Dataset(filepath_, 'r').variables[variable_].shape[0]\n",
    "\n",
    "    if not partitions:\n",
    "        partitions = PARTITIONS\n",
    "\n",
    "    if partitions > rows:\n",
    "        partitions = rows\n",
    "\n",
    "    step = rows / partitions\n",
    "\n",
    "    rdd_ = sc.range(0, rows, step)\\\n",
    "             .sortBy(lambda x: x, numPartitions=partitions)\\\n",
    "             .flatMap(lambda x: readonep(filepath_, variable_, x, step)).zipWithIndex()\\\n",
    "\n",
    "    return rdd_\n",
    "\n",
    "\n",
    "def readonep(filepath_, variable_, start_idx, chunk_size):\n",
    "    \"\"\"Read a slice from one file.\n",
    "\n",
    "    Args:\n",
    "        filepath_    (str): string containing the file path\n",
    "        variable_    (str): variable name\n",
    "        start_idx    (int): starting index\n",
    "        chunk_size   (int): the chunk size to be read at a time.\n",
    "\n",
    "    Returns:\n",
    "        list:   list of the chunk read\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = Dataset(filepath_, 'r')\n",
    "        dset = f.variables[variable_]\n",
    "\n",
    "        # get the number of dimensions of the variable\n",
    "        dims = dset.dimensions\n",
    "        ndims = len(dims)\n",
    "        end_idx = start_idx + chunk_size\n",
    "\n",
    "        if end_idx < dset.shape[0]:\n",
    "            chunk = dset[tuple([slice(start_idx, end_idx)] + [slice(None)]*(ndims-1))]\n",
    "\n",
    "        else:\n",
    "            chunk = dset[tuple([slice(start_idx, dset.shape[0])] + [slice(None)]*(ndims-1))]\n",
    "\n",
    "        return list(chunk[:])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"IOError: {} {}\".format(e, filepath_))\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def dataframe(sc, file_list_or_txt_file, mode='multi', partitions=None):\n",
    "    \"\"\"Creates a distributed dataframe from a netCDF file.\n",
    "\n",
    "    Args:\n",
    "        sc                     (object)       : sparkContext Object\n",
    "        file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
    "        partitions             (int)          : number of partitions\n",
    "        mode                   (str)          : (multi) if reading multiple files, otherwise(single)\n",
    "    Returns:\n",
    "        df                  (dataframe)          : Spark's distributed data frame\n",
    "    \"\"\"\n",
    "\n",
    "    df = create_rdd(sc, file_list_or_txt_file, mode=mode, partitions=partitions)\\\n",
    "        .map(flatten_data)\\\n",
    "        .flatMap(lambda x: x).repartition(partitions*10)\\\n",
    "        .map(row_transform)\\\n",
    "        .toDF()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def rdd_to_df(rdd):\n",
    "    \"\"\"Function that converts an RDD into a Spark data frame.\n",
    "    Arguments:\n",
    "        - rdd: (rdd)\n",
    "\n",
    "    Returns:\n",
    "        - df: Spark dataframe\n",
    "    \"\"\"\n",
    "    df = rdd.map(flatten_data)\\\n",
    "            .flatMap(lambda x: x).repartition(PARTITIONS*10)\\\n",
    "            .map(row_transform)\\\n",
    "            .toDF()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_data(line):\n",
    "    \"\"\"Flattens numpy array and return a tuple of each value\n",
    "    and its corresponding lat_lon coordinates together with other dimensions.\n",
    "\n",
    "    Args:\n",
    "        line (tuple) :  an rdd element in the form of a tuple (data, idx) where data is\n",
    "                        a numpy array and idx correspond to time index.\n",
    "\n",
    "    Returns:\n",
    "         results (tuple): a transformed rdd element in the form\n",
    "                           of a tuple (idx, dim1_value, dim_value2, ..., data_value)\n",
    "    \"\"\"\n",
    "    data = line[0].ravel().tolist()\n",
    "    idx = line[1]\n",
    "    results = map(lambda x: (idx, ) + (x[0]) + (x[1], ), zip(other_dims_values_tuple.value, data))\n",
    "    return results\n",
    "\n",
    "\n",
    "def row_transform(line):\n",
    "    \"\"\"Transforms a a tuple (idx, dim1_value, dim_value2, ..., data_value) into a Spark sql\n",
    "       Row object.\n",
    "\n",
    "    Args:\n",
    "        line (tuple): a tuple of the form (idx, dim1_value, dim_value2, ..., data_value)\n",
    "\n",
    "    Returns:\n",
    "        row(*line) : Spark Row object with arbitray number of items depending on the size of\n",
    "                     the tuple in line.\n",
    "\n",
    "    Examples:\n",
    "        >>> print(line)\n",
    "        (0, 100000.0, -90.0, 0.0, 257.8)\n",
    "        >>> row(*line)\n",
    "        Row(time=0, plev=100000.0, lat=-90.0, lon=0.0, ta=257.8)\n",
    "\n",
    "    \"\"\"\n",
    "    dims_ = dims.value\n",
    "    ndims_ = len(dims_)\n",
    "    variable_ = variable_name.value\n",
    "    columns = (\"time\",)+tuple(dims_[:])+(variable_,)\n",
    "    row = Row(*columns)\n",
    "    return row(*line)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 51 in stage 12.0 failed 4 times, most recent failure: Lost task 51.3 in stage 12.0 (TID 1336, 10.148.8.134, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"read.py\", line 220, in flatten_data\n    results = map(lambda x: (idx, ) + (x[0]) + (x[1], ), zip(other_dims_values_tuple.value, data))\nNameError: global name 'other_dims_values_tuple' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"read.py\", line 220, in flatten_data\n    results = map(lambda x: (idx, ) + (x[0]) + (x[1], ), zip(other_dims_values_tuple.value, data))\nNameError: global name 'other_dims_values_tuple' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7fd5a344fe34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtas_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/glade/p/work/abanihi/pyspark4climate/read.pyc\u001b[0m in \u001b[0;36mrdd_to_df\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARTITIONS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \"\"\"\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \"\"\"\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 51 in stage 12.0 failed 4 times, most recent failure: Lost task 51.3 in stage 12.0 (TID 1336, 10.148.8.134, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"read.py\", line 220, in flatten_data\n    results = map(lambda x: (idx, ) + (x[0]) + (x[1], ), zip(other_dims_values_tuple.value, data))\nNameError: global name 'other_dims_values_tuple' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/glade/p/work/abanihi/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"read.py\", line 220, in flatten_data\n    results = map(lambda x: (idx, ) + (x[0]) + (x[1], ), zip(other_dims_values_tuple.value, data))\nNameError: global name 'other_dims_values_tuple' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "tas_df = read.rdd_to_df(tas_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'PARTITIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3306f8a18aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtas_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-86888c4f0386>\u001b[0m in \u001b[0;36mrdd_to_df\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_data\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARTITIONS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_transform\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'PARTITIONS' is not defined"
     ]
    }
   ],
   "source": [
    "tas_df = rdd_to_df(tas_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94170"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tas_df = dataframe(sc, (filepath, 'tas'), mode='single', partitions=dset.partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------+------------------+\n",
      "|time|               lat|   lon|               tas|\n",
      "+----+------------------+------+------------------+\n",
      "|  88|-77.74868774414062| 145.0|252.76138305664062|\n",
      "|  88|-77.74868774414062|146.25|252.78738403320312|\n",
      "|  88|-77.74868774414062| 147.5| 252.7574462890625|\n",
      "|  88|-77.74868774414062|148.75|253.21543884277344|\n",
      "|  88|-77.74868774414062| 150.0| 253.6935577392578|\n",
      "|  88|-77.74868774414062|151.25|254.23153686523438|\n",
      "|  88|-77.74868774414062| 152.5|254.77780151367188|\n",
      "|  88|-77.74868774414062|153.75|  255.639404296875|\n",
      "|  88|-77.74868774414062| 155.0| 256.7210693359375|\n",
      "|  88|-77.74868774414062|156.25| 258.0180358886719|\n",
      "|  88|41.937171936035156|  50.0| 276.7320556640625|\n",
      "|  88|41.937171936035156| 51.25|   277.10302734375|\n",
      "|  88|41.937171936035156|  52.5| 276.8706359863281|\n",
      "|  88|41.937171936035156| 53.75|274.19281005859375|\n",
      "|  88|41.937171936035156|  55.0| 269.5821533203125|\n",
      "|  88|41.937171936035156| 56.25|268.18756103515625|\n",
      "|  88|41.937171936035156|  57.5| 266.5932312011719|\n",
      "|  88|41.937171936035156| 58.75| 266.3335266113281|\n",
      "|  88|41.937171936035156|  60.0| 266.4668273925781|\n",
      "|  88|41.937171936035156| 61.25| 264.8317565917969|\n",
      "+----+------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tas_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|             time|              lat|               lon|              tas|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|        161464320|        161464320|         161464320|        161464320|\n",
      "|   mean|           1459.5|              0.0|           179.375|278.1147441679061|\n",
      "| stddev|842.9313461964497|52.23286586926288|103.92242230892091|22.18978777577845|\n",
      "|    min|                0|            -90.0|               0.0|199.0844268798828|\n",
      "|    max|             2919|             90.0|            358.75|325.5912780761719|\n",
      "+-------+-----------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tas_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://i.imgur.com/AUKVUyV.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tas_df.createGlobalTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT time, lat, lon, tas\n",
    "          FROM temperature\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
