{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PySpark4Climate I/O primer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces some of the functionalities supported by PySpark4Climate ```read module``` and how to use PySpark4Climate ```read module``` in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hide warnings if there are any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import read\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-read-test\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's direct Spark to make **pyspark4climate read module** available to all executors by using ```sc.addPyFiles()``` function option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(\"/glade/p/work/abanihi/pyspark4climate/read.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module read:\n",
      "\n",
      "NAME\n",
      "    read\n",
      "\n",
      "FILE\n",
      "    /glade/p/work/abanihi/pyspark4climate/read.py\n",
      "\n",
      "DESCRIPTION\n",
      "    This module ingests netCDF file formats into Spark as:\n",
      "        - a resilient distributed dataset(RDD)\n",
      "        - a distributed dataframe\n",
      "    \n",
      "    Attributes:\n",
      "        PARTITIONS (int): default number of partitions to be used by Spark.\n",
      "    \n",
      "    TODO:\n",
      "        * Support multiple files reading\n",
      "        * Convert time_indices from numbers to dates\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        dataset\n",
      "    \n",
      "    class dataset(__builtin__.object)\n",
      "     |  Defines and initializes netCDF file attributes needed by Spark.\n",
      "     |  Attributes:\n",
      "     |      filepath                 (str)   :  path for the file to be read\n",
      "     |      variable_name            (str)   :  variable name\n",
      "     |      dims                     (tuple) :  dimensions (excluding time dimension) of the variable of interest\n",
      "     |      ndims                    (int)   :  size of dims tuple\n",
      "     |      partitions               (int)   :  number of partitions to be used by spark\n",
      "     |      other_dims_values_tuple  (list)  :  list of tuples containing cartesian product of all dims values\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples:\n",
      "     |      >>> dset = dataset(('ta_Amon_CCSM4_historical_r1i1p1_185001-189912.nc', 'ta'))\n",
      "     |      >>> print(dset.partitions)\n",
      "     |      75\n",
      "     |      >>> print(dset.variable_name)\n",
      "     |      ta\n",
      "     |      >>> print(dset.ndims)\n",
      "     |      3\n",
      "     |      >>> print(dset.dims)\n",
      "     |      (u'plev', u'lat', u'lon')\n",
      "     |      >>> print(dset.other_dims_values_tuple[:2])\n",
      "     |      [(100000.0, -90.0, 0.0), (100000.0, -90.0, 1.25)]\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, filepath_variable_tuple=None)\n",
      "     |      Args:\n",
      "     |          filepath_variable_tuple (tuple): tuple containing (filepath, 'variable')\n",
      "     |  \n",
      "     |  generate_cartesian_product(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    create_rdd(sc, file_list_or_txt_file, mode='multi', partitions=None)\n",
      "        Create an RDD from a file_list or tuple of (filepath, variable) and Returns the RDD.\n",
      "        \n",
      "        Args:\n",
      "            sc                     (object)       : sparkContext Object\n",
      "            file_list_or_txt_file  (list or tuple : list of tuples or a tuple of the format (filepath, variable)\n",
      "            mode                   (str)          : If reading multiple files (multi), otherwise(single)\n",
      "            partitions             (int)          : number of partitions\n",
      "    \n",
      "    dataframe(sc, file_list_or_txt_file, mode='multi', partitions=None)\n",
      "        Creates a distributed dataframe from a netCDF file.\n",
      "        \n",
      "        Args:\n",
      "            sc                     (object)       : sparkContext Object\n",
      "            file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
      "            partitions             (int)          : number of partitions\n",
      "            mode                   (str)          : (multi) if reading multiple files, otherwise(single)\n",
      "        Returns:\n",
      "            df                  (dataframe)          : Spark's distributed data frame\n",
      "    \n",
      "    flatten_data(line)\n",
      "        Flattens numpy array and return a tuple of each value\n",
      "        and its corresponding lat_lon coordinates together with other dimensions.\n",
      "        \n",
      "        Args:\n",
      "            line (tuple) :  an rdd element in the form of a tuple (data, idx) where data is\n",
      "                            a numpy array and idx correspond to time index.\n",
      "        \n",
      "        Returns:\n",
      "             results (tuple): a transformed rdd element in the form\n",
      "                               of a tuple (idx, dim1_value, dim_value2, ..., data_value)\n",
      "    \n",
      "    rdd_to_df(rdd)\n",
      "        Function that converts an RDD into a Spark data frame.\n",
      "        Arguments:\n",
      "            - rdd: (rdd)\n",
      "        \n",
      "        Returns:\n",
      "            - df: Spark dataframe\n",
      "    \n",
      "    read_nc_single_chunked(sc, filepath_variable_tuple, partitions=None)\n",
      "        Generates an RDD using the information passed by create_rdd function.\n",
      "         Args:\n",
      "            sc                     (object)       : sparkContext Object\n",
      "            file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
      "            partitions             (int)          : number of partitions\n",
      "        \n",
      "        Returns:\n",
      "            rdd_                   (rdd)          : Spark's resilient distributed dataset\n",
      "    \n",
      "    readonep(filepath_, variable_, start_idx, chunk_size)\n",
      "        Read a slice from one file.\n",
      "        \n",
      "        Args:\n",
      "            filepath_    (str): string containing the file path\n",
      "            variable_    (str): variable name\n",
      "            start_idx    (int): starting index\n",
      "            chunk_size   (int): the chunk size to be read at a time.\n",
      "        \n",
      "        Returns:\n",
      "            list:   list of the chunk read\n",
      "    \n",
      "    row_transform(line)\n",
      "        Transforms a a tuple (idx, dim1_value, dim_value2, ..., data_value) into a Spark sql\n",
      "           Row object.\n",
      "        \n",
      "        Args:\n",
      "            line (tuple): a tuple of the form (idx, dim1_value, dim_value2, ..., data_value)\n",
      "        \n",
      "        Returns:\n",
      "            row(*line) : Spark Row object with arbitray number of items depending on the size of\n",
      "                         the tuple in line.\n",
      "        \n",
      "        Examples:\n",
      "            >>> print(line)\n",
      "            (0, 100000.0, -90.0, 0.0, 257.8)\n",
      "            >>> row(*line)\n",
      "            Row(time=0, plev=100000.0, lat=-90.0, lon=0.0, ta=257.8)\n",
      "\n",
      "DATA\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.Kryoserializer.buffer.max.mb=4096\n",
      "spark.app.name=spark-read-test\n",
      "spark.driver.maxResultSize=10g\n",
      "spark.driver.memory=20g\n",
      "spark.executor.memory=15g\n",
      "spark.master=spark://r2i6n14.ib0.cheyenne.ucar.edu:7077\n",
      "spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
      "spark.speculation=True\n",
      "spark.submit.deployMode=client\n"
     ]
    }
   ],
   "source": [
    "# Print some information about Spark's configuration\n",
    "print(SparkConf().toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For this tutorial we will be using the following dataset.\n",
    "- ```/glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/hourly1/PRECC/b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z.nc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netcdf b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z {\r\n",
      "dimensions:\r\n",
      "\tlat = 192 ;\r\n",
      "\tlon = 288 ;\r\n",
      "\tslat = 191 ;\r\n",
      "\tslon = 288 ;\r\n",
      "\ttime = UNLIMITED ; // (753360 currently)\r\n",
      "\tnbnd = 2 ;\r\n",
      "\tchars = 8 ;\r\n",
      "\tlev = 30 ;\r\n",
      "\tilev = 31 ;\r\n",
      "\tcosp_prs = 7 ;\r\n",
      "\tcosp_tau = 7 ;\r\n",
      "\tcosp_scol = 10 ;\r\n",
      "\tcosp_ht = 40 ;\r\n",
      "\tcosp_sr = 15 ;\r\n",
      "\tcosp_sza = 5 ;\r\n",
      "\tcosp_htmisr = 16 ;\r\n",
      "\tcosp_tau_modis = 6 ;\r\n",
      "variables:\r\n",
      "\tdouble lev(lev) ;\r\n",
      "\t\tlev:long_name = \"hybrid level at midpoints (1000*(A+B))\" ;\r\n",
      "\t\tlev:units = \"level\" ;\r\n",
      "\t\tlev:positive = \"down\" ;\r\n",
      "\t\tlev:standard_name = \"atmosphere_hybrid_sigma_pressure_coordinate\" ;\r\n",
      "\t\tlev:formula_terms = \"a: hyam b: hybm p0: P0 ps: PS\" ;\r\n",
      "\tdouble hyam(lev) ;\r\n",
      "\t\thyam:long_name = \"hybrid A coefficient at layer midpoints\" ;\r\n",
      "\tdouble hybm(lev) ;\r\n",
      "\t\thybm:long_name = \"hybrid B coefficient at layer midpoints\" ;\r\n",
      "\tdouble ilev(ilev) ;\r\n",
      "\t\tilev:long_name = \"hybrid level at interfaces (1000*(A+B))\" ;\r\n",
      "\t\tilev:units = \"level\" ;\r\n",
      "\t\tilev:positive = \"down\" ;\r\n",
      "\t\tilev:standard_name = \"atmosphere_hybrid_sigma_pressure_coordinate\" ;\r\n",
      "\t\tilev:formula_terms = \"a: hyai b: hybi p0: P0 ps: PS\" ;\r\n",
      "\tdouble hyai(ilev) ;\r\n",
      "\t\thyai:long_name = \"hybrid A coefficient at layer interfaces\" ;\r\n",
      "\tdouble hybi(ilev) ;\r\n",
      "\t\thybi:long_name = \"hybrid B coefficient at layer interfaces\" ;\r\n",
      "\tdouble cosp_prs(cosp_prs) ;\r\n",
      "\t\tcosp_prs:long_name = \"COSP Mean ISCCP pressure\" ;\r\n",
      "\t\tcosp_prs:units = \"mb\" ;\r\n",
      "\t\tcosp_prs:bounds = \"cosp_prs_bnds\" ;\r\n",
      "\tdouble cosp_prs_bnds(cosp_prs, nbnd) ;\r\n",
      "\tdouble cosp_tau(cosp_tau) ;\r\n",
      "\t\tcosp_tau:long_name = \"COSP Mean ISCCP optical depth\" ;\r\n",
      "\t\tcosp_tau:units = \"unitless\" ;\r\n",
      "\t\tcosp_tau:bounds = \"cosp_tau_bnds\" ;\r\n",
      "\tdouble cosp_tau_bnds(cosp_tau, nbnd) ;\r\n",
      "\tdouble cosp_scol(cosp_scol) ;\r\n",
      "\t\tcosp_scol:long_name = \"COSP subcolumn\" ;\r\n",
      "\t\tcosp_scol:units = \"number\" ;\r\n",
      "\tdouble cosp_ht(cosp_ht) ;\r\n",
      "\t\tcosp_ht:long_name = \"COSP Mean Height for lidar and radar simulator outputs\" ;\r\n",
      "\t\tcosp_ht:units = \"m\" ;\r\n",
      "\t\tcosp_ht:bounds = \"cosp_ht_bnds\" ;\r\n",
      "\tdouble cosp_ht_bnds(cosp_ht, nbnd) ;\r\n",
      "\tdouble cosp_sr(cosp_sr) ;\r\n",
      "\t\tcosp_sr:long_name = \"COSP Mean Scattering Ratio for lidar simulator CFAD output\" ;\r\n",
      "\t\tcosp_sr:units = \"1\" ;\r\n",
      "\t\tcosp_sr:bounds = \"cosp_sr_bnds\" ;\r\n",
      "\tdouble cosp_sr_bnds(cosp_sr, nbnd) ;\r\n",
      "\tdouble cosp_sza(cosp_sza) ;\r\n",
      "\t\tcosp_sza:long_name = \"COSP Parasol SZA\" ;\r\n",
      "\t\tcosp_sza:units = \"degrees\" ;\r\n",
      "\tdouble cosp_htmisr(cosp_htmisr) ;\r\n",
      "\t\tcosp_htmisr:long_name = \"COSP MISR height\" ;\r\n",
      "\t\tcosp_htmisr:units = \"km\" ;\r\n",
      "\t\tcosp_htmisr:bounds = \"cosp_htmisr_bnds\" ;\r\n",
      "\tdouble cosp_htmisr_bnds(cosp_htmisr, nbnd) ;\r\n",
      "\tdouble cosp_tau_modis(cosp_tau_modis) ;\r\n",
      "\t\tcosp_tau_modis:long_name = \"COSP Mean MODIS optical depth\" ;\r\n",
      "\t\tcosp_tau_modis:units = \"unitless\" ;\r\n",
      "\t\tcosp_tau_modis:bounds = \"cosp_tau_modis_bnds\" ;\r\n",
      "\tdouble cosp_tau_modis_bnds(cosp_tau_modis, nbnd) ;\r\n",
      "\tdouble P0 ;\r\n",
      "\t\tP0:long_name = \"reference pressure\" ;\r\n",
      "\t\tP0:units = \"Pa\" ;\r\n",
      "\tdouble time(time) ;\r\n",
      "\t\ttime:long_name = \"time\" ;\r\n",
      "\t\ttime:units = \"days since 1920-01-01 00:00:00\" ;\r\n",
      "\t\ttime:calendar = \"noleap\" ;\r\n",
      "\t\ttime:bounds = \"time_bnds\" ;\r\n",
      "\tint date(time) ;\r\n",
      "\t\tdate:long_name = \"current date (YYYYMMDD)\" ;\r\n",
      "\tint datesec(time) ;\r\n",
      "\t\tdatesec:long_name = \"current seconds of current date\" ;\r\n",
      "\tdouble lat(lat) ;\r\n",
      "\t\tlat:long_name = \"latitude\" ;\r\n",
      "\t\tlat:units = \"degrees_north\" ;\r\n",
      "\tdouble lon(lon) ;\r\n",
      "\t\tlon:long_name = \"longitude\" ;\r\n",
      "\t\tlon:units = \"degrees_east\" ;\r\n",
      "\tdouble slat(slat) ;\r\n",
      "\t\tslat:long_name = \"staggered latitude\" ;\r\n",
      "\t\tslat:units = \"degrees_north\" ;\r\n",
      "\tdouble slon(slon) ;\r\n",
      "\t\tslon:long_name = \"staggered longitude\" ;\r\n",
      "\t\tslon:units = \"degrees_east\" ;\r\n",
      "\tdouble w_stag(slat) ;\r\n",
      "\t\tw_stag:long_name = \"staggered latitude weights\" ;\r\n",
      "\tdouble time_bnds(time, nbnd) ;\r\n",
      "\t\ttime_bnds:long_name = \"time interval endpoints\" ;\r\n",
      "\tchar date_written(time, chars) ;\r\n",
      "\tchar time_written(time, chars) ;\r\n",
      "\tint ntrm ;\r\n",
      "\t\tntrm:long_name = \"spectral truncation parameter M\" ;\r\n",
      "\tint ntrn ;\r\n",
      "\t\tntrn:long_name = \"spectral truncation parameter N\" ;\r\n",
      "\tint ntrk ;\r\n",
      "\t\tntrk:long_name = \"spectral truncation parameter K\" ;\r\n",
      "\tint ndbase ;\r\n",
      "\t\tndbase:long_name = \"base day\" ;\r\n",
      "\tint nsbase ;\r\n",
      "\t\tnsbase:long_name = \"seconds of base day\" ;\r\n",
      "\tint nbdate ;\r\n",
      "\t\tnbdate:long_name = \"base date (YYYYMMDD)\" ;\r\n",
      "\tint nbsec ;\r\n",
      "\t\tnbsec:long_name = \"seconds of base date\" ;\r\n",
      "\tint mdt ;\r\n",
      "\t\tmdt:long_name = \"timestep\" ;\r\n",
      "\t\tmdt:units = \"s\" ;\r\n",
      "\tint nlon(lat) ;\r\n",
      "\t\tnlon:long_name = \"number of longitudes\" ;\r\n",
      "\tint wnummax(lat) ;\r\n",
      "\t\twnummax:long_name = \"cutoff Fourier wavenumber\" ;\r\n",
      "\tdouble gw(lat) ;\r\n",
      "\t\tgw:long_name = \"gauss weights\" ;\r\n",
      "\tint ndcur(time) ;\r\n",
      "\t\tndcur:long_name = \"current day (from base day)\" ;\r\n",
      "\tint nscur(time) ;\r\n",
      "\t\tnscur:long_name = \"current seconds of current day\" ;\r\n",
      "\tdouble co2vmr(time) ;\r\n",
      "\t\tco2vmr:long_name = \"co2 volume mixing ratio\" ;\r\n",
      "\tdouble ch4vmr(time) ;\r\n",
      "\t\tch4vmr:long_name = \"ch4 volume mixing ratio\" ;\r\n",
      "\tdouble n2ovmr(time) ;\r\n",
      "\t\tn2ovmr:long_name = \"n2o volume mixing ratio\" ;\r\n",
      "\tdouble f11vmr(time) ;\r\n",
      "\t\tf11vmr:long_name = \"f11 volume mixing ratio\" ;\r\n",
      "\tdouble f12vmr(time) ;\r\n",
      "\t\tf12vmr:long_name = \"f12 volume mixing ratio\" ;\r\n",
      "\tdouble sol_tsi(time) ;\r\n",
      "\t\tsol_tsi:long_name = \"total solar irradiance\" ;\r\n",
      "\t\tsol_tsi:units = \"W/m2\" ;\r\n",
      "\tint nsteph(time) ;\r\n",
      "\t\tnsteph:long_name = \"current timestep\" ;\r\n",
      "\tfloat PRECC(time, lat, lon) ;\r\n",
      "\t\tPRECC:units = \"m/s\" ;\r\n",
      "\t\tPRECC:long_name = \"Convective precipitation rate (liq + ice)\" ;\r\n",
      "\t\tPRECC:cell_methods = \"time: mean\" ;\r\n",
      "\r\n",
      "// global attributes:\r\n",
      "\t\t:Conventions = \"CF-1.0\" ;\r\n",
      "\t\t:source = \"CAM\" ;\r\n",
      "\t\t:case = \"b.e11.B20TRC5CNBDRD.f09_g16.034\" ;\r\n",
      "\t\t:title = \"UNSET\" ;\r\n",
      "\t\t:logname = \"adrianne\" ;\r\n",
      "\t\t:host = \"ys0858\" ;\r\n",
      "\t\t:Version = \"$Name$\" ;\r\n",
      "\t\t:revision_Id = \"$Id$\" ;\r\n",
      "\t\t:initial_file = \"b.e11.B20TRC5CNBDRD.f09_g16.001.cam.i.1920-01-01-00000.nc\" ;\r\n",
      "\t\t:topography_file = \"/glade/p/cesmdata/cseg/inputdata/atm/cam/topo/USGS-gtopo30_0.9x1.25_remap_c051027.nc\" ;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!ncdump -h /glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/hourly1/PRECC/b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156G\t/glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/hourly1/PRECC/b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z.nc\r\n"
     ]
    }
   ],
   "source": [
    "!du -lh /glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/hourly1/PRECC/b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/hourly1/PRECC/b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Initialize ```dataset``` class available in ```read module```\n",
    "\n",
    "To initialize this class, we need to pass as an argument of a tuple containing ```(filepath, variable)```. In this case we are interested in ```PRECC variable```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = read.dataset((filepath, 'PRECC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'lat', u'lon')\n",
      "/glade/p/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/hourly1/PRECC/b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h3.PRECC.192001010000Z-200512312300Z.nc\n",
      "PRECC\n",
      "94170\n",
      "[(-90.0, 0.0), (-90.0, 1.25), (-90.0, 2.5), (-90.0, 3.75), (-90.0, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "print(dset.dims)\n",
    "print(dset.filepath)\n",
    "print(dset.variable_name)\n",
    "print(dset.partitions)\n",
    "print(dset.other_dims_values_tuple[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Use spark to broadcast the following dataset attributes to all the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_dims_values_tuple = sc.broadcast(dset.other_dims_values_tuple) \n",
    "variable_name = sc.broadcast(dset.variable_name)\n",
    "dims = sc.broadcast(dset.dims)\n",
    "ndims = sc.broadcast(dset.ndims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create an RDD using ```read.create_rdd()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precc_rdd = read.create_rdd(sc, (filepath, 'PRECC'), mode='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753360"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precc_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 0),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 1),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 2),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 3),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 4),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 5),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 6),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 7),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 8),\n",
       " (array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), 9)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precc_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/EWd3Zdq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create a DataFrame using ```read.dataframe()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /glade/p/work/abanihi/pyspark4climate/read.py\n",
    "\"\"\"\n",
    "This module ingests netCDF file formats into Spark as:\n",
    "    - a resilient distributed dataset(RDD)\n",
    "    - a distributed dataframe\n",
    "\n",
    "Attributes:\n",
    "    PARTITIONS (int): default number of partitions to be used by Spark.\n",
    "\n",
    "TODO:\n",
    "    * Support multiple files reading\n",
    "    * Convert time_indices from numbers to dates\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from netCDF4 import Dataset\n",
    "from netCDF4 import MFDataset\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "global PARTITIONS\n",
    "\n",
    "\n",
    "class dataset(object):\n",
    "    \"\"\"Defines and initializes netCDF file attributes needed by Spark.\n",
    "    Attributes:\n",
    "        filepath                 (str)   :  path for the file to be read\n",
    "        variable_name            (str)   :  variable name\n",
    "        dims                     (tuple) :  dimensions (excluding time dimension) of the variable of interest\n",
    "        ndims                    (int)   :  size of dims tuple\n",
    "        partitions               (int)   :  number of partitions to be used by spark\n",
    "        other_dims_values_tuple  (list)  :  list of tuples containing cartesian product of all dims values\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> dset = dataset(('ta_Amon_CCSM4_historical_r1i1p1_185001-189912.nc', 'ta'))\n",
    "        >>> print(dset.partitions)\n",
    "        75\n",
    "        >>> print(dset.variable_name)\n",
    "        ta\n",
    "        >>> print(dset.ndims)\n",
    "        3\n",
    "        >>> print(dset.dims)\n",
    "        (u'plev', u'lat', u'lon')\n",
    "        >>> print(dset.other_dims_values_tuple[:2])\n",
    "        [(100000.0, -90.0, 0.0), (100000.0, -90.0, 1.25)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath_variable_tuple=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            filepath_variable_tuple (tuple): tuple containing (filepath, 'variable')\n",
    "        \"\"\"\n",
    "\n",
    "        if filepath_variable_tuple is not None:\n",
    "            self.filepath = filepath_variable_tuple[0]\n",
    "            self.variable_name = filepath_variable_tuple[1]\n",
    "            self.dims = None\n",
    "            self.ndims = None\n",
    "            self.partitions = None\n",
    "            self.other_dims_values_tuple = self.generate_cartesian_product()\n",
    "\n",
    "    def generate_cartesian_product(self):\n",
    "        f = Dataset(self.filepath, 'r')\n",
    "        dset = f.variables[self.variable_name]\n",
    "        self.partitions = dset.shape[0] / 8\n",
    "        global PARTITIONS\n",
    "        PARTITIONS = self.partitions\n",
    "        self.dims = dset.dimensions[1:]\n",
    "        self.ndims = len(self.dims)\n",
    "        values = [f.variables[dim][:].tolist() for dim in self.dims]\n",
    "        f.close()\n",
    "        return [element for element in itertools.product(*values)]\n",
    "\n",
    "\n",
    "def create_rdd(sc, file_list_or_txt_file, mode='multi', partitions=None):\n",
    "    \"\"\"Create an RDD from a file_list or tuple of (filepath, variable) and Returns the RDD.\n",
    "\n",
    "    Args:\n",
    "        sc                     (object)       : sparkContext Object\n",
    "        file_list_or_txt_file  (list or tuple : list of tuples or a tuple of the format (filepath, variable)\n",
    "        mode                   (str)          : If reading multiple files (multi), otherwise(single)\n",
    "        partitions             (int)          : number of partitions\n",
    "\n",
    "    \"\"\"\n",
    "    if mode == 'multi':\n",
    "        return read_nc_multi(sc, file_list_or_txt_file, partitions=partitions)\n",
    "\n",
    "    elif mode == 'single':\n",
    "        return read_nc_single_chunked(sc, file_list_or_txt_file, partitions=partitions)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"You specified a mode that is not implemented.\")\n",
    "\n",
    "\n",
    "def read_nc_single_chunked(sc, filepath_variable_tuple, partitions=None):\n",
    "\n",
    "    \"\"\" Generates an RDD using the information passed by create_rdd function.\n",
    "     Args:\n",
    "        sc                     (object)       : sparkContext Object\n",
    "        file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
    "        partitions             (int)          : number of partitions\n",
    "\n",
    "    Returns:\n",
    "        rdd_                   (rdd)          : Spark's resilient distributed dataset\n",
    "    \"\"\"\n",
    "    assert isinstance(filepath_variable_tuple, tuple), \"For single file mode, you must must input a tuple\"\n",
    "    dset = dataset(filepath_variable_tuple)\n",
    "    filepath_ = dset.filepath\n",
    "    variable_ = dset.variable_name\n",
    "    rows = Dataset(filepath_, 'r').variables[variable_].shape[0]\n",
    "\n",
    "    if not partitions:\n",
    "        partitions = PARTITIONS\n",
    "\n",
    "    if partitions > rows:\n",
    "        partitions = rows\n",
    "\n",
    "    step = rows / partitions\n",
    "\n",
    "    rdd_ = sc.range(0, rows, step)\\\n",
    "             .sortBy(lambda x: x, numPartitions=partitions)\\\n",
    "             .flatMap(lambda x: readonep(filepath_, variable_, x, step)).zipWithIndex()\\\n",
    "\n",
    "    return rdd_\n",
    "\n",
    "\n",
    "def readonep(filepath_, variable_, start_idx, chunk_size):\n",
    "    \"\"\"Read a slice from one file.\n",
    "\n",
    "    Args:\n",
    "        filepath_    (str): string containing the file path\n",
    "        variable_    (str): variable name\n",
    "        start_idx    (int): starting index\n",
    "        chunk_size   (int): the chunk size to be read at a time.\n",
    "\n",
    "    Returns:\n",
    "        list:   list of the chunk read\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = Dataset(filepath_, 'r')\n",
    "        dset = f.variables[variable_]\n",
    "\n",
    "        # get the number of dimensions of the variable\n",
    "        dims = dset.dimensions\n",
    "        ndims = len(dims)\n",
    "        end_idx = start_idx + chunk_size\n",
    "\n",
    "        if end_idx < dset.shape[0]:\n",
    "            chunk = dset[tuple([slice(start_idx, end_idx)] + [slice(None)]*(ndims-1))]\n",
    "\n",
    "        else:\n",
    "            chunk = dset[tuple([slice(start_idx, dset.shape[0])] + [slice(None)]*(ndims-1))]\n",
    "\n",
    "        return list(chunk[:])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"IOError: {} {}\".format(e, filepath_))\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def dataframe(sc, file_list_or_txt_file, mode='multi', partitions=None):\n",
    "    \"\"\"Creates a distributed dataframe from a netCDF file.\n",
    "\n",
    "    Args:\n",
    "        sc                     (object)       : sparkContext Object\n",
    "        file_list_or_txt_file  (tuple)        : a tuple of the format (filepath, variable)\n",
    "        partitions             (int)          : number of partitions\n",
    "        mode                   (str)          : (multi) if reading multiple files, otherwise(single)\n",
    "    Returns:\n",
    "        df                  (dataframe)          : Spark's distributed data frame\n",
    "    \"\"\"\n",
    "\n",
    "    df = create_rdd(sc, file_list_or_txt_file, mode=mode, partitions=partitions)\\\n",
    "        .map(flatten_data)\\\n",
    "        .flatMap(lambda x: x).repartition(partitions*10)\\\n",
    "        .map(row_transform)\\\n",
    "        .toDF()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def rdd_to_df(rdd):\n",
    "    \"\"\"Function that converts an RDD into a Spark data frame.\n",
    "    Arguments:\n",
    "        - rdd: (rdd)\n",
    "\n",
    "    Returns:\n",
    "        - df: Spark dataframe\n",
    "    \"\"\"\n",
    "    df = rdd.map(flatten_data)\\\n",
    "            .flatMap(lambda x: x).repartition(PARTITIONS*10)\\\n",
    "            .map(row_transform)\\\n",
    "            .toDF()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_data(line):\n",
    "    \"\"\"Flattens numpy array and return a tuple of each value\n",
    "    and its corresponding lat_lon coordinates together with other dimensions.\n",
    "\n",
    "    Args:\n",
    "        line (tuple) :  an rdd element in the form of a tuple (data, idx) where data is\n",
    "                        a numpy array and idx correspond to time index.\n",
    "\n",
    "    Returns:\n",
    "         results (tuple): a transformed rdd element in the form\n",
    "                           of a tuple (idx, dim1_value, dim_value2, ..., data_value)\n",
    "    \"\"\"\n",
    "    data = line[0].ravel().tolist()\n",
    "    idx = line[1]\n",
    "    results = map(lambda x: (idx, ) + (x[0]) + (x[1], ), zip(other_dims_values_tuple.value, data))\n",
    "    return results\n",
    "\n",
    "\n",
    "def row_transform(line):\n",
    "    \"\"\"Transforms a a tuple (idx, dim1_value, dim_value2, ..., data_value) into a Spark sql\n",
    "       Row object.\n",
    "\n",
    "    Args:\n",
    "        line (tuple): a tuple of the form (idx, dim1_value, dim_value2, ..., data_value)\n",
    "\n",
    "    Returns:\n",
    "        row(*line) : Spark Row object with arbitray number of items depending on the size of\n",
    "                     the tuple in line.\n",
    "\n",
    "    Examples:\n",
    "        >>> print(line)\n",
    "        (0, 100000.0, -90.0, 0.0, 257.8)\n",
    "        >>> row(*line)\n",
    "        Row(time=0, plev=100000.0, lat=-90.0, lon=0.0, ta=257.8)\n",
    "\n",
    "    \"\"\"\n",
    "    dims_ = dims.value\n",
    "    ndims_ = len(dims_)\n",
    "    variable_ = variable_name.value\n",
    "    columns = (\"time\",)+tuple(dims_[:])+(variable_,)\n",
    "    row = Row(*columns)\n",
    "    return row(*line)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = rdd_to_df(precc_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94170"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "precc_df = dataframe(sc, (filepath, 'PRECC'), mode='single', partitions=dset.partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------+--------------------+\n",
      "|time|               lat|   lon|               PRECC|\n",
      "+----+------------------+------+--------------------+\n",
      "| 221|57.958115183246065| 317.5|1.915180458667009...|\n",
      "| 221|57.958115183246065|318.75|1.754952072552384...|\n",
      "| 221|57.958115183246065| 320.0|4.623490923449935...|\n",
      "| 221|57.958115183246065|321.25|2.340043536719349...|\n",
      "| 221|57.958115183246065| 322.5|3.059930264726063E-8|\n",
      "| 221|57.958115183246065|323.75|2.528910059140798...|\n",
      "| 221|57.958115183246065| 325.0|1.764903601042533...|\n",
      "| 221|57.958115183246065|326.25|1.104392222117667...|\n",
      "| 221|57.958115183246065| 327.5|7.332674911708636...|\n",
      "| 221|57.958115183246065|328.75|2.120446353226501...|\n",
      "| 315| 66.43979057591622|  17.5|                 0.0|\n",
      "| 315| 66.43979057591622| 18.75|                 0.0|\n",
      "| 315| 66.43979057591622|  20.0|                 0.0|\n",
      "| 315| 66.43979057591622| 21.25|                 0.0|\n",
      "| 315| 66.43979057591622|  22.5|                 0.0|\n",
      "| 315| 66.43979057591622| 23.75|                 0.0|\n",
      "| 315| 66.43979057591622|  25.0|                 0.0|\n",
      "| 315| 66.43979057591622| 26.25|                 0.0|\n",
      "| 315| 66.43979057591622|  27.5|                 0.0|\n",
      "| 315| 66.43979057591622| 28.75|                 0.0|\n",
      "+----+------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
